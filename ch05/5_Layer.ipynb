{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レイヤの実装と練習\n",
    "## おしながき\n",
    "- 乗算レイヤ\n",
    "- 加算レイヤ\n",
    "- sigmoidレイヤ\n",
    "- Affineレイヤ\n",
    "- Softmax-with-lossレイヤ\n",
    "- 誤差逆伝搬法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最初のお約束の設定\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "#import common.gradient as g #例えば、本書の共通ライブラリを読み込む場合の設定\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 乗算レイヤの写経"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#乗算レイヤ\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "    \n",
    "    def backward(self, z):\n",
    "        return z * self.y, z * self.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## りんご２個の買い物の計算グラフ〜の写経"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===forward 結果===\n",
      "200\n",
      "220.00000000000003\n",
      "=== backward 結果===\n",
      "tax_layerの微分\n",
      "taxの微分\n",
      "200\n",
      "apple_layerの微分\n",
      "appleの微分\n",
      "2.2\n",
      "num_of_appleの微分\n",
      "110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "apple         = 100 #りんごが100円\n",
    "num_of_apples = 2  #りんごの個数\n",
    "tax           = 1.1#消費税\n",
    "\n",
    "apple_layer = MulLayer()\n",
    "tax_layer   = MulLayer()\n",
    "\n",
    "apple_layer_result = apple_layer.forward(apple, num_of_apples)\n",
    "tax_layer_result   = tax_layer.forward(apple_layer_result, tax)\n",
    "\n",
    "print(\"===forward 結果===\")\n",
    "print(apple_layer_result)\n",
    "print(tax_layer_result)\n",
    "\n",
    "print(\"=== backward 結果===\")\n",
    "tax_layer_back = tax_layer.backward(1)\n",
    "\n",
    "print(\"tax_layerの微分\")\n",
    "print(\"taxの微分\")\n",
    "upper, lower = tax_layer_back\n",
    "print(lower)\n",
    "\n",
    "print(\"apple_layerの微分\")\n",
    "upper, lower = apple_layer.backward(upper)\n",
    "\n",
    "print(\"appleの微分\")\n",
    "print(upper)\n",
    "print(\"num_of_appleの微分\")\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加算レイヤの実装の写経"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, z):\n",
    "        return z, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複雑な例（りんご２個と、みかん３個の買い物）の計算グラフ〜の写経"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== forward ====\n",
      "715.0000000000001\n",
      "==== backward ====\n",
      "== taxの微分 ==\n",
      "650\n",
      "== りんご、りんごの個数、みかん、みかんの個数の微分 == \n",
      "== りんごの微分==\n",
      "2.2\n",
      "== りんごの個数の微分\n",
      "110.00000000000001\n",
      "== みかんの微分==\n",
      "3.3000000000000003\n",
      "== みかんの個数の微分\n",
      "165.0\n"
     ]
    }
   ],
   "source": [
    "apple         = 100 #りんごの値段\n",
    "num_of_apples = 2   #りんごの個数\n",
    "\n",
    "orange        = 150 #みかんの値段\n",
    "num_of_oranges= 3   #みかんの個数\n",
    "\n",
    "tax           = 1.1 #消費税率\n",
    "\n",
    "apple_layer            = MulLayer()\n",
    "orange_layer           = MulLayer()\n",
    "apple_and_orange_layer = AddLayer()\n",
    "tax_layer              = MulLayer()\n",
    "\n",
    "print(\"==== forward ====\")\n",
    "apple_layer_res            = apple_layer.forward(apple, num_of_apples)\n",
    "orange_layer_res           = orange_layer.forward(orange, num_of_oranges)\n",
    "apple_and_orange_layer_res = apple_and_orange_layer.forward(apple_layer_res, orange_layer_res)\n",
    "forward_res                = tax_layer.forward(apple_and_orange_layer_res, tax)\n",
    "\n",
    "print(forward_res)\n",
    "\n",
    "print(\"==== backward ====\")\n",
    "upper, lower = tax_layer.backward(1)\n",
    "print(\"== taxの微分 ==\")\n",
    "print(lower)\n",
    "\n",
    "print(\"== りんご、りんごの個数、みかん、みかんの個数の微分 == \")\n",
    "dx_apple, dx_orange = apple_and_orange_layer.backward(upper)\n",
    "upper, lower = apple_layer.backward(dx_apple)\n",
    "print(\"== りんごの微分==\")\n",
    "print(upper)\n",
    "print(\"== りんごの個数の微分\")\n",
    "print(lower)\n",
    "\n",
    "upper, lower = orange_layer.backward(dx_orange)\n",
    "print(\"== みかんの微分==\")\n",
    "print(upper)\n",
    "print(\"== みかんの個数の微分\")\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★ReLUレイヤの実装の写経\n",
    "### その前に、ReLU関数とはなにか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    ReLU(x) =\n",
    "        \\begin{cases}\n",
    "            x \\quad x > 0 \\\\\n",
    "            0 \\quad x \\leqq 0 \\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "その微分は以下。\n",
    "\n",
    "$$\n",
    "    ReLU(x)' =\n",
    "        \\begin{cases}\n",
    "            1 \\quad x > 0 \\\\\n",
    "            0 \\quad x \\leqq 0 \\\\\n",
    "        \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLUレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 間違った実装から\n",
    "#### なぜ勘違いしたのか？\n",
    "入力が関数の形を見て、スカラー的に判断してしまった。\n",
    "入力がベクトルの時、ベクトルのそれぞれの要素に対して演算を行う必要がある。\n",
    "以下の実装は、入力がスカラーの場合は良いが、ベクトルの場合に対応していない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluBad:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def backward(self):\n",
    "        if self.x > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正しい実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpyの機能をあまり使わない素朴な実装(さらなる間違いも発覚、根本的な理解不足）\n",
    "配列操作周りは改善したが、そもそもの根本的な理解不足があり。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluNaive:\n",
    "    def __init__(self):\n",
    "        #forward時に正の要素は通し、負のものは通さない。backward時も同様に正は通して、負は通さない。\n",
    "        #この時、forward時に通したもの（インデックス）を覚えておく必要があるので、そもそも、クラスのインスタンス変数として\n",
    "        #記憶しておくものは、xではなく、maskである。\n",
    "        self.x = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        y = x.copy()\n",
    "        for idx in range(y.shape[0]):\n",
    "            if y[idx] <= 0:\n",
    "                y[idx] = 0\n",
    "        return y\n",
    "        \n",
    "    def backward(self): #ここも間違い。逆伝搬してきた値に、自分自身の微分を掛けたものを復帰値とする必要がある。\n",
    "                        #このため、まず、引数を１つ受け取る必要があるが、設定されていない間違い\n",
    "        y = x.copy()\n",
    "        for idx in range(y.shape[0]):\n",
    "            if y[idx] > 0:\n",
    "                y[idx] = 1 #正のものは、逆伝搬してきた値に1を掛けるのが正しい実装。\n",
    "            else:\n",
    "                y[idx] = 0 #ゼロ以下のものを０にするだけ。\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpyの機能を使った実装および、Reluへの正しい理解での実装\n",
    "テキストの実装の通り、numpyを使うとこの辺の実装が大変スマートになる。\n",
    "ブロードキャストとマスク機能を使う。\n",
    "\n",
    "機械学習のプログラミングではベクトルや行列が頻出であり、この辺をしっかり抑えることが肝要。\n",
    "\n",
    "#### ブロードキャストを使った比較と、マスク機能の練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n",
      "[[1. 0.]\n",
      " [0. 3.]]\n",
      "range(0, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5],[-2.0, 3.0]])\n",
    "y = x.copy()\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "mask = (x <= 0) #ブロードキャスト機能により、配列の各要素を比較した結果の配列を作ることができる\n",
    "print(mask)\n",
    "\n",
    "y[mask] = 0 # mask機能を使うと、Trueとなるindexの要素に値（この場合は０）を代入できる\n",
    "print(y)\n",
    "\n",
    "#y[[True]] = 0 # Error\n",
    "#print(y)      # Error\n",
    "\n",
    "print(range(y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpyの機能を使った効率的な実装(テキストの写経)　〜　完成版\n",
    "以下、修正点\n",
    "- インプットをベクトルを意識したものにした\n",
    "- numpyのマスク機能を使って効率的に\n",
    "- Relu関数の微分（forwardで通したものを記憶して、backwardに反映)をした。つまり、インスタンス変数としてmaskを記憶するようにした"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluMy:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x.copy()\n",
    "        mask = (x <= 0)\n",
    "        y[mask] = 0\n",
    "        self.mask = mask\n",
    "        return y\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★sigmoidレイヤの実装（テキストの写経)\n",
    "## まずは数式から（テキストP145)\n",
    "\n",
    "計算グラフを自力で書いて求めるのは別途練習するとして、ここでは実装に重きを置くため、テキストをさっと読んで、結論だけを利用する。\n",
    "つまり、テキストＰ１４５を見ると、sigmoidレイヤのforwardとbackward（微分)は以下。\n",
    "\n",
    "まず、forward方向（すでにsigmoid関数として実装していることを思い出そう）。\n",
    "以下、yとxはベクトルであることに注意する。数式としてはやや正確ではないと思うが、numpyのブロードキャストを想像しながら数式を見るとわかりやすい。詳しくは実装にて。\n",
    "$$\n",
    "    y = \\frac{1}{1+exp^(-x)} \\\\\n",
    "$$\n",
    "\n",
    "以下、backward方向\n",
    "\n",
    "$$\n",
    "      y' = \\frac{\\partial L}{\\partial y} y^2 exp(-x) \\\\\n",
    "$$\n",
    "\n",
    "なお、以下は、sigmoidレイヤに最初に逆伝搬で入ってきた、隣のレイヤからの値である。yはsigmoidレイヤで出力した値であり、実装ではインスタンスに保持しておくべき値であろうことに注意しておこう。xも同様に記録しておく必要あり。\n",
    "\n",
    "### 自力で実装してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidMy:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        pass\n",
    "    \n",
    "    #入力xはnumpyのベクトルであることに注意。\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (self.y*self.y) * np.exp(-self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストではＰ１４６で導出される式から、出力yのみで効率よく実装されているが、自力版はそうしていない。\n",
    "テキスト版と自力版のsigmoidレイヤでテストデータを試してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.]\n",
      "[0.00669285 0.01798621 0.04742587 0.11920292 0.26894142 0.5\n",
      " 0.73105858 0.88079708 0.95257413 0.98201379]\n",
      "[0.00669285 0.01798621 0.04742587 0.11920292 0.26894142 0.5\n",
      " 0.73105858 0.88079708 0.95257413 0.98201379]\n",
      "[0.00664806 0.01766271 0.04517666 0.10499359 0.19661193 0.25\n",
      " 0.19661193 0.10499359 0.04517666 0.01766271]\n",
      "[0.00664806 0.01766271 0.04517666 0.10499359 0.19661193 0.25\n",
      " 0.19661193 0.10499359 0.04517666 0.01766271]\n"
     ]
    }
   ],
   "source": [
    "from common.layers import Sigmoid\n",
    "\n",
    "t = Sigmoid()\n",
    "m = SigmoidMy()\n",
    "\n",
    "X = np.arange(-5.0, 5.0, 1)\n",
    "print(X)\n",
    "\n",
    "t1 = t.forward(X)\n",
    "print(t1)\n",
    "\n",
    "m1 = m.forward(X)\n",
    "print(m1)\n",
    "\n",
    "t2 = t.backward(1)\n",
    "print(t2)\n",
    "\n",
    "m2 = m.backward(1)\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自力版とテキスト版の出力はあっていそうなので、自力版の実装も正しいと思う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★Affineレイヤの実装（テキストの写経)\n",
    "## まずは数式から\n",
    "\n",
    "forward方向。行列Yは次のレイヤへの出力。行列Xは入力、行列Wは重み。ベクトルbはバイアス。\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{W} + \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "ここで、バイアスの加算については、注意が必要で、XWのそれぞれの行に対して、bが加算されていくことに注意（バイアスの微分で再び登場するので頭の片隅に入れておく）\n",
    "\n",
    "backward方向。行列Xの微分と行列Wの微分の２つがある。なお、行列Lはネットワークから出力された値で、一番最初に逆伝搬する値（このAffineレイヤに逆伝搬してくる値ではないことに注意。テキストＰ１３３を参照）。\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\boldsymbol{X}} = \\frac{\\partial L}{\\partial \\boldsymbol{Y}} \\boldsymbol{W}^T  \\\\\n",
    "$$\n",
    "\n",
    "なお、以下は、Affineレイヤに最初に逆伝搬で入ってくる値であることに注意。\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\boldsymbol{Y}}\\\\\n",
    "$$\n",
    "\n",
    "行列Wの微分は以下。\n",
    "\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\boldsymbol{W}} = \\boldsymbol{X}^T \\frac{\\partial L}{\\partial \\boldsymbol{Y}}   \\\\\n",
    "$$\n",
    "\n",
    "次に、バイアスの微分を考える必要がある。\n",
    "forwardでのバイアスの加算は、それぞれのデータ（１行目のデータ、２行目のデータ・・・）に対して加算が行われる。このため、バイアスの微分では、逆伝搬の値がバイアスの要素に集約される必要がある。つまり、Affineレイヤに最初に逆伝搬してきた値であるdY(複数行、複数列）を列の要素毎に合計したものを要素にもつベクトルがバイアスの微分になる。\n",
    "\n",
    "これが、Affineレイヤのforwardとbackwardのすべてである。以下、この数式を頼りにまずは、自力で実装してみる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自力での実装(→少し誤っている)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineMy1:\n",
    "    def __init__(self, X, W, b):\n",
    "        self.X = X\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        pass\n",
    "    \n",
    "    def forward(self):\n",
    "        return np.dot(self.X,self.W) + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        dW = np.dot(self.X.T, dout)\n",
    "        db = np.sum(dout, axis=0) #列要素毎にsumしたベクトルを作る\n",
    "        \n",
    "        return dX, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの正解とは異なる部分がおおい。\n",
    "ただし、あまり本質的な誤りではなく、おそらく、AffineLayerMyをAffineLayerとしてプログラミングしやすくする際の差分だと思う。\n",
    "- AffineLayerMyではコンストラクタの引数にXがあるが、テキストのAffineでは無い\n",
    "- AffineLayerMyではforwardにXは無いが、テキストのAffineにはある。\n",
    "- backwardではdXのみ返却して、dWやdbはインスタンス変数に保持したまま\n",
    "\n",
    "つまり、入力値Ｘについては、コンストラクタで覚えるのではなく、forwardで都度覚えるという形のほうが良いということだろう。\n",
    "考えてみれば、コンストラクタ時にはまだ、入力Ｘは決まっていない気がする。\n",
    "backwardでdXのみ返却するのも、プログラミングのしやすさを重視したものだと想像する（テキストを読み進めるなかで、この予想が正しいかを検証する）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストでの実装（そのまま、上記写経に合わせる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineMy:\n",
    "    def __init__(self, W, b):\n",
    "        self.W  = W\n",
    "        self.b  = b\n",
    "        self.X  = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self,X):\n",
    "        #forwardのつど、Xを記憶\n",
    "        self.X = X\n",
    "        return np.dot(self.X,self.W) + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        #dW, dbはインスタンス変数に覚えるのみ。\n",
    "        self.dW = np.dot(self.X.T, dout) \n",
    "        self.db = np.sum(dout, axis=0) #列要素毎にsumしたベクトルを作る\n",
    "        \n",
    "        return dX #dXのみ返却するようにした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★ softmax-with-lossレイヤの実装\n",
    "## 数式から\n",
    "\n",
    "計算グラフからの導出はテキストに詳しい。詳細はまだ理解していないが、実装を優先したいため、Ｐ１５４の図５−３０を参照する。\n",
    "これによれば、softmax-with-lossレイヤのforward方向はsoftmax関数を通したもの、backward方向は、forward方向で得たベクトルの要素から対応する教師データのラベルに対応する値を引けば良い。詳しくは前述の図を参照。\n",
    "\n",
    "## 自力で実装してみる（誤り=テキストと異なる点が幾つか存在）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import cross_entropy_error, softmax\n",
    "\n",
    "class SoftmaxWithLossMy1:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        pass\n",
    "    \n",
    "    #x: 入力データ（ベクトル）\n",
    "    #t: 教師データ(ベクトル/one hot表現)\n",
    "    #出力：softmaxされたニューラルネットワークの推論結果\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        res = cross_entropy_error(self.y , t)\n",
    "        return res\n",
    "    \n",
    "    def backward(self):\n",
    "        return self.y - self.t #ブロードキャストを利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストと異なる点は以下。\n",
    "- cross_entropy_errorの結果がインスタンス変数に記録されていない。ただし、これは記録されていなくてもloss自体はbackwardの計算にあらわれてこないため。lossを記録するのは単にプログラミングの利便性を上げるためのものだと想像。\n",
    "- backwardでbatch_sizeで割った値を復帰していない。これは盲点だった。ミニバッチへの考慮が必要。tの行数(ミニバッチに含まれる画像などの枚数（ピクセル数ではない数）でself.y - self.tの各要素を割ることで、データあたり（例：画像あたり）の誤差を逆伝搬するというもの\n",
    "\n",
    "### ミニバッチ対応版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLossMy:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        self.loss = None #softmaxの出力結果\n",
    "        pass\n",
    "    \n",
    "    #x: 入力データ（ベクトル）\n",
    "    #t: 教師データ(ベクトル/one hot表現)\n",
    "    #出力：softmaxされたニューラルネットワークの推論結果\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y , t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1): #backward関数群のインタフェースの整合性のため、ダミーでdoutを追加している。\n",
    "        batch_size = self.t.shape[0]\n",
    "        return (self.y - self.t) / batch_size #ブロードキャストを利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★誤差逆伝搬法の実装\n",
    "## 概要から\n",
    "\n",
    "テキストのＰ１５７に記載があるように、ニューラルネットワークの手順は以下。詳細はテキストを参照。\n",
    "\n",
    "- ステップ1(ミニバッチ)\n",
    "- ステップ2(勾配の算出)\n",
    "- ステップ3(パラメータの更新)\n",
    "- ステップ4(繰り返す)\n",
    "\n",
    "そこで、TwoLayerNetMyを更新する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwoLayerNetMy2\n",
    "誤差逆伝搬法に対応したTwoLayerNetMy2を作る。差分は最初にレイヤを作る、init_layersメソッド、および、forward_layers_for_predictionメソッド、\n",
    "誤差逆伝搬法そのものである、gradientメソッドの追加である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNetMy:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        ## 【W1層】\n",
    "        ## インプットは、(１行、インプット列)の行列。それに対して、W1層は、\n",
    "        ## （インプット行、hidden_size列）の行列で、インプットの列数と同じ行数で受ける必要がある。\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        ## 【W2層】\n",
    "        ## W1層の出力が（インプット行、hidden_size列）の行列になる。これがそのまま、W2層へのインプットとなる。\n",
    "        ## それに対して、アウトプットを(１行、output_size列)の行列に変換する必要があるから、自ずと、W2は、\n",
    "        ## (hidden_size行、output_size列)の行列になる。\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        #レイヤの初期化\n",
    "        self.init_layers()\n",
    "        \n",
    "    #誤差逆伝搬法で対応した新規の関数\n",
    "    def init_layers(self): #商用実装などではprotected相当にするべきだろう。けど、実験用なのでpublicでよいや\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Affine1\"] = AffineMy(self.params[\"W1\"], self.params[\"b1\"])\n",
    "        self.layers[\"Relu1\"]   = ReluMy()\n",
    "        self.layers[\"Affine2\"] = AffineMy(self.params[\"W2\"], self.params[\"b2\"])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLossMy()\n",
    "\n",
    "    #誤差逆伝搬法で対応した新規の関数\n",
    "    def forward_layers_for_prediction(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward_layers_for_prediction(x) #誤差逆伝搬法で対応したものに実装を置き換え\n",
    "        \n",
    "        #以下の２行はレイヤ構成に対応する前の古いインプリ\n",
    "        #temp = sigmoid(np.dot(x, self.params['W1']) + self.params['b1'])\n",
    "        #return  softmax(np.dot(temp, self.params['W2']) + self.params['b2'])\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        #forward\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        \n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "    \n",
    "        grads = {}\n",
    "        \n",
    "        grads['W1'] = self.layers[\"Affine1\"].dW\n",
    "        grads['b1'] = self.layers[\"Affine1\"].db\n",
    "        grads['W2'] = self.layers[\"Affine2\"].dW\n",
    "        grads['b2'] = self.layers[\"Affine2\"].db\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    #x: 入力データ、 t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        return self.lastLayer.forward(self.predict(x), t)\n",
    "        #以下、誤差逆伝搬法に対応する前の古い実装。\n",
    "        #return cross_entropy_error(self.predict(x), t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        #最も確率の高い要素のインデックスを取得。ただし、複数行になるため、行毎(axis=1)にＭＡＸなインデックスを得る\n",
    "        #このため、zの要素数はy.shape[0]になる。\n",
    "        y1 = np.argmax(y, axis=1)\n",
    "        t1 = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum( y == t ) #予測のインデックス==教師データの正解インデックスとなる要素をブロードキャストで計算する。\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写経終了。誤差逆伝搬法バージョンを試していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show grads\n",
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNetMy(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "\n",
    "\n",
    "#傾きのパラメータサイズの確認\n",
    "x = np.random.rand(100, 784) # ダミーの入力データ(784ピクセルの画像が100枚分)\n",
    "t = np.random.rand(100, 10)  # ダミーの正解ラベルが100枚分\n",
    "\n",
    "grads = net.gradient(x,t)\n",
    "\n",
    "print(\"show grads\")\n",
    "print(grads[\"W1\"].shape)\n",
    "print(grads[\"b1\"].shape)\n",
    "print(grads[\"W2\"].shape)\n",
    "print(grads[\"b2\"].shape)\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 done\n",
      "batch 1000 done\n",
      "batch 2000 done\n",
      "batch 3000 done\n",
      "batch 4000 done\n",
      "batch 5000 done\n",
      "batch 6000 done\n",
      "batch 7000 done\n",
      "batch 8000 done\n",
      "batch 9000 done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet #せっかくなので、テキストのTwoLayerNetを使ってしまおう\n",
    "\n",
    "#テキストのTwoLayerNetとTwoLayerNetMyを比べてみる\n",
    "\n",
    "def one_train(network = None):\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "    train_loss_list = []\n",
    "\n",
    "    #ハイパーパラメータの設定\n",
    "    iters_num     = 10000\n",
    "    train_size    = x_train.shape[0] #訓練データの個数(画像の枚数)\n",
    "    batch_size    = 100\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    #network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "    for i in range(iters_num):\n",
    "        #バッチの状況をiters_numの10%毎に報告する\n",
    "        #print(\"batch %d done\"%(i) )\n",
    "        if ((i / iters_num) * 100) % 10 == 0:\n",
    "            print(\"batch %d done\"%(i) )\n",
    "\n",
    "        #ミニバッチの取得\n",
    "        batch_mask = np.random.choice(train_size, batch_size) #0~train_size未満の整数の中からランダムにbatch_size個の整数を生成\n",
    "        x_batch = x_train[batch_mask] #batch_maskで指定した訓練データを取得する\n",
    "        t_batch = t_train[batch_mask] #正解ラベルも同様に実施\n",
    "\n",
    "        #勾配の計算\n",
    "        #print(\"calc grad\")\n",
    "        #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "        #誤差逆伝搬法のほうがずっと高速になるらしい！ →　めちゃくちゃ速い\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "        #パラメータの更新\n",
    "        #print(\"update params\")\n",
    "        for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "        #学習経過の記録\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "    \n",
    "    print(\"done\")\n",
    "    return train_loss_list\n",
    "        \n",
    "\n",
    "#試すフェーズ\n",
    "network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "train_loss_list = one_train(network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すばらしい！学習が進む度にロス率が下がっているコトが確認できた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 done\n",
      "batch 1000 done\n",
      "batch 2000 done\n",
      "batch 3000 done\n",
      "batch 4000 done\n",
      "batch 5000 done\n",
      "batch 6000 done\n",
      "batch 7000 done\n",
      "batch 8000 done\n",
      "batch 9000 done\n",
      "done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#自分のネットワークを試すフェーズ\n",
    "network = TwoLayerNetMy(input_size=784, hidden_size=100, output_size=10)\n",
    "train_loss_list = one_train(network)\n",
    "\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自分のネットについても同じ結果となって嬉しい！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
