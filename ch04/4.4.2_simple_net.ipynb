{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シンプルなニューラルネットワークの確認をする\n",
    "（テキストＰ１１０～）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最初のお約束の設定\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "#import common.gradient as g #例えば、本書の共通ライブラリを読み込む場合の設定\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#今回、追加で読み込むもの\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シンプルなネットワークを実装する前に、シンプルなネットワークで使われているnumpyの使い方を２〜３確認しておく。\n",
    "●np.random.randn・・・標準正規分布(平均0、分散1（標準偏差1））)に従った乱数を得る。引数は得たい乱数列のshape\n",
    "https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html?highlight=random%20randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[-0.58973736  0.08235795  0.81268808]\n",
      " [ 0.88913709 -0.04516613  0.95275348]]\n"
     ]
    }
   ],
   "source": [
    "#np.random.randn(2,3)　について得られる乱数は(2,3)行列\n",
    "r = np.random.randn(2,3)\n",
    "print(r.shape)\n",
    "print(r) #実行する毎に内容が異なることに注意（乱数だから当たり前）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いよいよ、シンプルネットワークを実装する。なお、以下の実装は、テキストP110をそのまま写経したもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #重みを乱数で初期化する。乱数で初期化するコト自体は非常に重要。理由はテキストに記載があるので参照。\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    #損失関数で損失を計算。xがニューラルネットの予測、tが正解データ(one hot 表現)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にシンプルネットの動作を確かめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.95293667  1.27081936  0.82490609]\n",
      " [ 1.0175071   0.31572356 -0.60563498]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) #重みを表示してみる（ランダム初期化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25600561  1.04664282 -0.05012783]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6,0.9]) #サンプルの入力データ\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.570366551130616"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1]) #正解ラベル\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキストの実行結果と合っているかを確認するため、重みをテキストと合わせる\n",
    "net.W = np.array([[0.47355232, 0.9977393, 0.84668094],\n",
    "                  [0.85557411, 0.03563661, 0.69422093]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n"
     ]
    }
   ],
   "source": [
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05414809 0.63071653 1.1328074 ]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9280682857864075"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6,0.9]) #サンプルの入力データ\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "t = np.array([0,0,1]) #正解ラベル\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はテキストと同じになっており、実装はただしそうである。\n",
    "\n",
    "以下、勾配を求めてみる。勾配を求める対象の関数は損失関数になる。\n",
    "また、numeriacl_gradientの仕様は関数を渡すことになるので、以下のように関数を定義する必要がある。\n",
    "なお、tはグローバル定義になる（上記に定義済み)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== start of W === \n",
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n",
      "=================\n",
      "====W=====\n",
      "[[0.47365232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9280902109609267\n",
      "==========\n",
      "====W=====\n",
      "[[0.47345232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9280463614466788\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9978393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.928082642357028\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9976393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9280539298710598\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84678094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9280320052165812\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84658094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9281045672167824\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85567411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9281011738612368\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85547411 0.03563661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9280353995898565\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03573661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9280898208880719\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03553661 0.69422093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.928046752159107\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69432093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.9280138652543771\n",
      "==========\n",
      "====W=====\n",
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69412093]]\n",
      "[0.6 0.9]\n",
      "[0 0 1]\n",
      "0.928122708254672\n",
      "==========\n",
      "[[ 0.21924757  0.14356243 -0.36281   ]\n",
      " [ 0.32887136  0.21534364 -0.544215  ]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    print(\"====W=====\")\n",
    "    print(W)\n",
    "    print(x)\n",
    "    print(t)\n",
    "    loss = net.loss(x,t)\n",
    "    print(loss)\n",
    "    print(\"==========\")\n",
    "    return loss\n",
    "\n",
    "print(\"== start of W === \")\n",
    "print(net.W)\n",
    "print(\"=================\")\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の出力結果を見ると、関数fに0.0001をプラスマイナスしながら引数が渡されて実行されていることがわかる。\n",
    "なお、numerical_gradient関数にnet.Wを渡すと、net.Wへのポインタを受け取ったnumerical_gradientの内部処理がnet.Wの値を微小(0.0001)だけnet.Wの値を直接変えながら、net.loss(x,t)を実行することに注意するべし。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
