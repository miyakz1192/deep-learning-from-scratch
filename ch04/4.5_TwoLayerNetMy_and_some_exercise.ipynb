{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TwoLayerNetクラスのテキストの写経と幾つかの試行(P114~)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwoLayerNetクラスの写経"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最初のお約束の設定\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "#import common.gradient as g #例えば、本書の共通ライブラリを読み込む場合の設定\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#追加の読み込み設定\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetMy:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        ## 【W1層】\n",
    "        ## インプットは、(１行、インプット列)の行列。それに対して、W1層は、\n",
    "        ## （インプット行、hidden_size列）の行列で、インプットの列数と同じ行数で受ける必要がある。\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        ## 【W2層】\n",
    "        ## W1層の出力が（インプット行、hidden_size列）の行列になる。これがそのまま、W2層へのインプットとなる。\n",
    "        ## それに対して、アウトプットを(１行、output_size列)の行列に変換する必要があるから、自ずと、W2は、\n",
    "        ## (hidden_size行、output_size列)の行列になる。\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        temp = sigmoid(np.dot(x, self.params['W1']) + self.params['b1'])\n",
    "        return  softmax(np.dot(temp, self.params['W2']) + self.params['b2'])\n",
    "        \n",
    "    #x: 入力データ、 t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        return cross_entropy_error(self.predict(x), t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        #最も確率の高い要素のインデックスを取得。ただし、複数行になるため、行毎(axis=1)にＭＡＸなインデックスを得る\n",
    "        #このため、zの要素数はy.shape[0]になる。\n",
    "        y1 = np.argmax(y, axis=1)\n",
    "        t1 = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum( y == t ) #予測のインデックス==教師データの正解インデックスとなる要素をブロードキャストで計算する。\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写経終了。実際に試していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "#パラメータのサイズの確認\n",
    "net = TwoLayerNetMy(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "print(net.params[\"W1\"].shape)\n",
    "print(net.params[\"b1\"].shape)\n",
    "print(net.params[\"W2\"].shape)\n",
    "print(net.params[\"b2\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show grads\n",
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "#傾きのパラメータサイズの確認\n",
    "x = np.random.rand(100, 784) # ダミーの入力データ(784ピクセルの画像が100枚分)\n",
    "t = np.random.rand(100, 10)  # ダミーの正解ラベルが100枚分\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "print(\"show grads\")\n",
    "print(grads[\"W1\"].shape)\n",
    "print(grads[\"b1\"].shape)\n",
    "print(grads[\"W2\"].shape)\n",
    "print(grads[\"b2\"].shape)\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまで出来るようになったが、一言言えるのは、numerical_gradientの計算がめちゃくちゃ遅いということ。大体、２〜３分かかる（ＣＰＵを１５個くらい１００％利用率で動作させて）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ学習の実装(P118の写経)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 done\n",
      "batch 0 done\n",
      "calc grad\n",
      "update params\n",
      "batch 1 done\n",
      "batch 1 done\n",
      "calc grad\n",
      "update params\n",
      "batch 2 done\n",
      "batch 2 done\n",
      "calc grad\n",
      "update params\n",
      "batch 3 done\n",
      "batch 3 done\n",
      "calc grad\n",
      "update params\n",
      "batch 4 done\n",
      "batch 4 done\n",
      "calc grad\n",
      "update params\n",
      "batch 5 done\n",
      "batch 5 done\n",
      "calc grad\n",
      "update params\n",
      "batch 6 done\n",
      "batch 6 done\n",
      "calc grad\n",
      "update params\n",
      "batch 7 done\n",
      "batch 7 done\n",
      "calc grad\n",
      "update params\n",
      "batch 8 done\n",
      "batch 8 done\n",
      "calc grad\n",
      "update params\n",
      "batch 9 done\n",
      "batch 9 done\n",
      "calc grad\n",
      "update params\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet #せっかくなので、テキストのTwoLayerNetを使ってしまおう\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "#ハイパーパラメータの設定\n",
    "iters_num     = 10\n",
    "train_size    = x_train.shape[0] #訓練データの個数(画像の枚数)\n",
    "batch_size    = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #バッチの状況をiters_numの10%毎に報告する\n",
    "    print(\"batch %d done\"%(i) )\n",
    "    if ((i / iters_num) * 100) % 10 == 0:\n",
    "        print(\"batch %d done\"%(i) )\n",
    "    \n",
    "    #ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size) #0~train_size未満の整数の中からランダムにbatch_size個の整数を生成\n",
    "    x_batch = x_train[batch_mask] #batch_maskで指定した訓練データを取得する\n",
    "    t_batch = t_train[batch_mask] #正解ラベルも同様に実施\n",
    "    \n",
    "    #勾配の計算\n",
    "    print(\"calc grad\")\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #誤差逆伝搬法のほうがずっと高速になるらしい！\n",
    "    #grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    #パラメータの更新\n",
    "    print(\"update params\")\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #学習経過の記録\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストでは10000イテレーションとなっていたが、１イテレーションあたり2分くらいかかりそうなので、10イテレーションに落とした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2889820114450554, 2.2744496324076366, 2.2855275950057408, 2.2975220019498606, 2.2942392060129473, 2.2953792300734426, 2.284913721526043, 2.2715120581118704, 2.2984081760085284, 2.280698811283469]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たった、10イテレーションしか回していないが、ロス率は減っていそうだ。\n",
    "numerical_gradientは非常に時間がかかるので、誤差逆伝搬法を実装した後で、イテレーションを増やしたい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
