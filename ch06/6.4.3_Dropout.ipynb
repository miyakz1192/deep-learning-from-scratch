{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropoutを試してみる\n",
    "過学習を防ぐ仕組みとしてweight decayは簡単に実装できるのだが、ニューラルネットワークのモデルが複雑になるとweight decayでは対応できなくなるとのこと。Dropoutはニューラルネットワークのニューロンをランダムに消去することで、信号の伝達を遮断することで、過学習を防ぐというもの\n",
    "1. 訓練時：データが流れる度にニューロンをランダムに選択して消去する\n",
    "2. テスト時：すべてのニューロンの信号を伝達するが、各ニューロンの出力に対して訓練時に消去した割合を乗算して出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD, Adam\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装を見て理解する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropoutクラス本体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試行\"deep-learning-from-scratch/my_jupyter/numpy_excersice.ipynb\"に記載のあるとおり、numpyの性質を上手く使ってmaskを掛けている。\n",
    "forward時では入力行列(x)からランダムに要素を消す。この時に、消した要素の位置を覚えておき(self.mask)、\n",
    "backward時にはそれを利用して、逆伝搬してきた要素と同じ位置の要素を消す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### レイヤの位置\n",
    "テキストのMultiLayerNetExtendクラスの実装を見てみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
    "                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.params = {}\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # レイヤの生成\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            if self.use_batchnorm:\n",
    "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
    "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
    "                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
    "                \n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "            \n",
    "            if self.use_dropout: #★ここ！\n",
    "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コメントの記しているところ（★ここ！の部分）がdropoutの挿入箇所である。隠れ層毎にアクティベーションレイヤが終わった後に、Dropoutを行っている。また、最後でAffineをしてSoftmaxWithLossを実施している。これはBatchNormalizationと同じ。要するにDropoutはBatchNormalizationに上手く挿入することができるというコトである。ここに、ニューラルネットワークの上手いモジュール性が活かされていることが良くわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際に動かしてみる。\n",
    "テキストの実験コードをコピペして動かしてみる(\"ch06/overfit_dropout.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "def trial(use_dropout=True, dropout_ratio=0.2):\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "    # 過学習を再現するために、学習データを削減\n",
    "    x_train = x_train[:300]\n",
    "    t_train = t_train[:300]\n",
    "\n",
    "    # Dropuoutの有無、割り合いの設定 ========================\n",
    "    #use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
    "    #dropout_ratio = 0.2\n",
    "    # ====================================================\n",
    "\n",
    "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                                  output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                      epochs=301, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "    trainer.train()\n",
    "\n",
    "    train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "    # グラフの描画==========\n",
    "    markers = {'train': 'o', 'test': 's'}\n",
    "    x = np.arange(len(train_acc_list))\n",
    "    plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "    plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.figure()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3075257053444305\n",
      "=== epoch:1, train acc:0.10666666666666667, test acc:0.0914 ===\n",
      "train loss:2.28090672955925\n",
      "train loss:2.301476446437571\n",
      "train loss:2.2913174416105964\n",
      "=== epoch:2, train acc:0.11, test acc:0.0932 ===\n",
      "train loss:2.3071096225837726\n",
      "train loss:2.298518210787746\n",
      "train loss:2.3030143094924127\n",
      "=== epoch:3, train acc:0.11, test acc:0.0947 ===\n",
      "train loss:2.2805561406033434\n",
      "train loss:2.2966089404931664\n",
      "train loss:2.290722232077873\n",
      "=== epoch:4, train acc:0.11, test acc:0.0961 ===\n",
      "train loss:2.2981981942529495\n",
      "train loss:2.297808462371442\n",
      "train loss:2.2962521180034092\n",
      "=== epoch:5, train acc:0.10666666666666667, test acc:0.096 ===\n",
      "train loss:2.284139243356933\n",
      "train loss:2.2895800706043308\n",
      "train loss:2.3017954699432392\n",
      "=== epoch:6, train acc:0.11333333333333333, test acc:0.0984 ===\n",
      "train loss:2.27750479157615\n",
      "train loss:2.306244802196673\n",
      "train loss:2.299318148677594\n",
      "=== epoch:7, train acc:0.11, test acc:0.0987 ===\n",
      "train loss:2.274799047157314\n",
      "train loss:2.3030589946001974\n",
      "train loss:2.2941211516183775\n",
      "=== epoch:8, train acc:0.11, test acc:0.1001 ===\n",
      "train loss:2.302935088953876\n",
      "train loss:2.307892262944495\n",
      "train loss:2.3105570473902426\n",
      "=== epoch:9, train acc:0.11333333333333333, test acc:0.1014 ===\n",
      "train loss:2.287474505933879\n",
      "train loss:2.3026883548660186\n",
      "train loss:2.2891698649736267\n",
      "=== epoch:10, train acc:0.11666666666666667, test acc:0.1025 ===\n",
      "train loss:2.292974728830867\n",
      "train loss:2.288120958031345\n",
      "train loss:2.28213601277831\n",
      "=== epoch:11, train acc:0.12333333333333334, test acc:0.1058 ===\n",
      "train loss:2.2925067278933877\n",
      "train loss:2.2961140109989255\n",
      "train loss:2.2854416213510746\n",
      "=== epoch:12, train acc:0.12333333333333334, test acc:0.1061 ===\n",
      "train loss:2.268311098008387\n",
      "train loss:2.2664205044810997\n",
      "train loss:2.3087235030421986\n",
      "=== epoch:13, train acc:0.12333333333333334, test acc:0.1076 ===\n",
      "train loss:2.2780238004635645\n",
      "train loss:2.2903236761069454\n",
      "train loss:2.299474226351458\n",
      "=== epoch:14, train acc:0.12666666666666668, test acc:0.1102 ===\n",
      "train loss:2.2922746678959527\n",
      "train loss:2.286961860745086\n",
      "train loss:2.292438661621173\n",
      "=== epoch:15, train acc:0.12666666666666668, test acc:0.1129 ===\n",
      "train loss:2.2893309603268883\n",
      "train loss:2.2983135315729553\n",
      "train loss:2.277878285614184\n",
      "=== epoch:16, train acc:0.12333333333333334, test acc:0.1118 ===\n",
      "train loss:2.296903024347923\n",
      "train loss:2.300773625190112\n",
      "train loss:2.284842715390675\n",
      "=== epoch:17, train acc:0.12, test acc:0.1121 ===\n",
      "train loss:2.278660919381897\n",
      "train loss:2.2759833615281804\n",
      "train loss:2.26898712288868\n",
      "=== epoch:18, train acc:0.12, test acc:0.1123 ===\n",
      "train loss:2.2874960887739304\n",
      "train loss:2.285300907206555\n",
      "train loss:2.2651732806970624\n",
      "=== epoch:19, train acc:0.11666666666666667, test acc:0.1127 ===\n",
      "train loss:2.2907902228605277\n",
      "train loss:2.2761952127450193\n",
      "train loss:2.297542470427853\n",
      "=== epoch:20, train acc:0.12, test acc:0.1163 ===\n",
      "train loss:2.280484264958566\n",
      "train loss:2.261523643035072\n",
      "train loss:2.2823172984504727\n",
      "=== epoch:21, train acc:0.12, test acc:0.1179 ===\n",
      "train loss:2.274782202056163\n",
      "train loss:2.277678966110717\n",
      "train loss:2.2717195778519907\n",
      "=== epoch:22, train acc:0.13333333333333333, test acc:0.1242 ===\n",
      "train loss:2.262993047095402\n",
      "train loss:2.2826366350926985\n",
      "train loss:2.287634476225419\n",
      "=== epoch:23, train acc:0.13666666666666666, test acc:0.1289 ===\n",
      "train loss:2.285622778916617\n",
      "train loss:2.2755688948074266\n",
      "train loss:2.280358243973931\n",
      "=== epoch:24, train acc:0.13333333333333333, test acc:0.1281 ===\n",
      "train loss:2.274291248620184\n",
      "train loss:2.2768609743131463\n",
      "train loss:2.2554599925599224\n",
      "=== epoch:25, train acc:0.13, test acc:0.1271 ===\n",
      "train loss:2.276223771053332\n",
      "train loss:2.2893906245257147\n",
      "train loss:2.2846411774383077\n",
      "=== epoch:26, train acc:0.13333333333333333, test acc:0.1297 ===\n",
      "train loss:2.2927882554424546\n",
      "train loss:2.2704948564335035\n",
      "train loss:2.2781172848584013\n",
      "=== epoch:27, train acc:0.13666666666666666, test acc:0.1328 ===\n",
      "train loss:2.2736330292160476\n",
      "train loss:2.272270027555153\n",
      "train loss:2.291060961645296\n",
      "=== epoch:28, train acc:0.15, test acc:0.1395 ===\n",
      "train loss:2.2691748860150356\n",
      "train loss:2.2610811834917786\n",
      "train loss:2.2717581436437326\n",
      "=== epoch:29, train acc:0.15666666666666668, test acc:0.1427 ===\n",
      "train loss:2.296292471796819\n",
      "train loss:2.2726714045500724\n",
      "train loss:2.272032819227641\n",
      "=== epoch:30, train acc:0.16, test acc:0.1456 ===\n",
      "train loss:2.2511527822411366\n",
      "train loss:2.285371907985979\n",
      "train loss:2.26838772266276\n",
      "=== epoch:31, train acc:0.16, test acc:0.1458 ===\n",
      "train loss:2.2666823484878837\n",
      "train loss:2.2581568310637854\n",
      "train loss:2.275539340578179\n",
      "=== epoch:32, train acc:0.16, test acc:0.1469 ===\n",
      "train loss:2.280789681908379\n",
      "train loss:2.284935110583934\n",
      "train loss:2.272652545025633\n",
      "=== epoch:33, train acc:0.16333333333333333, test acc:0.1486 ===\n",
      "train loss:2.2543823317444396\n",
      "train loss:2.275518791606216\n",
      "train loss:2.2680727958584854\n",
      "=== epoch:34, train acc:0.17, test acc:0.1515 ===\n",
      "train loss:2.266238670752052\n",
      "train loss:2.262850656018278\n",
      "train loss:2.2749160269061255\n",
      "=== epoch:35, train acc:0.17, test acc:0.1533 ===\n",
      "train loss:2.282436785282761\n",
      "train loss:2.279032842547551\n",
      "train loss:2.270487125585435\n",
      "=== epoch:36, train acc:0.18333333333333332, test acc:0.1603 ===\n",
      "train loss:2.2536660497976726\n",
      "train loss:2.273583127167849\n",
      "train loss:2.2655329898834866\n",
      "=== epoch:37, train acc:0.19333333333333333, test acc:0.1606 ===\n",
      "train loss:2.256216700378466\n",
      "train loss:2.2794499710454676\n",
      "train loss:2.2631262123297136\n",
      "=== epoch:38, train acc:0.19666666666666666, test acc:0.1661 ===\n",
      "train loss:2.265967074515706\n",
      "train loss:2.2709646460044492\n",
      "train loss:2.2588333948244324\n",
      "=== epoch:39, train acc:0.19666666666666666, test acc:0.1697 ===\n",
      "train loss:2.265202182532396\n",
      "train loss:2.2518900645163002\n",
      "train loss:2.2611396621363444\n",
      "=== epoch:40, train acc:0.20333333333333334, test acc:0.1738 ===\n",
      "train loss:2.262781173308793\n",
      "train loss:2.2530288451520035\n",
      "train loss:2.251695020392142\n",
      "=== epoch:41, train acc:0.21, test acc:0.1793 ===\n",
      "train loss:2.2611440659948174\n",
      "train loss:2.2652880576083696\n",
      "train loss:2.263727310374019\n",
      "=== epoch:42, train acc:0.21666666666666667, test acc:0.1878 ===\n",
      "train loss:2.248642267230139\n",
      "train loss:2.276658377409942\n",
      "train loss:2.2483198210449666\n",
      "=== epoch:43, train acc:0.22666666666666666, test acc:0.1919 ===\n",
      "train loss:2.249699674011827\n",
      "train loss:2.258180399270246\n",
      "train loss:2.272112326707076\n",
      "=== epoch:44, train acc:0.23666666666666666, test acc:0.1948 ===\n",
      "train loss:2.2557561164721474\n",
      "train loss:2.2660914729989474\n",
      "train loss:2.253747164114058\n",
      "=== epoch:45, train acc:0.24, test acc:0.2017 ===\n",
      "train loss:2.2788739878356084\n",
      "train loss:2.2618389625138797\n",
      "train loss:2.246689541585424\n",
      "=== epoch:46, train acc:0.24333333333333335, test acc:0.2058 ===\n",
      "train loss:2.2671054246305586\n",
      "train loss:2.26832981999564\n",
      "train loss:2.2784560341504827\n",
      "=== epoch:47, train acc:0.24333333333333335, test acc:0.2082 ===\n",
      "train loss:2.281710932193842\n",
      "train loss:2.23779839450568\n",
      "train loss:2.2743379177993863\n",
      "=== epoch:48, train acc:0.24666666666666667, test acc:0.2119 ===\n",
      "train loss:2.255933673696287\n",
      "train loss:2.2520661210658166\n",
      "train loss:2.2458193886459874\n",
      "=== epoch:49, train acc:0.24333333333333335, test acc:0.2105 ===\n",
      "train loss:2.2517313710113425\n",
      "train loss:2.257673419305635\n",
      "train loss:2.250113764365869\n",
      "=== epoch:50, train acc:0.25, test acc:0.2146 ===\n",
      "train loss:2.25497599894344\n",
      "train loss:2.2528155061115904\n",
      "train loss:2.2650459592354943\n",
      "=== epoch:51, train acc:0.25333333333333335, test acc:0.2185 ===\n",
      "train loss:2.2385452221851314\n",
      "train loss:2.246558063208973\n",
      "train loss:2.253758687528498\n",
      "=== epoch:52, train acc:0.25666666666666665, test acc:0.2204 ===\n",
      "train loss:2.2531500136639804\n",
      "train loss:2.240029320858861\n",
      "train loss:2.2481460773970934\n",
      "=== epoch:53, train acc:0.26666666666666666, test acc:0.2245 ===\n",
      "train loss:2.2493582885988337\n",
      "train loss:2.247620273472645\n",
      "train loss:2.227709356668168\n",
      "=== epoch:54, train acc:0.26666666666666666, test acc:0.2277 ===\n",
      "train loss:2.2344867208402617\n",
      "train loss:2.259194784385207\n",
      "train loss:2.2443717947188797\n",
      "=== epoch:55, train acc:0.2733333333333333, test acc:0.2323 ===\n",
      "train loss:2.2353137387816355\n",
      "train loss:2.2428052075918035\n",
      "train loss:2.246758935307106\n",
      "=== epoch:56, train acc:0.2733333333333333, test acc:0.2343 ===\n",
      "train loss:2.232663179395955\n",
      "train loss:2.2536041414636845\n",
      "train loss:2.2506377379823053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.2733333333333333, test acc:0.2361 ===\n",
      "train loss:2.254380243866072\n",
      "train loss:2.2253471438083214\n",
      "train loss:2.237095014344229\n",
      "=== epoch:58, train acc:0.27666666666666667, test acc:0.2379 ===\n",
      "train loss:2.2355571965047747\n",
      "train loss:2.2293820015932053\n",
      "train loss:2.267172585624678\n",
      "=== epoch:59, train acc:0.27666666666666667, test acc:0.2388 ===\n",
      "train loss:2.238206353140479\n",
      "train loss:2.2717355032418296\n",
      "train loss:2.2220172795551907\n",
      "=== epoch:60, train acc:0.2833333333333333, test acc:0.2401 ===\n",
      "train loss:2.252370865811189\n",
      "train loss:2.232117926324153\n",
      "train loss:2.2484249607086473\n",
      "=== epoch:61, train acc:0.2833333333333333, test acc:0.2413 ===\n",
      "train loss:2.2412046677548254\n",
      "train loss:2.2506377222667076\n",
      "train loss:2.2555284704085063\n",
      "=== epoch:62, train acc:0.2833333333333333, test acc:0.2429 ===\n",
      "train loss:2.2441129144471335\n",
      "train loss:2.246320164411244\n",
      "train loss:2.232228410921735\n",
      "=== epoch:63, train acc:0.2833333333333333, test acc:0.2438 ===\n",
      "train loss:2.255312072546252\n",
      "train loss:2.253498627221772\n",
      "train loss:2.2307953091656283\n",
      "=== epoch:64, train acc:0.2833333333333333, test acc:0.2461 ===\n",
      "train loss:2.2558819910196455\n",
      "train loss:2.2564930771461857\n",
      "train loss:2.239095059236103\n",
      "=== epoch:65, train acc:0.2833333333333333, test acc:0.2445 ===\n",
      "train loss:2.249143416499056\n",
      "train loss:2.2586268239151486\n",
      "train loss:2.213541130017231\n",
      "=== epoch:66, train acc:0.2833333333333333, test acc:0.2474 ===\n",
      "train loss:2.2289522721354014\n",
      "train loss:2.2411325978079693\n",
      "train loss:2.2465425620785062\n",
      "=== epoch:67, train acc:0.2833333333333333, test acc:0.2472 ===\n",
      "train loss:2.2165529019069727\n",
      "train loss:2.236084779859978\n",
      "train loss:2.245314402991488\n",
      "=== epoch:68, train acc:0.2833333333333333, test acc:0.2478 ===\n",
      "train loss:2.216174346896108\n",
      "train loss:2.2602687530175176\n",
      "train loss:2.217994487585909\n",
      "=== epoch:69, train acc:0.2833333333333333, test acc:0.247 ===\n",
      "train loss:2.2523848860040934\n",
      "train loss:2.2247619021280918\n",
      "train loss:2.2662826996295165\n",
      "=== epoch:70, train acc:0.2866666666666667, test acc:0.248 ===\n",
      "train loss:2.2260871912082045\n",
      "train loss:2.197429982803526\n",
      "train loss:2.2365360788676742\n",
      "=== epoch:71, train acc:0.29, test acc:0.2486 ===\n",
      "train loss:2.2045933115865237\n",
      "train loss:2.2211994059966833\n",
      "train loss:2.2282724994176926\n",
      "=== epoch:72, train acc:0.29, test acc:0.2473 ===\n",
      "train loss:2.2330107967848396\n",
      "train loss:2.2380916581820296\n",
      "train loss:2.209127146458633\n",
      "=== epoch:73, train acc:0.29, test acc:0.2492 ===\n",
      "train loss:2.2233172225493876\n",
      "train loss:2.231749111536059\n",
      "train loss:2.19281466512302\n",
      "=== epoch:74, train acc:0.2966666666666667, test acc:0.2486 ===\n",
      "train loss:2.2279426749542894\n",
      "train loss:2.2318915984105527\n",
      "train loss:2.207225924202081\n",
      "=== epoch:75, train acc:0.2966666666666667, test acc:0.2472 ===\n",
      "train loss:2.231013380428703\n",
      "train loss:2.219894300107583\n",
      "train loss:2.230264287758078\n",
      "=== epoch:76, train acc:0.2966666666666667, test acc:0.2477 ===\n",
      "train loss:2.210782142697372\n",
      "train loss:2.1913675209466477\n",
      "train loss:2.2396661206693107\n",
      "=== epoch:77, train acc:0.2966666666666667, test acc:0.2504 ===\n",
      "train loss:2.241723064418744\n",
      "train loss:2.203383839193817\n",
      "train loss:2.233870165073361\n",
      "=== epoch:78, train acc:0.3, test acc:0.2501 ===\n",
      "train loss:2.2504188403946093\n",
      "train loss:2.2453052904189166\n",
      "train loss:2.2147773095119803\n",
      "=== epoch:79, train acc:0.31, test acc:0.2537 ===\n",
      "train loss:2.1974914006279302\n",
      "train loss:2.228277342900722\n",
      "train loss:2.2246885734702575\n",
      "=== epoch:80, train acc:0.31, test acc:0.2553 ===\n",
      "train loss:2.227446596471255\n",
      "train loss:2.2106496381279337\n",
      "train loss:2.2268284530779363\n",
      "=== epoch:81, train acc:0.31, test acc:0.2549 ===\n",
      "train loss:2.221374449126775\n",
      "train loss:2.1903411376175193\n",
      "train loss:2.2369930574000993\n",
      "=== epoch:82, train acc:0.31333333333333335, test acc:0.2592 ===\n",
      "train loss:2.211582501905873\n",
      "train loss:2.206404103194073\n",
      "train loss:2.2149698488247482\n",
      "=== epoch:83, train acc:0.31666666666666665, test acc:0.2569 ===\n",
      "train loss:2.203040533273981\n",
      "train loss:2.192296717933218\n",
      "train loss:2.221583555827115\n",
      "=== epoch:84, train acc:0.31666666666666665, test acc:0.257 ===\n",
      "train loss:2.22703788965249\n",
      "train loss:2.2026323313371647\n",
      "train loss:2.230056642845892\n",
      "=== epoch:85, train acc:0.31666666666666665, test acc:0.2592 ===\n",
      "train loss:2.229636822868624\n",
      "train loss:2.2043121089872573\n",
      "train loss:2.1885070132317446\n",
      "=== epoch:86, train acc:0.31666666666666665, test acc:0.2598 ===\n",
      "train loss:2.2301818363354466\n",
      "train loss:2.2185774515465484\n",
      "train loss:2.188693482482935\n",
      "=== epoch:87, train acc:0.31333333333333335, test acc:0.2617 ===\n",
      "train loss:2.188551551779882\n",
      "train loss:2.1979448422004126\n",
      "train loss:2.1841829642520394\n",
      "=== epoch:88, train acc:0.31333333333333335, test acc:0.2622 ===\n",
      "train loss:2.2116976401422392\n",
      "train loss:2.181692179047678\n",
      "train loss:2.227155037533874\n",
      "=== epoch:89, train acc:0.31333333333333335, test acc:0.2616 ===\n",
      "train loss:2.15994758871463\n",
      "train loss:2.202066708717426\n",
      "train loss:2.2165140728111807\n",
      "=== epoch:90, train acc:0.31, test acc:0.2598 ===\n",
      "train loss:2.175772374623225\n",
      "train loss:2.215778451589246\n",
      "train loss:2.2016259082684315\n",
      "=== epoch:91, train acc:0.31, test acc:0.258 ===\n",
      "train loss:2.178857122278618\n",
      "train loss:2.214346657678601\n",
      "train loss:2.191394266846069\n",
      "=== epoch:92, train acc:0.31, test acc:0.259 ===\n",
      "train loss:2.1477389483663116\n",
      "train loss:2.231558692347453\n",
      "train loss:2.210852072638245\n",
      "=== epoch:93, train acc:0.31, test acc:0.2559 ===\n",
      "train loss:2.156751325016685\n",
      "train loss:2.187803192856174\n",
      "train loss:2.227922985154913\n",
      "=== epoch:94, train acc:0.31, test acc:0.258 ===\n",
      "train loss:2.199173145720602\n",
      "train loss:2.2222822971698473\n",
      "train loss:2.187502411290443\n",
      "=== epoch:95, train acc:0.31333333333333335, test acc:0.26 ===\n",
      "train loss:2.222303697600807\n",
      "train loss:2.182026739397578\n",
      "train loss:2.194294975620766\n",
      "=== epoch:96, train acc:0.31333333333333335, test acc:0.2622 ===\n",
      "train loss:2.175070118821629\n",
      "train loss:2.2030401909098143\n",
      "train loss:2.158603447974352\n",
      "=== epoch:97, train acc:0.31333333333333335, test acc:0.2625 ===\n",
      "train loss:2.1981821306348683\n",
      "train loss:2.1865668158925824\n",
      "train loss:2.2097675154848817\n",
      "=== epoch:98, train acc:0.31333333333333335, test acc:0.264 ===\n",
      "train loss:2.209496777731295\n",
      "train loss:2.1773167671269906\n",
      "train loss:2.189753885153282\n",
      "=== epoch:99, train acc:0.31333333333333335, test acc:0.2649 ===\n",
      "train loss:2.1508614198489813\n",
      "train loss:2.1804997040305185\n",
      "train loss:2.1655125779198205\n",
      "=== epoch:100, train acc:0.31, test acc:0.2647 ===\n",
      "train loss:2.200778787998167\n",
      "train loss:2.1957488956177587\n",
      "train loss:2.194872971494837\n",
      "=== epoch:101, train acc:0.31, test acc:0.2649 ===\n",
      "train loss:2.148512924563794\n",
      "train loss:2.212673065431196\n",
      "train loss:2.153457661095648\n",
      "=== epoch:102, train acc:0.31, test acc:0.264 ===\n",
      "train loss:2.198904827933655\n",
      "train loss:2.157333409852928\n",
      "train loss:2.187819687395519\n",
      "=== epoch:103, train acc:0.31666666666666665, test acc:0.2655 ===\n",
      "train loss:2.1530467785867704\n",
      "train loss:2.168706031756386\n",
      "train loss:2.175111931041963\n",
      "=== epoch:104, train acc:0.31, test acc:0.2645 ===\n",
      "train loss:2.177770209699483\n",
      "train loss:2.1688380761526433\n",
      "train loss:2.1603721643598957\n",
      "=== epoch:105, train acc:0.32, test acc:0.267 ===\n",
      "train loss:2.1585394081985707\n",
      "train loss:2.185203466206497\n",
      "train loss:2.1654137112830667\n",
      "=== epoch:106, train acc:0.31666666666666665, test acc:0.2657 ===\n",
      "train loss:2.186220493184307\n",
      "train loss:2.1506340085951594\n",
      "train loss:2.173397448898085\n",
      "=== epoch:107, train acc:0.31333333333333335, test acc:0.267 ===\n",
      "train loss:2.1629889295367244\n",
      "train loss:2.1492117782512605\n",
      "train loss:2.1368273098486794\n",
      "=== epoch:108, train acc:0.31, test acc:0.2663 ===\n",
      "train loss:2.1835733519728024\n",
      "train loss:2.175022411331711\n",
      "train loss:2.164436996310315\n",
      "=== epoch:109, train acc:0.31666666666666665, test acc:0.2657 ===\n",
      "train loss:2.1465650081078946\n",
      "train loss:2.1961997932104524\n",
      "train loss:2.1352541467177617\n",
      "=== epoch:110, train acc:0.32, test acc:0.2673 ===\n",
      "train loss:2.1880222054949483\n",
      "train loss:2.1509424771002528\n",
      "train loss:2.1877449542746175\n",
      "=== epoch:111, train acc:0.32, test acc:0.2688 ===\n",
      "train loss:2.1528421962480673\n",
      "train loss:2.153908569936197\n",
      "train loss:2.1688301506301757\n",
      "=== epoch:112, train acc:0.3233333333333333, test acc:0.2685 ===\n",
      "train loss:2.1705807538913824\n",
      "train loss:2.1327827641491255\n",
      "train loss:2.132707533792302\n",
      "=== epoch:113, train acc:0.32, test acc:0.2687 ===\n",
      "train loss:2.162229410908079\n",
      "train loss:2.1518431468204744\n",
      "train loss:2.2203448668528414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:114, train acc:0.31666666666666665, test acc:0.2693 ===\n",
      "train loss:2.1532276816021136\n",
      "train loss:2.1249289761112697\n",
      "train loss:2.172227224058181\n",
      "=== epoch:115, train acc:0.32666666666666666, test acc:0.2715 ===\n",
      "train loss:2.1363520025382328\n",
      "train loss:2.1528737486462637\n",
      "train loss:2.161121877684097\n",
      "=== epoch:116, train acc:0.32666666666666666, test acc:0.2713 ===\n",
      "train loss:2.1608423916618666\n",
      "train loss:2.146780668211337\n",
      "train loss:2.1682290008606198\n",
      "=== epoch:117, train acc:0.3233333333333333, test acc:0.2722 ===\n",
      "train loss:2.15443995658285\n",
      "train loss:2.1541541049081654\n",
      "train loss:2.1842728079547675\n",
      "=== epoch:118, train acc:0.32666666666666666, test acc:0.2721 ===\n",
      "train loss:2.15506293870734\n",
      "train loss:2.158996959393529\n",
      "train loss:2.125400830145363\n",
      "=== epoch:119, train acc:0.3333333333333333, test acc:0.2759 ===\n",
      "train loss:2.1425011431540972\n",
      "train loss:2.1805450431387188\n",
      "train loss:2.174471526308983\n",
      "=== epoch:120, train acc:0.3333333333333333, test acc:0.2783 ===\n",
      "train loss:2.091990683468518\n",
      "train loss:2.2092182984169595\n",
      "train loss:2.1571071598998133\n",
      "=== epoch:121, train acc:0.33666666666666667, test acc:0.2787 ===\n",
      "train loss:2.1078598005112608\n",
      "train loss:2.145311103897402\n",
      "train loss:2.1741630659060296\n",
      "=== epoch:122, train acc:0.3333333333333333, test acc:0.28 ===\n",
      "train loss:2.150378247648191\n",
      "train loss:2.1284320105248624\n",
      "train loss:2.1139139233229267\n",
      "=== epoch:123, train acc:0.3333333333333333, test acc:0.282 ===\n",
      "train loss:2.1782770323211285\n",
      "train loss:2.175130897099465\n",
      "train loss:2.142186028989888\n",
      "=== epoch:124, train acc:0.3333333333333333, test acc:0.2831 ===\n",
      "train loss:2.110471548405507\n",
      "train loss:2.1448969477221187\n",
      "train loss:2.1065585353208394\n",
      "=== epoch:125, train acc:0.3333333333333333, test acc:0.2834 ===\n",
      "train loss:2.1176587665666684\n",
      "train loss:2.076242851423788\n",
      "train loss:2.118658321724934\n",
      "=== epoch:126, train acc:0.3333333333333333, test acc:0.2833 ===\n",
      "train loss:2.0653400426969073\n",
      "train loss:2.090969830910112\n",
      "train loss:2.149668279229004\n",
      "=== epoch:127, train acc:0.3333333333333333, test acc:0.2825 ===\n",
      "train loss:2.165977139764925\n",
      "train loss:2.056768203402344\n",
      "train loss:2.1066652539575768\n",
      "=== epoch:128, train acc:0.33, test acc:0.2832 ===\n",
      "train loss:2.076320223401946\n",
      "train loss:2.0998291733838967\n",
      "train loss:2.074251191328484\n",
      "=== epoch:129, train acc:0.33, test acc:0.2822 ===\n",
      "train loss:2.1033384638275736\n",
      "train loss:2.114074521874595\n",
      "train loss:2.1340905251852624\n",
      "=== epoch:130, train acc:0.33666666666666667, test acc:0.2847 ===\n",
      "train loss:2.0967266178647797\n",
      "train loss:2.0774726850963194\n",
      "train loss:2.12013297663385\n",
      "=== epoch:131, train acc:0.33666666666666667, test acc:0.2838 ===\n",
      "train loss:2.124125634180379\n",
      "train loss:2.073580910007726\n",
      "train loss:2.0706442780175607\n",
      "=== epoch:132, train acc:0.33, test acc:0.2822 ===\n",
      "train loss:2.139139044996239\n",
      "train loss:2.078606717297235\n",
      "train loss:2.0889121357213054\n",
      "=== epoch:133, train acc:0.33666666666666667, test acc:0.2846 ===\n",
      "train loss:2.114172920372036\n",
      "train loss:2.048156402802737\n",
      "train loss:2.1273320018042017\n",
      "=== epoch:134, train acc:0.33, test acc:0.2849 ===\n",
      "train loss:2.1838227798983327\n",
      "train loss:2.079508938751756\n",
      "train loss:2.0665377218542362\n",
      "=== epoch:135, train acc:0.33, test acc:0.2853 ===\n",
      "train loss:2.090333623963972\n",
      "train loss:2.096495906394002\n",
      "train loss:2.1038378279963053\n",
      "=== epoch:136, train acc:0.3333333333333333, test acc:0.287 ===\n",
      "train loss:2.102409492244601\n",
      "train loss:2.0839637561050397\n",
      "train loss:2.123635957480631\n",
      "=== epoch:137, train acc:0.33, test acc:0.2879 ===\n",
      "train loss:2.1051982530605944\n",
      "train loss:2.0660434017709135\n",
      "train loss:2.030670444711592\n",
      "=== epoch:138, train acc:0.3333333333333333, test acc:0.2897 ===\n",
      "train loss:2.0413106540419\n",
      "train loss:1.9988215357638845\n",
      "train loss:2.0621404094376796\n",
      "=== epoch:139, train acc:0.32666666666666666, test acc:0.2857 ===\n",
      "train loss:2.0699772003401726\n",
      "train loss:2.056570812854893\n",
      "train loss:2.0576735141395615\n",
      "=== epoch:140, train acc:0.33, test acc:0.2858 ===\n",
      "train loss:2.0923180330890854\n",
      "train loss:2.0854487806747537\n",
      "train loss:2.0024869778831738\n",
      "=== epoch:141, train acc:0.3333333333333333, test acc:0.2869 ===\n",
      "train loss:2.084606554767129\n",
      "train loss:2.0428816390068767\n",
      "train loss:2.1261680466674258\n",
      "=== epoch:142, train acc:0.3333333333333333, test acc:0.2894 ===\n",
      "train loss:2.0047143749520693\n",
      "train loss:2.067279238498583\n",
      "train loss:2.1043233919795012\n",
      "=== epoch:143, train acc:0.3333333333333333, test acc:0.2916 ===\n",
      "train loss:2.129098442197494\n",
      "train loss:2.0038961811783533\n",
      "train loss:2.037360649827005\n",
      "=== epoch:144, train acc:0.3333333333333333, test acc:0.2931 ===\n",
      "train loss:2.0323589033234715\n",
      "train loss:2.098021612140055\n",
      "train loss:2.0066655755545098\n",
      "=== epoch:145, train acc:0.3333333333333333, test acc:0.2945 ===\n",
      "train loss:1.9888082030441567\n",
      "train loss:2.0805078383978524\n",
      "train loss:2.007523511035137\n",
      "=== epoch:146, train acc:0.3333333333333333, test acc:0.2939 ===\n",
      "train loss:2.072519340938347\n",
      "train loss:1.9920252228074167\n",
      "train loss:2.084239557719178\n",
      "=== epoch:147, train acc:0.3333333333333333, test acc:0.2932 ===\n",
      "train loss:2.047172096545884\n",
      "train loss:2.0265236187143607\n",
      "train loss:2.0141395855506063\n",
      "=== epoch:148, train acc:0.33666666666666667, test acc:0.2941 ===\n",
      "train loss:1.951638920183628\n",
      "train loss:2.0903648754909896\n",
      "train loss:2.036476001861707\n",
      "=== epoch:149, train acc:0.34, test acc:0.2921 ===\n",
      "train loss:2.0462324657086723\n",
      "train loss:2.000517024712886\n",
      "train loss:2.082875896357511\n",
      "=== epoch:150, train acc:0.33666666666666667, test acc:0.2923 ===\n",
      "train loss:2.0567907975933686\n",
      "train loss:2.058021548293018\n",
      "train loss:2.017104141918758\n",
      "=== epoch:151, train acc:0.34, test acc:0.2933 ===\n",
      "train loss:2.056284229194818\n",
      "train loss:2.081903797018039\n",
      "train loss:1.9771475600299677\n",
      "=== epoch:152, train acc:0.33666666666666667, test acc:0.2937 ===\n",
      "train loss:2.0758051040591825\n",
      "train loss:2.0223875538363973\n",
      "train loss:2.0027371786942068\n",
      "=== epoch:153, train acc:0.34, test acc:0.2968 ===\n",
      "train loss:1.9643352487393966\n",
      "train loss:2.048383304989767\n",
      "train loss:2.028076683171611\n",
      "=== epoch:154, train acc:0.3433333333333333, test acc:0.2965 ===\n",
      "train loss:1.9896565656456289\n",
      "train loss:2.0620831531007813\n",
      "train loss:2.02965894148648\n",
      "=== epoch:155, train acc:0.3433333333333333, test acc:0.2975 ===\n",
      "train loss:1.9810454665270434\n",
      "train loss:2.0412301826299357\n",
      "train loss:1.9688168339511998\n",
      "=== epoch:156, train acc:0.3466666666666667, test acc:0.2984 ===\n",
      "train loss:2.0396038511177577\n",
      "train loss:2.018849182267904\n",
      "train loss:2.044338259277767\n",
      "=== epoch:157, train acc:0.35, test acc:0.3013 ===\n",
      "train loss:1.992972460918523\n",
      "train loss:2.026836699555387\n",
      "train loss:1.8983462209483541\n",
      "=== epoch:158, train acc:0.35, test acc:0.2994 ===\n",
      "train loss:1.9590342610686036\n",
      "train loss:2.070057472948168\n",
      "train loss:2.074936333131024\n",
      "=== epoch:159, train acc:0.35, test acc:0.3014 ===\n",
      "train loss:1.9303564495502772\n",
      "train loss:2.0282218551831064\n",
      "train loss:2.027910585755747\n",
      "=== epoch:160, train acc:0.35333333333333333, test acc:0.3006 ===\n",
      "train loss:1.931607124025848\n",
      "train loss:1.910079627605453\n",
      "train loss:1.928716777234188\n",
      "=== epoch:161, train acc:0.35, test acc:0.3019 ===\n",
      "train loss:2.045594895715722\n",
      "train loss:2.0692609694468183\n",
      "train loss:1.9653430099460716\n",
      "=== epoch:162, train acc:0.3466666666666667, test acc:0.302 ===\n",
      "train loss:1.9902628386045311\n",
      "train loss:2.0024692233546535\n",
      "train loss:1.9990917108412405\n",
      "=== epoch:163, train acc:0.35, test acc:0.3043 ===\n",
      "train loss:1.9432732997822837\n",
      "train loss:2.0003849396186024\n",
      "train loss:1.9999101770904375\n",
      "=== epoch:164, train acc:0.35333333333333333, test acc:0.3038 ===\n",
      "train loss:2.0112981559026433\n",
      "train loss:2.0420555806266023\n",
      "train loss:1.8997876993253704\n",
      "=== epoch:165, train acc:0.3466666666666667, test acc:0.3003 ===\n",
      "train loss:1.9904006987584766\n",
      "train loss:2.008090384721598\n",
      "train loss:1.8874777187081022\n",
      "=== epoch:166, train acc:0.35, test acc:0.3009 ===\n",
      "train loss:1.877202061640031\n",
      "train loss:1.9360469047552364\n",
      "train loss:1.8601047777759052\n",
      "=== epoch:167, train acc:0.3433333333333333, test acc:0.2994 ===\n",
      "train loss:1.9802821388984861\n",
      "train loss:2.029939946643473\n",
      "train loss:1.980245062395577\n",
      "=== epoch:168, train acc:0.3433333333333333, test acc:0.3 ===\n",
      "train loss:2.0132979756685048\n",
      "train loss:1.9820419253568335\n",
      "train loss:2.0001550925849916\n",
      "=== epoch:169, train acc:0.36, test acc:0.3011 ===\n",
      "train loss:1.967714241437881\n",
      "train loss:1.8917136587119001\n",
      "train loss:1.9494980426849984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:170, train acc:0.36, test acc:0.3037 ===\n",
      "train loss:1.9524050893767448\n",
      "train loss:1.999317123047018\n",
      "train loss:1.9849874935174583\n",
      "=== epoch:171, train acc:0.36333333333333334, test acc:0.3066 ===\n",
      "train loss:1.9385321591002844\n",
      "train loss:1.9857638894928251\n",
      "train loss:1.9851219402696267\n",
      "=== epoch:172, train acc:0.36666666666666664, test acc:0.3095 ===\n",
      "train loss:1.9835730186559488\n",
      "train loss:1.9653640979105245\n",
      "train loss:2.001200788673517\n",
      "=== epoch:173, train acc:0.37, test acc:0.3126 ===\n",
      "train loss:1.9587402237332248\n",
      "train loss:1.9146782480365476\n",
      "train loss:1.949269005116105\n",
      "=== epoch:174, train acc:0.37, test acc:0.3121 ===\n",
      "train loss:1.89550251551147\n",
      "train loss:1.9453243805105114\n",
      "train loss:1.8507151401491677\n",
      "=== epoch:175, train acc:0.37, test acc:0.3142 ===\n",
      "train loss:1.8587510855775797\n",
      "train loss:1.8764459450853448\n",
      "train loss:1.9316551946943403\n",
      "=== epoch:176, train acc:0.37, test acc:0.3132 ===\n",
      "train loss:1.9172776017662592\n",
      "train loss:1.8850627753981868\n",
      "train loss:1.8469975009212332\n",
      "=== epoch:177, train acc:0.37333333333333335, test acc:0.3126 ===\n",
      "train loss:2.012027755169164\n",
      "train loss:1.8128445564498747\n",
      "train loss:1.9487395052626193\n",
      "=== epoch:178, train acc:0.38333333333333336, test acc:0.3168 ===\n",
      "train loss:2.007657127207217\n",
      "train loss:1.9688709835237\n",
      "train loss:1.9587388459302102\n",
      "=== epoch:179, train acc:0.38333333333333336, test acc:0.3211 ===\n",
      "train loss:1.8688973690419524\n",
      "train loss:1.9356208603002723\n",
      "train loss:1.8642314488858804\n",
      "=== epoch:180, train acc:0.38, test acc:0.3195 ===\n",
      "train loss:1.9774711673776093\n",
      "train loss:1.9151731307723607\n",
      "train loss:1.9328613804907735\n",
      "=== epoch:181, train acc:0.38, test acc:0.3238 ===\n",
      "train loss:1.9635537248167776\n",
      "train loss:1.9727007559368728\n",
      "train loss:1.865451763084204\n",
      "=== epoch:182, train acc:0.38333333333333336, test acc:0.3245 ===\n",
      "train loss:1.8590142755776287\n",
      "train loss:1.903387765929613\n",
      "train loss:1.9076568479201461\n",
      "=== epoch:183, train acc:0.38666666666666666, test acc:0.3249 ===\n",
      "train loss:1.8161376453264189\n",
      "train loss:1.9449720078392392\n",
      "train loss:1.8262334036213435\n",
      "=== epoch:184, train acc:0.38666666666666666, test acc:0.3246 ===\n",
      "train loss:1.9584524673179748\n",
      "train loss:2.007975004110228\n",
      "train loss:1.9162419427880035\n",
      "=== epoch:185, train acc:0.4, test acc:0.3262 ===\n",
      "train loss:1.9700980713701235\n",
      "train loss:1.8780925350401352\n",
      "train loss:1.9235367127579384\n",
      "=== epoch:186, train acc:0.4033333333333333, test acc:0.3287 ===\n",
      "train loss:1.9060090359297355\n",
      "train loss:1.8649133453063016\n",
      "train loss:1.873425832564842\n",
      "=== epoch:187, train acc:0.4033333333333333, test acc:0.3276 ===\n",
      "train loss:1.9267075647279441\n",
      "train loss:1.872760556688252\n",
      "train loss:1.9601329095786317\n",
      "=== epoch:188, train acc:0.4066666666666667, test acc:0.3316 ===\n",
      "train loss:1.9009705447763423\n",
      "train loss:1.8994549090246469\n",
      "train loss:1.9320833551600152\n",
      "=== epoch:189, train acc:0.4166666666666667, test acc:0.3336 ===\n",
      "train loss:1.9628266473664542\n",
      "train loss:1.856424578960293\n",
      "train loss:1.8721990214161597\n",
      "=== epoch:190, train acc:0.4166666666666667, test acc:0.3351 ===\n",
      "train loss:1.934380986129269\n",
      "train loss:1.9231364255048293\n",
      "train loss:1.9307139097146813\n",
      "=== epoch:191, train acc:0.42333333333333334, test acc:0.3372 ===\n",
      "train loss:1.8796756157896959\n",
      "train loss:1.9074349767179097\n",
      "train loss:1.9025977305546962\n",
      "=== epoch:192, train acc:0.42333333333333334, test acc:0.3358 ===\n",
      "train loss:1.826208670086038\n",
      "train loss:1.7957533891577098\n",
      "train loss:1.8503107561872838\n",
      "=== epoch:193, train acc:0.4266666666666667, test acc:0.3347 ===\n",
      "train loss:1.9007609650871167\n",
      "train loss:1.8612253525450884\n",
      "train loss:1.8245911399059742\n",
      "=== epoch:194, train acc:0.43, test acc:0.3365 ===\n",
      "train loss:1.8403921919925608\n",
      "train loss:1.8667621587812357\n",
      "train loss:1.8193940693495787\n",
      "=== epoch:195, train acc:0.43, test acc:0.3373 ===\n",
      "train loss:1.8446584181000396\n",
      "train loss:1.7955715994862067\n",
      "train loss:1.850645817057594\n",
      "=== epoch:196, train acc:0.4266666666666667, test acc:0.3375 ===\n",
      "train loss:1.7709584182250082\n",
      "train loss:1.8572991805359071\n",
      "train loss:2.0009362173213714\n",
      "=== epoch:197, train acc:0.43, test acc:0.3407 ===\n",
      "train loss:1.7905881489034992\n",
      "train loss:1.8292069526256833\n",
      "train loss:1.7923641238725878\n",
      "=== epoch:198, train acc:0.4266666666666667, test acc:0.3382 ===\n",
      "train loss:1.8417426081555104\n",
      "train loss:1.8701470325991096\n",
      "train loss:1.861592251499066\n",
      "=== epoch:199, train acc:0.43, test acc:0.3401 ===\n",
      "train loss:1.7364338841353333\n",
      "train loss:1.761050792945221\n",
      "train loss:1.8430644552770483\n",
      "=== epoch:200, train acc:0.4266666666666667, test acc:0.3382 ===\n",
      "train loss:1.815682603476695\n",
      "train loss:1.7898132082204115\n",
      "train loss:1.8009423273531808\n",
      "=== epoch:201, train acc:0.4266666666666667, test acc:0.3398 ===\n",
      "train loss:1.8531373906606847\n",
      "train loss:1.790190115219881\n",
      "train loss:1.7251285483733063\n",
      "=== epoch:202, train acc:0.4266666666666667, test acc:0.3372 ===\n",
      "train loss:1.7360731601250645\n",
      "train loss:1.8321762053308317\n",
      "train loss:1.8551784827049262\n",
      "=== epoch:203, train acc:0.4266666666666667, test acc:0.3363 ===\n",
      "train loss:1.8404477533605081\n",
      "train loss:1.8897983276578798\n",
      "train loss:1.8617924740909415\n",
      "=== epoch:204, train acc:0.43, test acc:0.3391 ===\n",
      "train loss:1.8450648258833846\n",
      "train loss:1.7997172708742766\n",
      "train loss:1.8199272446778414\n",
      "=== epoch:205, train acc:0.43, test acc:0.3399 ===\n",
      "train loss:1.8071051293633544\n",
      "train loss:1.8846357969786356\n",
      "train loss:1.799603151562777\n",
      "=== epoch:206, train acc:0.43, test acc:0.3426 ===\n",
      "train loss:1.8246146170038142\n",
      "train loss:1.8349385296473\n",
      "train loss:1.8384606943836928\n",
      "=== epoch:207, train acc:0.43666666666666665, test acc:0.3444 ===\n",
      "train loss:1.7868671978940267\n",
      "train loss:1.7553330231436879\n",
      "train loss:1.6728555407056427\n",
      "=== epoch:208, train acc:0.43333333333333335, test acc:0.3436 ===\n",
      "train loss:1.6471053486440308\n",
      "train loss:1.7922648775446495\n",
      "train loss:1.7471599446804396\n",
      "=== epoch:209, train acc:0.43333333333333335, test acc:0.3416 ===\n",
      "train loss:1.7618931645036284\n",
      "train loss:1.769436503999465\n",
      "train loss:1.785270495350762\n",
      "=== epoch:210, train acc:0.43333333333333335, test acc:0.3416 ===\n",
      "train loss:1.8366882218379346\n",
      "train loss:1.8605397135550863\n",
      "train loss:1.7893957328305863\n",
      "=== epoch:211, train acc:0.44, test acc:0.3447 ===\n",
      "train loss:1.7999127239643335\n",
      "train loss:1.712929497272841\n",
      "train loss:1.7010255700174923\n",
      "=== epoch:212, train acc:0.44333333333333336, test acc:0.3463 ===\n",
      "train loss:1.7778627266403957\n",
      "train loss:1.7301707382210803\n",
      "train loss:1.6701964953221735\n",
      "=== epoch:213, train acc:0.44666666666666666, test acc:0.347 ===\n",
      "train loss:1.6377427442318504\n",
      "train loss:1.7572859432747423\n",
      "train loss:1.7694164314793241\n",
      "=== epoch:214, train acc:0.45, test acc:0.3473 ===\n",
      "train loss:1.7251480268943964\n",
      "train loss:1.7390577725526006\n",
      "train loss:1.8004309281025266\n",
      "=== epoch:215, train acc:0.44666666666666666, test acc:0.3499 ===\n",
      "train loss:1.6379943828133694\n",
      "train loss:1.8646574408015395\n",
      "train loss:1.7995592233124018\n",
      "=== epoch:216, train acc:0.44333333333333336, test acc:0.3506 ===\n",
      "train loss:1.6732342789799586\n",
      "train loss:1.68482606435322\n",
      "train loss:1.7474205238839908\n",
      "=== epoch:217, train acc:0.44666666666666666, test acc:0.3498 ===\n",
      "train loss:1.7893794787423136\n",
      "train loss:1.5951782592983732\n",
      "train loss:1.6925525157449726\n",
      "=== epoch:218, train acc:0.44666666666666666, test acc:0.3527 ===\n",
      "train loss:1.7603938585419723\n",
      "train loss:1.6220428747302265\n",
      "train loss:1.7484926408423098\n",
      "=== epoch:219, train acc:0.44666666666666666, test acc:0.356 ===\n",
      "train loss:1.8461459670061748\n",
      "train loss:1.6969149320574264\n",
      "train loss:1.6555759159909613\n",
      "=== epoch:220, train acc:0.44666666666666666, test acc:0.3553 ===\n",
      "train loss:1.7443320990882956\n",
      "train loss:1.8791832848279284\n",
      "train loss:1.814857709625118\n",
      "=== epoch:221, train acc:0.44666666666666666, test acc:0.356 ===\n",
      "train loss:1.768391963839796\n",
      "train loss:1.6233607301726158\n",
      "train loss:1.7271798019385898\n",
      "=== epoch:222, train acc:0.45, test acc:0.3556 ===\n",
      "train loss:1.7520297321719491\n",
      "train loss:1.7906034173991017\n",
      "train loss:1.8138397868520222\n",
      "=== epoch:223, train acc:0.45, test acc:0.3598 ===\n",
      "train loss:1.8557076008328892\n",
      "train loss:1.7757155011349894\n",
      "train loss:1.6661486297319659\n",
      "=== epoch:224, train acc:0.4533333333333333, test acc:0.3593 ===\n",
      "train loss:1.775676594310732\n",
      "train loss:1.7319515835372499\n",
      "train loss:1.8305006938772226\n",
      "=== epoch:225, train acc:0.4533333333333333, test acc:0.3627 ===\n",
      "train loss:1.7885981830982325\n",
      "train loss:1.6458364151414202\n",
      "train loss:1.7895275862236724\n",
      "=== epoch:226, train acc:0.46, test acc:0.3633 ===\n",
      "train loss:1.814017992608198\n",
      "train loss:1.8325959329395856\n",
      "train loss:1.558308179678275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:227, train acc:0.45666666666666667, test acc:0.364 ===\n",
      "train loss:1.6694700235868447\n",
      "train loss:1.7993891074904287\n",
      "train loss:1.713357120240825\n",
      "=== epoch:228, train acc:0.46, test acc:0.3659 ===\n",
      "train loss:1.5655189912552445\n",
      "train loss:1.6752869204081364\n",
      "train loss:1.6916058581142182\n",
      "=== epoch:229, train acc:0.46, test acc:0.365 ===\n",
      "train loss:1.692019653132919\n",
      "train loss:1.8013370035187168\n",
      "train loss:1.820135388526286\n",
      "=== epoch:230, train acc:0.4633333333333333, test acc:0.3655 ===\n",
      "train loss:1.6844625467334315\n",
      "train loss:1.7249225426536026\n",
      "train loss:1.6718562320393489\n",
      "=== epoch:231, train acc:0.46, test acc:0.3669 ===\n",
      "train loss:1.6936553601794415\n",
      "train loss:1.6732576869413143\n",
      "train loss:1.6700920079936585\n",
      "=== epoch:232, train acc:0.47, test acc:0.3693 ===\n",
      "train loss:1.7069089376050146\n",
      "train loss:1.6426499606066889\n",
      "train loss:1.7066663599229601\n",
      "=== epoch:233, train acc:0.47333333333333333, test acc:0.3691 ===\n",
      "train loss:1.771273630159515\n",
      "train loss:1.614496704301255\n",
      "train loss:1.6458138819353791\n",
      "=== epoch:234, train acc:0.4766666666666667, test acc:0.3702 ===\n",
      "train loss:1.67035171995119\n",
      "train loss:1.5786774987761771\n",
      "train loss:1.6844160550739389\n",
      "=== epoch:235, train acc:0.47333333333333333, test acc:0.3704 ===\n",
      "train loss:1.5976475902572762\n",
      "train loss:1.5587711620509574\n",
      "train loss:1.7239364609220564\n",
      "=== epoch:236, train acc:0.48, test acc:0.3705 ===\n",
      "train loss:1.6684090436431784\n",
      "train loss:1.6413669856788868\n",
      "train loss:1.7123740016513258\n",
      "=== epoch:237, train acc:0.4766666666666667, test acc:0.3699 ===\n",
      "train loss:1.5615145418393523\n",
      "train loss:1.7216632561189555\n",
      "train loss:1.7161852815015675\n",
      "=== epoch:238, train acc:0.4766666666666667, test acc:0.3718 ===\n",
      "train loss:1.6627858478016113\n",
      "train loss:1.6826778491855237\n",
      "train loss:1.749507264034717\n",
      "=== epoch:239, train acc:0.4866666666666667, test acc:0.375 ===\n",
      "train loss:1.673887849160597\n",
      "train loss:1.7472216504401938\n",
      "train loss:1.5905657798406496\n",
      "=== epoch:240, train acc:0.48, test acc:0.3772 ===\n",
      "train loss:1.6147894946329502\n",
      "train loss:1.77739887128187\n",
      "train loss:1.6830801696926065\n",
      "=== epoch:241, train acc:0.48333333333333334, test acc:0.3793 ===\n",
      "train loss:1.601933945286317\n",
      "train loss:1.7054430630268573\n",
      "train loss:1.6528731633917562\n",
      "=== epoch:242, train acc:0.4866666666666667, test acc:0.3792 ===\n",
      "train loss:1.5349687770157854\n",
      "train loss:1.7656771974451126\n",
      "train loss:1.5720964213596311\n",
      "=== epoch:243, train acc:0.48333333333333334, test acc:0.3787 ===\n",
      "train loss:1.5594733446690632\n",
      "train loss:1.7287762823826702\n",
      "train loss:1.6797695796069525\n",
      "=== epoch:244, train acc:0.49, test acc:0.3803 ===\n",
      "train loss:1.597147033307459\n",
      "train loss:1.556954940345536\n",
      "train loss:1.693031554181626\n",
      "=== epoch:245, train acc:0.49666666666666665, test acc:0.3833 ===\n",
      "train loss:1.6592485293964603\n",
      "train loss:1.6249104971251418\n",
      "train loss:1.73274959681348\n",
      "=== epoch:246, train acc:0.4866666666666667, test acc:0.3856 ===\n",
      "train loss:1.6638325221412709\n",
      "train loss:1.7182522661584452\n",
      "train loss:1.6875702537878317\n",
      "=== epoch:247, train acc:0.49, test acc:0.3885 ===\n",
      "train loss:1.5593179392365581\n",
      "train loss:1.5137303339679224\n",
      "train loss:1.6334407078510598\n",
      "=== epoch:248, train acc:0.5, test acc:0.3887 ===\n",
      "train loss:1.6566128754101195\n",
      "train loss:1.8026352918152961\n",
      "train loss:1.505275280769931\n",
      "=== epoch:249, train acc:0.5066666666666667, test acc:0.3908 ===\n",
      "train loss:1.6242474102168258\n",
      "train loss:1.740422118954349\n",
      "train loss:1.593861753714186\n",
      "=== epoch:250, train acc:0.5033333333333333, test acc:0.3939 ===\n",
      "train loss:1.6603850356668979\n",
      "train loss:1.5630427286081618\n",
      "train loss:1.7784027918526693\n",
      "=== epoch:251, train acc:0.5066666666666667, test acc:0.3941 ===\n",
      "train loss:1.5141620645242526\n",
      "train loss:1.607195756080349\n",
      "train loss:1.6025686982263239\n",
      "=== epoch:252, train acc:0.49333333333333335, test acc:0.3936 ===\n",
      "train loss:1.5569825580324486\n",
      "train loss:1.4987950253860414\n",
      "train loss:1.4835527266267408\n",
      "=== epoch:253, train acc:0.49, test acc:0.3937 ===\n",
      "train loss:1.6270582853161153\n",
      "train loss:1.5667463105188293\n",
      "train loss:1.493212158601273\n",
      "=== epoch:254, train acc:0.49333333333333335, test acc:0.395 ===\n",
      "train loss:1.6982727237934667\n",
      "train loss:1.5949022825546002\n",
      "train loss:1.5223950890291256\n",
      "=== epoch:255, train acc:0.49666666666666665, test acc:0.3966 ===\n",
      "train loss:1.5472879679229505\n",
      "train loss:1.696408338692918\n",
      "train loss:1.6153020241910954\n",
      "=== epoch:256, train acc:0.5, test acc:0.3982 ===\n",
      "train loss:1.6426277053955038\n",
      "train loss:1.6478404724637472\n",
      "train loss:1.5901652962152086\n",
      "=== epoch:257, train acc:0.5, test acc:0.3981 ===\n",
      "train loss:1.41213708910907\n",
      "train loss:1.563174396560421\n",
      "train loss:1.4341741480069519\n",
      "=== epoch:258, train acc:0.5, test acc:0.3987 ===\n",
      "train loss:1.5726422163194025\n",
      "train loss:1.496047542652372\n",
      "train loss:1.6171585277662863\n",
      "=== epoch:259, train acc:0.5, test acc:0.3994 ===\n",
      "train loss:1.551247405115605\n",
      "train loss:1.533466540941576\n",
      "train loss:1.5456549544745122\n",
      "=== epoch:260, train acc:0.5, test acc:0.4012 ===\n",
      "train loss:1.5853587293712865\n",
      "train loss:1.4033897675611824\n",
      "train loss:1.3357586823158163\n",
      "=== epoch:261, train acc:0.51, test acc:0.4027 ===\n",
      "train loss:1.6226798260910882\n",
      "train loss:1.540026498471814\n",
      "train loss:1.563678390786073\n",
      "=== epoch:262, train acc:0.5033333333333333, test acc:0.4033 ===\n",
      "train loss:1.526335634144766\n",
      "train loss:1.533353739915652\n",
      "train loss:1.4905099109635103\n",
      "=== epoch:263, train acc:0.51, test acc:0.4076 ===\n",
      "train loss:1.4167006194936465\n",
      "train loss:1.6162768682587967\n",
      "train loss:1.4299030013892406\n",
      "=== epoch:264, train acc:0.5066666666666667, test acc:0.4076 ===\n",
      "train loss:1.6263245441069916\n",
      "train loss:1.5964149039867126\n",
      "train loss:1.6150146265685763\n",
      "=== epoch:265, train acc:0.5166666666666667, test acc:0.4114 ===\n",
      "train loss:1.4315043805153624\n",
      "train loss:1.663708516830564\n",
      "train loss:1.4797299097164298\n",
      "=== epoch:266, train acc:0.5233333333333333, test acc:0.4133 ===\n",
      "train loss:1.5539126679834834\n",
      "train loss:1.6110493047491001\n",
      "train loss:1.6442414407237325\n",
      "=== epoch:267, train acc:0.5266666666666666, test acc:0.4149 ===\n",
      "train loss:1.448032967143636\n",
      "train loss:1.5179452608008699\n",
      "train loss:1.5985718074684834\n",
      "=== epoch:268, train acc:0.52, test acc:0.4145 ===\n",
      "train loss:1.6280265712855992\n",
      "train loss:1.471600034196801\n",
      "train loss:1.405607215280337\n",
      "=== epoch:269, train acc:0.5233333333333333, test acc:0.4164 ===\n",
      "train loss:1.6101285639636957\n",
      "train loss:1.6015439347848408\n",
      "train loss:1.636107372742463\n",
      "=== epoch:270, train acc:0.52, test acc:0.4184 ===\n",
      "train loss:1.462824456475422\n",
      "train loss:1.479160584821289\n",
      "train loss:1.48539027367192\n",
      "=== epoch:271, train acc:0.5166666666666667, test acc:0.4198 ===\n",
      "train loss:1.3729234753414503\n",
      "train loss:1.5061386678091782\n",
      "train loss:1.4863176756315928\n",
      "=== epoch:272, train acc:0.5166666666666667, test acc:0.4198 ===\n",
      "train loss:1.5012085012407403\n",
      "train loss:1.4906526549165995\n",
      "train loss:1.5336693422318333\n",
      "=== epoch:273, train acc:0.5266666666666666, test acc:0.422 ===\n",
      "train loss:1.464298395776259\n",
      "train loss:1.4738169271680124\n",
      "train loss:1.505256013520181\n",
      "=== epoch:274, train acc:0.5333333333333333, test acc:0.4211 ===\n",
      "train loss:1.59717833923269\n",
      "train loss:1.4929587596752774\n",
      "train loss:1.4972183983892384\n",
      "=== epoch:275, train acc:0.5333333333333333, test acc:0.4243 ===\n",
      "train loss:1.4602390311798803\n",
      "train loss:1.3409531665934942\n",
      "train loss:1.5978667500639603\n",
      "=== epoch:276, train acc:0.5366666666666666, test acc:0.4278 ===\n",
      "train loss:1.3509726348996687\n",
      "train loss:1.466017927010985\n",
      "train loss:1.458742326589173\n",
      "=== epoch:277, train acc:0.5333333333333333, test acc:0.4284 ===\n",
      "train loss:1.4811633964121018\n",
      "train loss:1.4599792219860581\n",
      "train loss:1.4961307979616538\n",
      "=== epoch:278, train acc:0.5333333333333333, test acc:0.4328 ===\n",
      "train loss:1.3365607357797602\n",
      "train loss:1.5214842231900558\n",
      "train loss:1.4615230343565406\n",
      "=== epoch:279, train acc:0.5366666666666666, test acc:0.4326 ===\n",
      "train loss:1.6134699227111708\n",
      "train loss:1.2692653186565883\n",
      "train loss:1.4386883797673036\n",
      "=== epoch:280, train acc:0.5366666666666666, test acc:0.4339 ===\n",
      "train loss:1.5136784896337514\n",
      "train loss:1.4531507976001934\n",
      "train loss:1.495621906627683\n",
      "=== epoch:281, train acc:0.5433333333333333, test acc:0.4354 ===\n",
      "train loss:1.4115398229797924\n",
      "train loss:1.3166753493834997\n",
      "train loss:1.513010153435141\n",
      "=== epoch:282, train acc:0.5433333333333333, test acc:0.439 ===\n",
      "train loss:1.3304566229115757\n",
      "train loss:1.4444332278319103\n",
      "train loss:1.5476569196232095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:283, train acc:0.5433333333333333, test acc:0.44 ===\n",
      "train loss:1.3728876569926347\n",
      "train loss:1.4765986162682028\n",
      "train loss:1.4677787665521533\n",
      "=== epoch:284, train acc:0.5466666666666666, test acc:0.4414 ===\n",
      "train loss:1.396648282278091\n",
      "train loss:1.5169986101285549\n",
      "train loss:1.4701804369330735\n",
      "=== epoch:285, train acc:0.55, test acc:0.4422 ===\n",
      "train loss:1.3756967903623707\n",
      "train loss:1.5085066306939816\n",
      "train loss:1.318065030953425\n",
      "=== epoch:286, train acc:0.5433333333333333, test acc:0.4415 ===\n",
      "train loss:1.450655869379826\n",
      "train loss:1.4213318189999864\n",
      "train loss:1.385577460873856\n",
      "=== epoch:287, train acc:0.55, test acc:0.4447 ===\n",
      "train loss:1.2687189168555681\n",
      "train loss:1.374879595364272\n",
      "train loss:1.5256631927939714\n",
      "=== epoch:288, train acc:0.5466666666666666, test acc:0.4459 ===\n",
      "train loss:1.4593473104328387\n",
      "train loss:1.3740088340657035\n",
      "train loss:1.2725565160144736\n",
      "=== epoch:289, train acc:0.5433333333333333, test acc:0.4466 ===\n",
      "train loss:1.419089952099241\n",
      "train loss:1.4223953193842787\n",
      "train loss:1.4922414981063759\n",
      "=== epoch:290, train acc:0.5466666666666666, test acc:0.4493 ===\n",
      "train loss:1.443161325943211\n",
      "train loss:1.3595072916290534\n",
      "train loss:1.29188502606797\n",
      "=== epoch:291, train acc:0.5466666666666666, test acc:0.4514 ===\n",
      "train loss:1.405349300628641\n",
      "train loss:1.423639610291423\n",
      "train loss:1.3794729451687102\n",
      "=== epoch:292, train acc:0.5566666666666666, test acc:0.4534 ===\n",
      "train loss:1.4033731848247502\n",
      "train loss:1.273017678508247\n",
      "train loss:1.3748344210880157\n",
      "=== epoch:293, train acc:0.5666666666666667, test acc:0.4556 ===\n",
      "train loss:1.4757825941902807\n",
      "train loss:1.3735201672946986\n",
      "train loss:1.4514903749279255\n",
      "=== epoch:294, train acc:0.57, test acc:0.4559 ===\n",
      "train loss:1.4406198515951\n",
      "train loss:1.3282540555050282\n",
      "train loss:1.481089206138367\n",
      "=== epoch:295, train acc:0.57, test acc:0.4623 ===\n",
      "train loss:1.2418974099126412\n",
      "train loss:1.3245569618286073\n",
      "train loss:1.2185708020499222\n",
      "=== epoch:296, train acc:0.57, test acc:0.4613 ===\n",
      "train loss:1.2793063521167587\n",
      "train loss:1.3336907884346747\n",
      "train loss:1.304662218069195\n",
      "=== epoch:297, train acc:0.5733333333333334, test acc:0.4652 ===\n",
      "train loss:1.2761066074925815\n",
      "train loss:1.2086134001428297\n",
      "train loss:1.3299910742757652\n",
      "=== epoch:298, train acc:0.5733333333333334, test acc:0.4624 ===\n",
      "train loss:1.4012169442780789\n",
      "train loss:1.3989234420567802\n",
      "train loss:1.3577954911071553\n",
      "=== epoch:299, train acc:0.5766666666666667, test acc:0.4647 ===\n",
      "train loss:1.209233744859002\n",
      "train loss:1.4883219353436532\n",
      "train loss:1.356430328460916\n",
      "=== epoch:300, train acc:0.58, test acc:0.4679 ===\n",
      "train loss:1.2716908981747221\n",
      "train loss:1.2932340317025917\n",
      "train loss:1.2772602297241873\n",
      "=== epoch:301, train acc:0.5766666666666667, test acc:0.4664 ===\n",
      "train loss:1.2236578191260385\n",
      "train loss:1.3400064366334834\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.4682\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.313170695129644\n",
      "=== epoch:1, train acc:0.09666666666666666, test acc:0.0997 ===\n",
      "train loss:2.3460776203458673\n",
      "train loss:2.327002475775741\n",
      "train loss:2.312442150437667\n",
      "=== epoch:2, train acc:0.09666666666666666, test acc:0.0988 ===\n",
      "train loss:2.3252670244491296\n",
      "train loss:2.3156612686326894\n",
      "train loss:2.298222504737302\n",
      "=== epoch:3, train acc:0.1, test acc:0.0982 ===\n",
      "train loss:2.318149274350587\n",
      "train loss:2.308595620184753\n",
      "train loss:2.304282467881457\n",
      "=== epoch:4, train acc:0.11, test acc:0.0975 ===\n",
      "train loss:2.311098263547546\n",
      "train loss:2.3023191130627554\n",
      "train loss:2.308292743063557\n",
      "=== epoch:5, train acc:0.11, test acc:0.0954 ===\n",
      "train loss:2.3139201091768684\n",
      "train loss:2.3172148758480633\n",
      "train loss:2.3009290664098074\n",
      "=== epoch:6, train acc:0.11666666666666667, test acc:0.096 ===\n",
      "train loss:2.2908828797441183\n",
      "train loss:2.2961444028376685\n",
      "train loss:2.2891106648149786\n",
      "=== epoch:7, train acc:0.11666666666666667, test acc:0.0953 ===\n",
      "train loss:2.273354090372607\n",
      "train loss:2.28148345053265\n",
      "train loss:2.2890244924948204\n",
      "=== epoch:8, train acc:0.12333333333333334, test acc:0.0979 ===\n",
      "train loss:2.281168566296154\n",
      "train loss:2.284284418105501\n",
      "train loss:2.2937926122652086\n",
      "=== epoch:9, train acc:0.13, test acc:0.1014 ===\n",
      "train loss:2.2842692405491247\n",
      "train loss:2.2504298814014847\n",
      "train loss:2.272156090927267\n",
      "=== epoch:10, train acc:0.14, test acc:0.1038 ===\n",
      "train loss:2.284604298652335\n",
      "train loss:2.3030943583781007\n",
      "train loss:2.276882289526898\n",
      "=== epoch:11, train acc:0.15333333333333332, test acc:0.1072 ===\n",
      "train loss:2.2924716191497496\n",
      "train loss:2.269890403710877\n",
      "train loss:2.2666317980692883\n",
      "=== epoch:12, train acc:0.16, test acc:0.1123 ===\n",
      "train loss:2.267312639873856\n",
      "train loss:2.2775966170465005\n",
      "train loss:2.265719332840277\n",
      "=== epoch:13, train acc:0.17, test acc:0.1159 ===\n",
      "train loss:2.281529563854632\n",
      "train loss:2.2712746622981173\n",
      "train loss:2.2842692056947813\n",
      "=== epoch:14, train acc:0.17, test acc:0.125 ===\n",
      "train loss:2.2567288152893874\n",
      "train loss:2.2707951065773546\n",
      "train loss:2.2779319778981257\n",
      "=== epoch:15, train acc:0.17333333333333334, test acc:0.129 ===\n",
      "train loss:2.274675608144035\n",
      "train loss:2.2669392729537408\n",
      "train loss:2.2707246238206285\n",
      "=== epoch:16, train acc:0.18666666666666668, test acc:0.1418 ===\n",
      "train loss:2.273372126972535\n",
      "train loss:2.2589910169160787\n",
      "train loss:2.2624738928800974\n",
      "=== epoch:17, train acc:0.2, test acc:0.1469 ===\n",
      "train loss:2.2624353919048725\n",
      "train loss:2.2608217015062984\n",
      "train loss:2.2552792604007066\n",
      "=== epoch:18, train acc:0.20666666666666667, test acc:0.1495 ===\n",
      "train loss:2.252059231983007\n",
      "train loss:2.252411394930529\n",
      "train loss:2.265565073715935\n",
      "=== epoch:19, train acc:0.22666666666666666, test acc:0.1579 ===\n",
      "train loss:2.263410545709854\n",
      "train loss:2.2485702832952152\n",
      "train loss:2.254184351352224\n",
      "=== epoch:20, train acc:0.22, test acc:0.1659 ===\n",
      "train loss:2.2594974045469205\n",
      "train loss:2.265043855749035\n",
      "train loss:2.2467805516103954\n",
      "=== epoch:21, train acc:0.22666666666666666, test acc:0.1757 ===\n",
      "train loss:2.2471298624469456\n",
      "train loss:2.2386280457070544\n",
      "train loss:2.251021311910578\n",
      "=== epoch:22, train acc:0.22333333333333333, test acc:0.1803 ===\n",
      "train loss:2.262393834135206\n",
      "train loss:2.245887851223105\n",
      "train loss:2.255594970038977\n",
      "=== epoch:23, train acc:0.22666666666666666, test acc:0.1897 ===\n",
      "train loss:2.24458859769682\n",
      "train loss:2.254715619246883\n",
      "train loss:2.2587075233089253\n",
      "=== epoch:24, train acc:0.24666666666666667, test acc:0.2004 ===\n",
      "train loss:2.2488353269398313\n",
      "train loss:2.245871660733764\n",
      "train loss:2.2531378745498984\n",
      "=== epoch:25, train acc:0.23666666666666666, test acc:0.2064 ===\n",
      "train loss:2.2435506505963567\n",
      "train loss:2.240683442065441\n",
      "train loss:2.2425053872378915\n",
      "=== epoch:26, train acc:0.25, test acc:0.2052 ===\n",
      "train loss:2.2436940801888006\n",
      "train loss:2.2471062215784725\n",
      "train loss:2.240969250949806\n",
      "=== epoch:27, train acc:0.27, test acc:0.2177 ===\n",
      "train loss:2.2298984385188714\n",
      "train loss:2.2393985750908927\n",
      "train loss:2.2405317189887386\n",
      "=== epoch:28, train acc:0.27666666666666667, test acc:0.2172 ===\n",
      "train loss:2.242880048052181\n",
      "train loss:2.2314145381572428\n",
      "train loss:2.234123197150119\n",
      "=== epoch:29, train acc:0.29, test acc:0.2311 ===\n",
      "train loss:2.239472849205841\n",
      "train loss:2.2207812354456693\n",
      "train loss:2.2228391819384203\n",
      "=== epoch:30, train acc:0.30333333333333334, test acc:0.2374 ===\n",
      "train loss:2.2237618085936646\n",
      "train loss:2.2417458782409327\n",
      "train loss:2.239424512466447\n",
      "=== epoch:31, train acc:0.30666666666666664, test acc:0.2422 ===\n",
      "train loss:2.2330875727524298\n",
      "train loss:2.242457951566198\n",
      "train loss:2.2252487719184884\n",
      "=== epoch:32, train acc:0.31333333333333335, test acc:0.2492 ===\n",
      "train loss:2.21318307535546\n",
      "train loss:2.2343348312878035\n",
      "train loss:2.2386603856673033\n",
      "=== epoch:33, train acc:0.32666666666666666, test acc:0.2537 ===\n",
      "train loss:2.209557224225434\n",
      "train loss:2.2290090250279477\n",
      "train loss:2.213285138077825\n",
      "=== epoch:34, train acc:0.33, test acc:0.2553 ===\n",
      "train loss:2.2167605439299707\n",
      "train loss:2.210771508068041\n",
      "train loss:2.2205123459959046\n",
      "=== epoch:35, train acc:0.3233333333333333, test acc:0.2558 ===\n",
      "train loss:2.222856575416282\n",
      "train loss:2.2401775162605846\n",
      "train loss:2.2212187748663688\n",
      "=== epoch:36, train acc:0.3433333333333333, test acc:0.2584 ===\n",
      "train loss:2.235222318838737\n",
      "train loss:2.232672478263511\n",
      "train loss:2.2203057571819262\n",
      "=== epoch:37, train acc:0.34, test acc:0.2633 ===\n",
      "train loss:2.242187585918617\n",
      "train loss:2.2209765803029793\n",
      "train loss:2.1963033371600575\n",
      "=== epoch:38, train acc:0.3466666666666667, test acc:0.2681 ===\n",
      "train loss:2.209974794549734\n",
      "train loss:2.191931124385698\n",
      "train loss:2.2086885407009382\n",
      "=== epoch:39, train acc:0.35, test acc:0.2683 ===\n",
      "train loss:2.203943578526034\n",
      "train loss:2.1956711593517593\n",
      "train loss:2.2132306701174596\n",
      "=== epoch:40, train acc:0.36, test acc:0.2696 ===\n",
      "train loss:2.1974846659015257\n",
      "train loss:2.2134054790680664\n",
      "train loss:2.1952269692218356\n",
      "=== epoch:41, train acc:0.37333333333333335, test acc:0.2721 ===\n",
      "train loss:2.193334195751972\n",
      "train loss:2.1997706728020914\n",
      "train loss:2.2062298196684282\n",
      "=== epoch:42, train acc:0.37666666666666665, test acc:0.2801 ===\n",
      "train loss:2.2192903559955544\n",
      "train loss:2.1975761394483957\n",
      "train loss:2.179558778547738\n",
      "=== epoch:43, train acc:0.4, test acc:0.2843 ===\n",
      "train loss:2.186600517535293\n",
      "train loss:2.197727008625979\n",
      "train loss:2.182656637125365\n",
      "=== epoch:44, train acc:0.39666666666666667, test acc:0.2874 ===\n",
      "train loss:2.2157622235633623\n",
      "train loss:2.1939734108463673\n",
      "train loss:2.189299103983128\n",
      "=== epoch:45, train acc:0.4066666666666667, test acc:0.2972 ===\n",
      "train loss:2.2016792853130394\n",
      "train loss:2.1815496606225504\n",
      "train loss:2.197918005398535\n",
      "=== epoch:46, train acc:0.42, test acc:0.3048 ===\n",
      "train loss:2.1972745261839646\n",
      "train loss:2.1778370608220516\n",
      "train loss:2.2005784564306587\n",
      "=== epoch:47, train acc:0.42, test acc:0.3111 ===\n",
      "train loss:2.2094194335724624\n",
      "train loss:2.1995528532739606\n",
      "train loss:2.1994480895867894\n",
      "=== epoch:48, train acc:0.42, test acc:0.3132 ===\n",
      "train loss:2.1670907190641553\n",
      "train loss:2.1989261758614767\n",
      "train loss:2.2132253657440226\n",
      "=== epoch:49, train acc:0.42333333333333334, test acc:0.3118 ===\n",
      "train loss:2.19556589075035\n",
      "train loss:2.1813483919998062\n",
      "train loss:2.1922869673392147\n",
      "=== epoch:50, train acc:0.43, test acc:0.3143 ===\n",
      "train loss:2.1828546564566933\n",
      "train loss:2.1914857352586616\n",
      "train loss:2.1946148788607265\n",
      "=== epoch:51, train acc:0.43, test acc:0.318 ===\n",
      "train loss:2.1777077872155326\n",
      "train loss:2.1951052501930994\n",
      "train loss:2.1621674774053354\n",
      "=== epoch:52, train acc:0.43, test acc:0.3198 ===\n",
      "train loss:2.1604226213554067\n",
      "train loss:2.1744603522447763\n",
      "train loss:2.1829581005812715\n",
      "=== epoch:53, train acc:0.43, test acc:0.3235 ===\n",
      "train loss:2.160045458739342\n",
      "train loss:2.194410986758763\n",
      "train loss:2.1799368412323643\n",
      "=== epoch:54, train acc:0.4266666666666667, test acc:0.3219 ===\n",
      "train loss:2.1664942736208257\n",
      "train loss:2.170847784652905\n",
      "train loss:2.144834240119892\n",
      "=== epoch:55, train acc:0.4266666666666667, test acc:0.3215 ===\n",
      "train loss:2.1457026609038925\n",
      "train loss:2.1800743524025936\n",
      "train loss:2.1709428842648535\n",
      "=== epoch:56, train acc:0.43666666666666665, test acc:0.3265 ===\n",
      "train loss:2.1548538731811715\n",
      "train loss:2.1586617435504887\n",
      "train loss:2.1589453404030583\n",
      "=== epoch:57, train acc:0.44, test acc:0.3253 ===\n",
      "train loss:2.1666889527538453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.1611836094501147\n",
      "train loss:2.1815024851264138\n",
      "=== epoch:58, train acc:0.45, test acc:0.3349 ===\n",
      "train loss:2.15672724202975\n",
      "train loss:2.1433430129652895\n",
      "train loss:2.1579150365124846\n",
      "=== epoch:59, train acc:0.4533333333333333, test acc:0.3374 ===\n",
      "train loss:2.1196204339696116\n",
      "train loss:2.1533057255079893\n",
      "train loss:2.1469221821615463\n",
      "=== epoch:60, train acc:0.45, test acc:0.3372 ===\n",
      "train loss:2.162178209623976\n",
      "train loss:2.1637152337593726\n",
      "train loss:2.139859914354688\n",
      "=== epoch:61, train acc:0.4533333333333333, test acc:0.3426 ===\n",
      "train loss:2.130254116092813\n",
      "train loss:2.180869176342039\n",
      "train loss:2.137654153568942\n",
      "=== epoch:62, train acc:0.4666666666666667, test acc:0.3501 ===\n",
      "train loss:2.1309570994037297\n",
      "train loss:2.1463165601937093\n",
      "train loss:2.13045441646919\n",
      "=== epoch:63, train acc:0.47, test acc:0.3504 ===\n",
      "train loss:2.115457294364634\n",
      "train loss:2.113332810972041\n",
      "train loss:2.1691811775751226\n",
      "=== epoch:64, train acc:0.47, test acc:0.3564 ===\n",
      "train loss:2.14564110279346\n",
      "train loss:2.1420681002927626\n",
      "train loss:2.1065898001215073\n",
      "=== epoch:65, train acc:0.4766666666666667, test acc:0.3611 ===\n",
      "train loss:2.1305756055113996\n",
      "train loss:2.1701014604910025\n",
      "train loss:2.1483218426701662\n",
      "=== epoch:66, train acc:0.49333333333333335, test acc:0.3727 ===\n",
      "train loss:2.128259174549107\n",
      "train loss:2.1377185323168186\n",
      "train loss:2.1674322712456493\n",
      "=== epoch:67, train acc:0.49, test acc:0.3782 ===\n",
      "train loss:2.1601292551786413\n",
      "train loss:2.1117285952017153\n",
      "train loss:2.0971924592390763\n",
      "=== epoch:68, train acc:0.49333333333333335, test acc:0.3798 ===\n",
      "train loss:2.086427658441003\n",
      "train loss:2.1275526817391235\n",
      "train loss:2.1052275664905924\n",
      "=== epoch:69, train acc:0.5033333333333333, test acc:0.3786 ===\n",
      "train loss:2.0990152041151693\n",
      "train loss:2.1182129590989205\n",
      "train loss:2.1407389808953226\n",
      "=== epoch:70, train acc:0.49, test acc:0.3789 ===\n",
      "train loss:2.1099885897260946\n",
      "train loss:2.0728013612444656\n",
      "train loss:2.1084363831830264\n",
      "=== epoch:71, train acc:0.5033333333333333, test acc:0.3894 ===\n",
      "train loss:2.1106752205736057\n",
      "train loss:2.1030387530889683\n",
      "train loss:2.07313305299761\n",
      "=== epoch:72, train acc:0.5066666666666667, test acc:0.3963 ===\n",
      "train loss:2.125697353600018\n",
      "train loss:2.1029304665300974\n",
      "train loss:2.0614892347858187\n",
      "=== epoch:73, train acc:0.5, test acc:0.3945 ===\n",
      "train loss:2.083915754836791\n",
      "train loss:2.0457712003035526\n",
      "train loss:2.048597679472463\n",
      "=== epoch:74, train acc:0.49, test acc:0.3934 ===\n",
      "train loss:2.0674180386378676\n",
      "train loss:2.0740205013313124\n",
      "train loss:2.0873314393844713\n",
      "=== epoch:75, train acc:0.49, test acc:0.3866 ===\n",
      "train loss:2.106563931526818\n",
      "train loss:2.070989488177948\n",
      "train loss:2.075812233072925\n",
      "=== epoch:76, train acc:0.49, test acc:0.3876 ===\n",
      "train loss:2.0530368939376022\n",
      "train loss:2.04731854487208\n",
      "train loss:2.073368692566862\n",
      "=== epoch:77, train acc:0.5066666666666667, test acc:0.3931 ===\n",
      "train loss:2.098287507124343\n",
      "train loss:2.0660373657844877\n",
      "train loss:2.0689887540429113\n",
      "=== epoch:78, train acc:0.5, test acc:0.3968 ===\n",
      "train loss:2.0235430897333675\n",
      "train loss:2.098355453227642\n",
      "train loss:2.0817768945153223\n",
      "=== epoch:79, train acc:0.51, test acc:0.4043 ===\n",
      "train loss:2.0829891512078214\n",
      "train loss:2.048281991572253\n",
      "train loss:2.042067931717789\n",
      "=== epoch:80, train acc:0.5033333333333333, test acc:0.4052 ===\n",
      "train loss:2.0793877835770678\n",
      "train loss:2.023740258697067\n",
      "train loss:2.0593817425172727\n",
      "=== epoch:81, train acc:0.5, test acc:0.4035 ===\n",
      "train loss:2.073710963669441\n",
      "train loss:2.090759680694004\n",
      "train loss:2.0331651954998886\n",
      "=== epoch:82, train acc:0.5, test acc:0.4032 ===\n",
      "train loss:2.0380025454631276\n",
      "train loss:2.0383056186468855\n",
      "train loss:2.0185523434740373\n",
      "=== epoch:83, train acc:0.5033333333333333, test acc:0.3996 ===\n",
      "train loss:2.0453470943468646\n",
      "train loss:2.0540586289209735\n",
      "train loss:1.9990527207483857\n",
      "=== epoch:84, train acc:0.49333333333333335, test acc:0.3919 ===\n",
      "train loss:1.978036109238179\n",
      "train loss:2.0154304603374325\n",
      "train loss:2.0306434140636487\n",
      "=== epoch:85, train acc:0.49666666666666665, test acc:0.3964 ===\n",
      "train loss:1.9980068918822826\n",
      "train loss:2.041983740601549\n",
      "train loss:2.0485701703109402\n",
      "=== epoch:86, train acc:0.49666666666666665, test acc:0.3982 ===\n",
      "train loss:2.0382097315205443\n",
      "train loss:2.047662494017873\n",
      "train loss:2.0211212671633314\n",
      "=== epoch:87, train acc:0.49666666666666665, test acc:0.4046 ===\n",
      "train loss:2.013784370729912\n",
      "train loss:2.0329170408087385\n",
      "train loss:1.9961354387719512\n",
      "=== epoch:88, train acc:0.5066666666666667, test acc:0.4218 ===\n",
      "train loss:1.9323357582554168\n",
      "train loss:2.0327331252364838\n",
      "train loss:2.0283976254816243\n",
      "=== epoch:89, train acc:0.5166666666666667, test acc:0.4271 ===\n",
      "train loss:1.993043085331435\n",
      "train loss:2.003605965217622\n",
      "train loss:1.972225162434976\n",
      "=== epoch:90, train acc:0.5266666666666666, test acc:0.4306 ===\n",
      "train loss:1.9750898653947013\n",
      "train loss:1.9487071507401945\n",
      "train loss:1.9492799432491392\n",
      "=== epoch:91, train acc:0.5133333333333333, test acc:0.4228 ===\n",
      "train loss:1.9983215072410392\n",
      "train loss:1.9948171797161687\n",
      "train loss:1.9659317958210765\n",
      "=== epoch:92, train acc:0.5133333333333333, test acc:0.4231 ===\n",
      "train loss:1.9761743658978017\n",
      "train loss:1.977926197064351\n",
      "train loss:2.0305464586376827\n",
      "=== epoch:93, train acc:0.52, test acc:0.4221 ===\n",
      "train loss:2.0242554726187882\n",
      "train loss:1.9659551166189901\n",
      "train loss:1.9268455545297616\n",
      "=== epoch:94, train acc:0.5233333333333333, test acc:0.4214 ===\n",
      "train loss:1.9366785004812395\n",
      "train loss:1.9810477977413379\n",
      "train loss:1.9813448093015464\n",
      "=== epoch:95, train acc:0.5033333333333333, test acc:0.4225 ===\n",
      "train loss:1.9640942298277293\n",
      "train loss:1.9364647503501464\n",
      "train loss:1.9287084089596371\n",
      "=== epoch:96, train acc:0.51, test acc:0.4259 ===\n",
      "train loss:1.8817440921826667\n",
      "train loss:1.9459986106756009\n",
      "train loss:1.9459710223663513\n",
      "=== epoch:97, train acc:0.51, test acc:0.4327 ===\n",
      "train loss:1.9452799469524726\n",
      "train loss:1.854705441682515\n",
      "train loss:1.9380519417126365\n",
      "=== epoch:98, train acc:0.5133333333333333, test acc:0.4357 ===\n",
      "train loss:1.9428254145795218\n",
      "train loss:1.907074175328084\n",
      "train loss:1.8797992618492814\n",
      "=== epoch:99, train acc:0.5233333333333333, test acc:0.4423 ===\n",
      "train loss:1.872585074785191\n",
      "train loss:1.8344011110590546\n",
      "train loss:1.8529487008761956\n",
      "=== epoch:100, train acc:0.5133333333333333, test acc:0.4382 ===\n",
      "train loss:1.896461372216725\n",
      "train loss:1.8710899828425231\n",
      "train loss:1.872540213391597\n",
      "=== epoch:101, train acc:0.5266666666666666, test acc:0.4444 ===\n",
      "train loss:1.9024473266052775\n",
      "train loss:1.8887094058396454\n",
      "train loss:1.9518346372559803\n",
      "=== epoch:102, train acc:0.5266666666666666, test acc:0.451 ===\n",
      "train loss:1.8726303946930971\n",
      "train loss:1.9259704622927756\n",
      "train loss:1.8161041668794047\n",
      "=== epoch:103, train acc:0.5466666666666666, test acc:0.4593 ===\n",
      "train loss:1.9075360812002677\n",
      "train loss:1.8857856485543425\n",
      "train loss:1.859283667966141\n",
      "=== epoch:104, train acc:0.5366666666666666, test acc:0.4502 ===\n",
      "train loss:1.7719530250332236\n",
      "train loss:1.8820398595239254\n",
      "train loss:1.838807488405823\n",
      "=== epoch:105, train acc:0.54, test acc:0.4525 ===\n",
      "train loss:1.878456432731788\n",
      "train loss:1.8621024685868353\n",
      "train loss:1.8273225813282234\n",
      "=== epoch:106, train acc:0.55, test acc:0.4572 ===\n",
      "train loss:1.8415349688466067\n",
      "train loss:1.887341452676644\n",
      "train loss:1.8612206073445086\n",
      "=== epoch:107, train acc:0.5633333333333334, test acc:0.4556 ===\n",
      "train loss:1.8659796479259336\n",
      "train loss:1.7422399072274186\n",
      "train loss:1.7940405939106439\n",
      "=== epoch:108, train acc:0.55, test acc:0.4524 ===\n",
      "train loss:1.7521076299212057\n",
      "train loss:1.742566105310776\n",
      "train loss:1.7941811136664663\n",
      "=== epoch:109, train acc:0.55, test acc:0.4614 ===\n",
      "train loss:1.7472837277290147\n",
      "train loss:1.7897349992236788\n",
      "train loss:1.6980762142538226\n",
      "=== epoch:110, train acc:0.56, test acc:0.463 ===\n",
      "train loss:1.8449783837606781\n",
      "train loss:1.8481600833659197\n",
      "train loss:1.7490555558917933\n",
      "=== epoch:111, train acc:0.5733333333333334, test acc:0.4684 ===\n",
      "train loss:1.7873729180980678\n",
      "train loss:1.8016262400681287\n",
      "train loss:1.8549957881081343\n",
      "=== epoch:112, train acc:0.57, test acc:0.4674 ===\n",
      "train loss:1.84507222154387\n",
      "train loss:1.8413524212015913\n",
      "train loss:1.7404572731477717\n",
      "=== epoch:113, train acc:0.5733333333333334, test acc:0.4661 ===\n",
      "train loss:1.7468875177702696\n",
      "train loss:1.8461038807401184\n",
      "train loss:1.7592833886739823\n",
      "=== epoch:114, train acc:0.5766666666666667, test acc:0.4703 ===\n",
      "train loss:1.764549684340565\n",
      "train loss:1.80974784259931\n",
      "train loss:1.6700166985590612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:115, train acc:0.58, test acc:0.4656 ===\n",
      "train loss:1.7522202282144113\n",
      "train loss:1.7387882263405114\n",
      "train loss:1.68948083317269\n",
      "=== epoch:116, train acc:0.5866666666666667, test acc:0.4694 ===\n",
      "train loss:1.8488907120331635\n",
      "train loss:1.7337067089174267\n",
      "train loss:1.7483579209606086\n",
      "=== epoch:117, train acc:0.5866666666666667, test acc:0.4711 ===\n",
      "train loss:1.71516024954281\n",
      "train loss:1.813138147239407\n",
      "train loss:1.7274619930631296\n",
      "=== epoch:118, train acc:0.5866666666666667, test acc:0.4756 ===\n",
      "train loss:1.7446497920554882\n",
      "train loss:1.790547404897889\n",
      "train loss:1.7382267205484336\n",
      "=== epoch:119, train acc:0.5666666666666667, test acc:0.4744 ===\n",
      "train loss:1.745831220026402\n",
      "train loss:1.6504505950010546\n",
      "train loss:1.7381349507514667\n",
      "=== epoch:120, train acc:0.5833333333333334, test acc:0.478 ===\n",
      "train loss:1.568156533263626\n",
      "train loss:1.7743209712563401\n",
      "train loss:1.7514898830709698\n",
      "=== epoch:121, train acc:0.5866666666666667, test acc:0.4754 ===\n",
      "train loss:1.7170871997799795\n",
      "train loss:1.7165044590002725\n",
      "train loss:1.5439914808470747\n",
      "=== epoch:122, train acc:0.5866666666666667, test acc:0.4788 ===\n",
      "train loss:1.6497513402499058\n",
      "train loss:1.7319966880729034\n",
      "train loss:1.6174949755594807\n",
      "=== epoch:123, train acc:0.58, test acc:0.4736 ===\n",
      "train loss:1.6392539089267035\n",
      "train loss:1.6913907394239578\n",
      "train loss:1.6296734758086695\n",
      "=== epoch:124, train acc:0.5866666666666667, test acc:0.4775 ===\n",
      "train loss:1.7083123631541608\n",
      "train loss:1.5785987500726575\n",
      "train loss:1.6528891270035273\n",
      "=== epoch:125, train acc:0.5933333333333334, test acc:0.4839 ===\n",
      "train loss:1.6200723146789844\n",
      "train loss:1.578851722365337\n",
      "train loss:1.5839865133690836\n",
      "=== epoch:126, train acc:0.5866666666666667, test acc:0.4839 ===\n",
      "train loss:1.6699911714576041\n",
      "train loss:1.7242990881546465\n",
      "train loss:1.7001936394706658\n",
      "=== epoch:127, train acc:0.5966666666666667, test acc:0.4878 ===\n",
      "train loss:1.6348131676053108\n",
      "train loss:1.5721913709301014\n",
      "train loss:1.6112561010146649\n",
      "=== epoch:128, train acc:0.6033333333333334, test acc:0.4927 ===\n",
      "train loss:1.6382525892883577\n",
      "train loss:1.6745137947804418\n",
      "train loss:1.765170416795356\n",
      "=== epoch:129, train acc:0.6, test acc:0.4928 ===\n",
      "train loss:1.5743936848961608\n",
      "train loss:1.6394048448259841\n",
      "train loss:1.5119783462745267\n",
      "=== epoch:130, train acc:0.6033333333333334, test acc:0.4975 ===\n",
      "train loss:1.4847925592575404\n",
      "train loss:1.497015708663715\n",
      "train loss:1.5608779342063743\n",
      "=== epoch:131, train acc:0.61, test acc:0.4967 ===\n",
      "train loss:1.6261068009361193\n",
      "train loss:1.5501890096106983\n",
      "train loss:1.430696706329543\n",
      "=== epoch:132, train acc:0.6133333333333333, test acc:0.5003 ===\n",
      "train loss:1.521300055743295\n",
      "train loss:1.6041024323370792\n",
      "train loss:1.5223908396363088\n",
      "=== epoch:133, train acc:0.62, test acc:0.5042 ===\n",
      "train loss:1.5395728205195258\n",
      "train loss:1.5232729129476725\n",
      "train loss:1.5307985206742254\n",
      "=== epoch:134, train acc:0.62, test acc:0.4999 ===\n",
      "train loss:1.540759770535508\n",
      "train loss:1.6386266962808549\n",
      "train loss:1.6076275838192133\n",
      "=== epoch:135, train acc:0.62, test acc:0.5039 ===\n",
      "train loss:1.554102720829965\n",
      "train loss:1.598169038945731\n",
      "train loss:1.5117005553166163\n",
      "=== epoch:136, train acc:0.6233333333333333, test acc:0.5013 ===\n",
      "train loss:1.5242622953473421\n",
      "train loss:1.5120760767156063\n",
      "train loss:1.5570693160971658\n",
      "=== epoch:137, train acc:0.6233333333333333, test acc:0.5051 ===\n",
      "train loss:1.625545690327569\n",
      "train loss:1.605185430617107\n",
      "train loss:1.5228843039602302\n",
      "=== epoch:138, train acc:0.6233333333333333, test acc:0.5068 ===\n",
      "train loss:1.5338147854238042\n",
      "train loss:1.4510902266025667\n",
      "train loss:1.492695315547266\n",
      "=== epoch:139, train acc:0.63, test acc:0.5104 ===\n",
      "train loss:1.4361462830028153\n",
      "train loss:1.377753124305824\n",
      "train loss:1.4488427831210355\n",
      "=== epoch:140, train acc:0.63, test acc:0.5172 ===\n",
      "train loss:1.444364073560778\n",
      "train loss:1.512403735920392\n",
      "train loss:1.4292511671653927\n",
      "=== epoch:141, train acc:0.63, test acc:0.5216 ===\n",
      "train loss:1.4076796103112812\n",
      "train loss:1.5240675003692048\n",
      "train loss:1.5167103043866792\n",
      "=== epoch:142, train acc:0.63, test acc:0.5217 ===\n",
      "train loss:1.4695724540328834\n",
      "train loss:1.5202014434262492\n",
      "train loss:1.4968436290754317\n",
      "=== epoch:143, train acc:0.63, test acc:0.5203 ===\n",
      "train loss:1.4002689015511676\n",
      "train loss:1.4080022606526226\n",
      "train loss:1.442630643719503\n",
      "=== epoch:144, train acc:0.6333333333333333, test acc:0.5252 ===\n",
      "train loss:1.437075187489719\n",
      "train loss:1.5427698400592809\n",
      "train loss:1.4186572118269345\n",
      "=== epoch:145, train acc:0.63, test acc:0.5249 ===\n",
      "train loss:1.3617043279705785\n",
      "train loss:1.4177624788347172\n",
      "train loss:1.3435861084940288\n",
      "=== epoch:146, train acc:0.63, test acc:0.5217 ===\n",
      "train loss:1.3451250415525176\n",
      "train loss:1.3653464291764084\n",
      "train loss:1.2968810347452058\n",
      "=== epoch:147, train acc:0.6333333333333333, test acc:0.5234 ===\n",
      "train loss:1.5429556327045726\n",
      "train loss:1.4709389974127516\n",
      "train loss:1.3699156353709292\n",
      "=== epoch:148, train acc:0.64, test acc:0.5301 ===\n",
      "train loss:1.3016252672755877\n",
      "train loss:1.37799638475152\n",
      "train loss:1.345347028786391\n",
      "=== epoch:149, train acc:0.64, test acc:0.5314 ===\n",
      "train loss:1.3682646454090142\n",
      "train loss:1.3192696480634893\n",
      "train loss:1.290431775252101\n",
      "=== epoch:150, train acc:0.64, test acc:0.5332 ===\n",
      "train loss:1.2540694919361428\n",
      "train loss:1.4554460471991204\n",
      "train loss:1.370385332236623\n",
      "=== epoch:151, train acc:0.6366666666666667, test acc:0.5386 ===\n",
      "train loss:1.2993096848147232\n",
      "train loss:1.3558703975056488\n",
      "train loss:1.3692693639567377\n",
      "=== epoch:152, train acc:0.64, test acc:0.5411 ===\n",
      "train loss:1.4416966582198456\n",
      "train loss:1.3865507970492235\n",
      "train loss:1.3401801610676725\n",
      "=== epoch:153, train acc:0.65, test acc:0.545 ===\n",
      "train loss:1.2647368020733376\n",
      "train loss:1.410279246510007\n",
      "train loss:1.2937470595160763\n",
      "=== epoch:154, train acc:0.65, test acc:0.5444 ===\n",
      "train loss:1.248534681266731\n",
      "train loss:1.289503543617978\n",
      "train loss:1.3617011561530006\n",
      "=== epoch:155, train acc:0.6633333333333333, test acc:0.5462 ===\n",
      "train loss:1.2788886613451609\n",
      "train loss:1.3468901386519274\n",
      "train loss:1.288775411355591\n",
      "=== epoch:156, train acc:0.6666666666666666, test acc:0.5507 ===\n",
      "train loss:1.221473450937699\n",
      "train loss:1.3412098029828718\n",
      "train loss:1.2513776320580907\n",
      "=== epoch:157, train acc:0.6733333333333333, test acc:0.5488 ===\n",
      "train loss:1.3640062383889802\n",
      "train loss:1.2118300088188858\n",
      "train loss:1.3428853592075136\n",
      "=== epoch:158, train acc:0.6833333333333333, test acc:0.5563 ===\n",
      "train loss:1.2876919286753143\n",
      "train loss:1.3507654494078756\n",
      "train loss:1.2112193547847492\n",
      "=== epoch:159, train acc:0.69, test acc:0.5558 ===\n",
      "train loss:1.21685448289136\n",
      "train loss:1.325709268909109\n",
      "train loss:1.30971296788445\n",
      "=== epoch:160, train acc:0.6766666666666666, test acc:0.5543 ===\n",
      "train loss:1.1013267793923078\n",
      "train loss:1.3425042844961943\n",
      "train loss:1.2414118659089712\n",
      "=== epoch:161, train acc:0.6666666666666666, test acc:0.5494 ===\n",
      "train loss:1.1905475846595237\n",
      "train loss:1.3448137789741552\n",
      "train loss:1.2899496076946813\n",
      "=== epoch:162, train acc:0.6666666666666666, test acc:0.5526 ===\n",
      "train loss:1.0952396436221825\n",
      "train loss:1.2372942547014503\n",
      "train loss:1.2437172531679357\n",
      "=== epoch:163, train acc:0.68, test acc:0.5579 ===\n",
      "train loss:1.0959186702706711\n",
      "train loss:1.1503908704870809\n",
      "train loss:1.225377816084994\n",
      "=== epoch:164, train acc:0.69, test acc:0.5599 ===\n",
      "train loss:1.2523676516183015\n",
      "train loss:1.1502808565369487\n",
      "train loss:1.249293226273479\n",
      "=== epoch:165, train acc:0.6833333333333333, test acc:0.5606 ===\n",
      "train loss:1.1736097864543102\n",
      "train loss:1.200565491759173\n",
      "train loss:1.171096527233268\n",
      "=== epoch:166, train acc:0.68, test acc:0.5582 ===\n",
      "train loss:1.1427510824079596\n",
      "train loss:1.1492926641807575\n",
      "train loss:1.1706194610427825\n",
      "=== epoch:167, train acc:0.6766666666666666, test acc:0.5587 ===\n",
      "train loss:1.1079360964549436\n",
      "train loss:1.0790529947494487\n",
      "train loss:1.2747088975164964\n",
      "=== epoch:168, train acc:0.69, test acc:0.5663 ===\n",
      "train loss:1.1452027886053984\n",
      "train loss:1.0450166179552123\n",
      "train loss:1.1877399300039457\n",
      "=== epoch:169, train acc:0.69, test acc:0.5642 ===\n",
      "train loss:1.2160011206231207\n",
      "train loss:1.1125246327703031\n",
      "train loss:1.2487250016676137\n",
      "=== epoch:170, train acc:0.6966666666666667, test acc:0.569 ===\n",
      "train loss:1.2397330056071245\n",
      "train loss:1.10250473786598\n",
      "train loss:1.1688770132303063\n",
      "=== epoch:171, train acc:0.7033333333333334, test acc:0.5714 ===\n",
      "train loss:1.1498697619285125\n",
      "train loss:1.2128203652682599\n",
      "train loss:1.0911192274420747\n",
      "=== epoch:172, train acc:0.7166666666666667, test acc:0.5787 ===\n",
      "train loss:1.1231515742747908\n",
      "train loss:1.1374360131576522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.105248473574791\n",
      "=== epoch:173, train acc:0.7033333333333334, test acc:0.5723 ===\n",
      "train loss:1.036299683637701\n",
      "train loss:1.0986525061524208\n",
      "train loss:1.0556797391201211\n",
      "=== epoch:174, train acc:0.69, test acc:0.5668 ===\n",
      "train loss:1.1723259468185896\n",
      "train loss:1.1073517600848373\n",
      "train loss:1.0810279821696\n",
      "=== epoch:175, train acc:0.7066666666666667, test acc:0.5714 ===\n",
      "train loss:1.0433155814684918\n",
      "train loss:1.0307991939060295\n",
      "train loss:1.0548635936188022\n",
      "=== epoch:176, train acc:0.7033333333333334, test acc:0.5743 ===\n",
      "train loss:1.197086998043441\n",
      "train loss:1.2386456452052803\n",
      "train loss:1.0243358191593173\n",
      "=== epoch:177, train acc:0.7166666666666667, test acc:0.5856 ===\n",
      "train loss:1.188399193222913\n",
      "train loss:1.1152378899294346\n",
      "train loss:1.156019954821205\n",
      "=== epoch:178, train acc:0.7366666666666667, test acc:0.594 ===\n",
      "train loss:1.1775372671605844\n",
      "train loss:1.1191563637369097\n",
      "train loss:0.9888226834825922\n",
      "=== epoch:179, train acc:0.7466666666666667, test acc:0.596 ===\n",
      "train loss:0.9997852700617251\n",
      "train loss:1.05808203068585\n",
      "train loss:1.0369793192933014\n",
      "=== epoch:180, train acc:0.7433333333333333, test acc:0.5948 ===\n",
      "train loss:1.0810638548367888\n",
      "train loss:1.0347651391920376\n",
      "train loss:0.9997692483556786\n",
      "=== epoch:181, train acc:0.7233333333333334, test acc:0.5893 ===\n",
      "train loss:1.0130863286702736\n",
      "train loss:1.1067788656602473\n",
      "train loss:1.082650362467008\n",
      "=== epoch:182, train acc:0.73, test acc:0.5918 ===\n",
      "train loss:1.0590764153383616\n",
      "train loss:0.9983480182285008\n",
      "train loss:1.054701459285\n",
      "=== epoch:183, train acc:0.7333333333333333, test acc:0.5881 ===\n",
      "train loss:1.0331047233164223\n",
      "train loss:1.2061430810932705\n",
      "train loss:1.0348579039227292\n",
      "=== epoch:184, train acc:0.7466666666666667, test acc:0.5976 ===\n",
      "train loss:0.8682632049041373\n",
      "train loss:0.9697026893140742\n",
      "train loss:1.1486925611936627\n",
      "=== epoch:185, train acc:0.7566666666666667, test acc:0.5978 ===\n",
      "train loss:1.022074726260448\n",
      "train loss:0.9387907715307329\n",
      "train loss:1.0125510522503718\n",
      "=== epoch:186, train acc:0.7566666666666667, test acc:0.6014 ===\n",
      "train loss:0.9724615072389448\n",
      "train loss:1.0621859434137944\n",
      "train loss:1.0760202229106377\n",
      "=== epoch:187, train acc:0.7566666666666667, test acc:0.6074 ===\n",
      "train loss:0.9172301552602375\n",
      "train loss:0.9143269105276589\n",
      "train loss:0.9471713448207992\n",
      "=== epoch:188, train acc:0.76, test acc:0.6089 ===\n",
      "train loss:0.9429006415205471\n",
      "train loss:0.9777033708842177\n",
      "train loss:0.9541803515160983\n",
      "=== epoch:189, train acc:0.7666666666666667, test acc:0.608 ===\n",
      "train loss:0.9502955737510658\n",
      "train loss:0.9746448555629584\n",
      "train loss:0.8989894119972065\n",
      "=== epoch:190, train acc:0.76, test acc:0.6074 ===\n",
      "train loss:0.8804342136292662\n",
      "train loss:1.0222179914153766\n",
      "train loss:0.9661922596753223\n",
      "=== epoch:191, train acc:0.77, test acc:0.615 ===\n",
      "train loss:0.8944447476897173\n",
      "train loss:0.920721428883016\n",
      "train loss:1.0741526046795307\n",
      "=== epoch:192, train acc:0.77, test acc:0.611 ===\n",
      "train loss:0.8842615453178269\n",
      "train loss:0.9549441281948039\n",
      "train loss:0.9501110755193729\n",
      "=== epoch:193, train acc:0.7566666666666667, test acc:0.6102 ===\n",
      "train loss:0.9276827941768109\n",
      "train loss:0.8370387174182641\n",
      "train loss:0.9497399304591653\n",
      "=== epoch:194, train acc:0.7566666666666667, test acc:0.6117 ===\n",
      "train loss:0.9519463358923608\n",
      "train loss:0.8432962199918188\n",
      "train loss:0.9857668897292479\n",
      "=== epoch:195, train acc:0.7633333333333333, test acc:0.6144 ===\n",
      "train loss:0.9486296184840849\n",
      "train loss:0.857323845686208\n",
      "train loss:0.8315854837931305\n",
      "=== epoch:196, train acc:0.76, test acc:0.6045 ===\n",
      "train loss:0.9373342484463559\n",
      "train loss:0.9456302787955643\n",
      "train loss:0.9373790202851731\n",
      "=== epoch:197, train acc:0.7566666666666667, test acc:0.6068 ===\n",
      "train loss:0.8751054326264063\n",
      "train loss:0.8727182647482797\n",
      "train loss:0.7679000262976641\n",
      "=== epoch:198, train acc:0.7533333333333333, test acc:0.595 ===\n",
      "train loss:0.7809361761903172\n",
      "train loss:0.9751932140180739\n",
      "train loss:0.7910965952389755\n",
      "=== epoch:199, train acc:0.77, test acc:0.6159 ===\n",
      "train loss:0.8414512196674105\n",
      "train loss:0.8395646320312322\n",
      "train loss:0.8784808717293265\n",
      "=== epoch:200, train acc:0.77, test acc:0.6264 ===\n",
      "train loss:0.9508093932462622\n",
      "train loss:0.7551122724497316\n",
      "train loss:0.8886381829411748\n",
      "=== epoch:201, train acc:0.7733333333333333, test acc:0.6256 ===\n",
      "train loss:0.9569432065941426\n",
      "train loss:0.9090930072322817\n",
      "train loss:0.8806549570254247\n",
      "=== epoch:202, train acc:0.76, test acc:0.6262 ===\n",
      "train loss:0.9364378831558118\n",
      "train loss:0.9432541299028085\n",
      "train loss:0.9374888795534533\n",
      "=== epoch:203, train acc:0.7733333333333333, test acc:0.6302 ===\n",
      "train loss:1.0001548715813413\n",
      "train loss:0.8359433703025045\n",
      "train loss:0.9081440198971953\n",
      "=== epoch:204, train acc:0.78, test acc:0.6299 ===\n",
      "train loss:0.8427418881939457\n",
      "train loss:0.7670648916985533\n",
      "train loss:0.7718595697300482\n",
      "=== epoch:205, train acc:0.7833333333333333, test acc:0.6354 ===\n",
      "train loss:0.8985512274084795\n",
      "train loss:0.9513089353068586\n",
      "train loss:0.8026664693599751\n",
      "=== epoch:206, train acc:0.7833333333333333, test acc:0.634 ===\n",
      "train loss:0.869109343624885\n",
      "train loss:0.8920496804286776\n",
      "train loss:0.8157933527487884\n",
      "=== epoch:207, train acc:0.79, test acc:0.6339 ===\n",
      "train loss:0.7719979345423631\n",
      "train loss:0.7121857003493604\n",
      "train loss:0.8085569880847813\n",
      "=== epoch:208, train acc:0.7933333333333333, test acc:0.6316 ===\n",
      "train loss:0.8110525575081333\n",
      "train loss:0.8622087080366515\n",
      "train loss:0.814746032558976\n",
      "=== epoch:209, train acc:0.7833333333333333, test acc:0.6365 ===\n",
      "train loss:0.7845870746222164\n",
      "train loss:0.9100550036157284\n",
      "train loss:0.7374307049302682\n",
      "=== epoch:210, train acc:0.7966666666666666, test acc:0.6374 ===\n",
      "train loss:0.7889273307138837\n",
      "train loss:0.7021201489252528\n",
      "train loss:0.9024447995413175\n",
      "=== epoch:211, train acc:0.79, test acc:0.6396 ===\n",
      "train loss:0.7879688281285524\n",
      "train loss:0.6844472534484839\n",
      "train loss:0.8248105952640836\n",
      "=== epoch:212, train acc:0.7866666666666666, test acc:0.639 ===\n",
      "train loss:0.7774631312525119\n",
      "train loss:0.8044375914030438\n",
      "train loss:0.7595471190824555\n",
      "=== epoch:213, train acc:0.7833333333333333, test acc:0.6368 ===\n",
      "train loss:0.8727306482552926\n",
      "train loss:0.8153979491974964\n",
      "train loss:0.8189991358452465\n",
      "=== epoch:214, train acc:0.7866666666666666, test acc:0.6413 ===\n",
      "train loss:0.7454899380397295\n",
      "train loss:0.6884325883757014\n",
      "train loss:0.5941915747690732\n",
      "=== epoch:215, train acc:0.7866666666666666, test acc:0.6379 ===\n",
      "train loss:0.7533846803198258\n",
      "train loss:0.7373899591336933\n",
      "train loss:0.798585431516792\n",
      "=== epoch:216, train acc:0.7966666666666666, test acc:0.6413 ===\n",
      "train loss:0.7513035172280685\n",
      "train loss:0.7120563249452797\n",
      "train loss:0.9376282904249494\n",
      "=== epoch:217, train acc:0.7866666666666666, test acc:0.6405 ===\n",
      "train loss:0.7756928648749511\n",
      "train loss:0.875252963775279\n",
      "train loss:0.7855619885560173\n",
      "=== epoch:218, train acc:0.79, test acc:0.6439 ===\n",
      "train loss:0.7707400227952925\n",
      "train loss:0.8507595672521485\n",
      "train loss:0.7170518468212326\n",
      "=== epoch:219, train acc:0.8133333333333334, test acc:0.6454 ===\n",
      "train loss:0.6292285913215517\n",
      "train loss:0.7676067937451362\n",
      "train loss:0.7704058280803168\n",
      "=== epoch:220, train acc:0.8133333333333334, test acc:0.6468 ===\n",
      "train loss:0.7147185225862652\n",
      "train loss:0.8592807373621665\n",
      "train loss:0.7356515156451852\n",
      "=== epoch:221, train acc:0.82, test acc:0.6524 ===\n",
      "train loss:0.832368728216908\n",
      "train loss:0.7704460099706232\n",
      "train loss:0.746763687236327\n",
      "=== epoch:222, train acc:0.8133333333333334, test acc:0.6554 ===\n",
      "train loss:0.814981070722672\n",
      "train loss:0.6796527066477863\n",
      "train loss:0.6562809195128255\n",
      "=== epoch:223, train acc:0.81, test acc:0.6557 ===\n",
      "train loss:0.7719339177649636\n",
      "train loss:0.698485235947489\n",
      "train loss:0.7565846931929775\n",
      "=== epoch:224, train acc:0.8133333333333334, test acc:0.6592 ===\n",
      "train loss:0.778480389884143\n",
      "train loss:0.7438722318260494\n",
      "train loss:0.6874424558435543\n",
      "=== epoch:225, train acc:0.7966666666666666, test acc:0.6561 ===\n",
      "train loss:0.6930560727806386\n",
      "train loss:0.6707635509048315\n",
      "train loss:0.7336276831136465\n",
      "=== epoch:226, train acc:0.8233333333333334, test acc:0.6587 ===\n",
      "train loss:0.6928080705670577\n",
      "train loss:0.7138574527885659\n",
      "train loss:0.7994731565657376\n",
      "=== epoch:227, train acc:0.8233333333333334, test acc:0.6528 ===\n",
      "train loss:0.7806573718187546\n",
      "train loss:0.739426640267159\n",
      "train loss:0.7450246566758625\n",
      "=== epoch:228, train acc:0.8366666666666667, test acc:0.6636 ===\n",
      "train loss:0.7174660182959355\n",
      "train loss:0.6429404594160836\n",
      "train loss:0.8355897408993779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:229, train acc:0.83, test acc:0.6617 ===\n",
      "train loss:0.7275374109217707\n",
      "train loss:0.7802165243428084\n",
      "train loss:0.7344806726884713\n",
      "=== epoch:230, train acc:0.8366666666666667, test acc:0.6639 ===\n",
      "train loss:0.6777778889112891\n",
      "train loss:0.6468894391929085\n",
      "train loss:0.7979582352750364\n",
      "=== epoch:231, train acc:0.8366666666666667, test acc:0.666 ===\n",
      "train loss:0.6022181835605781\n",
      "train loss:0.8754271421987702\n",
      "train loss:0.652231831474894\n",
      "=== epoch:232, train acc:0.82, test acc:0.6654 ===\n",
      "train loss:0.6966210418189287\n",
      "train loss:0.6905399658453814\n",
      "train loss:0.7041217954570618\n",
      "=== epoch:233, train acc:0.8333333333333334, test acc:0.6699 ===\n",
      "train loss:0.6579625984423969\n",
      "train loss:0.6734129480731713\n",
      "train loss:0.7979550180064963\n",
      "=== epoch:234, train acc:0.8366666666666667, test acc:0.6719 ===\n",
      "train loss:0.8060708192371918\n",
      "train loss:0.5922916380930583\n",
      "train loss:0.6203254530461895\n",
      "=== epoch:235, train acc:0.8266666666666667, test acc:0.6741 ===\n",
      "train loss:0.8123848438127552\n",
      "train loss:0.6040737168414076\n",
      "train loss:0.6308332147850275\n",
      "=== epoch:236, train acc:0.83, test acc:0.6733 ===\n",
      "train loss:0.7745878344165021\n",
      "train loss:0.6997373121483217\n",
      "train loss:0.6143908198670205\n",
      "=== epoch:237, train acc:0.8333333333333334, test acc:0.6761 ===\n",
      "train loss:0.6083168654364581\n",
      "train loss:0.6021758256721539\n",
      "train loss:0.6487326452750233\n",
      "=== epoch:238, train acc:0.8333333333333334, test acc:0.673 ===\n",
      "train loss:0.7821754800670392\n",
      "train loss:0.6427830518892607\n",
      "train loss:0.7596144351780088\n",
      "=== epoch:239, train acc:0.84, test acc:0.672 ===\n",
      "train loss:0.7855097591385329\n",
      "train loss:0.7130921209877573\n",
      "train loss:0.6992011991034757\n",
      "=== epoch:240, train acc:0.84, test acc:0.6674 ===\n",
      "train loss:0.6771993471300108\n",
      "train loss:0.6936813373632826\n",
      "train loss:0.4996430007200135\n",
      "=== epoch:241, train acc:0.84, test acc:0.6689 ===\n",
      "train loss:0.6414546069197327\n",
      "train loss:0.7065879027217469\n",
      "train loss:0.7171395922182423\n",
      "=== epoch:242, train acc:0.8266666666666667, test acc:0.6564 ===\n",
      "train loss:0.57560926658832\n",
      "train loss:0.7095904307687486\n",
      "train loss:0.6893387357136609\n",
      "=== epoch:243, train acc:0.8166666666666667, test acc:0.6541 ===\n",
      "train loss:0.7647547759188129\n",
      "train loss:0.5850924482525062\n",
      "train loss:0.5899131285666861\n",
      "=== epoch:244, train acc:0.8233333333333334, test acc:0.6544 ===\n",
      "train loss:0.7148189560908179\n",
      "train loss:0.6513893623165805\n",
      "train loss:0.5472241897369936\n",
      "=== epoch:245, train acc:0.8466666666666667, test acc:0.6713 ===\n",
      "train loss:0.575596777830101\n",
      "train loss:0.6435107711641891\n",
      "train loss:0.5419491132453047\n",
      "=== epoch:246, train acc:0.8433333333333334, test acc:0.6708 ===\n",
      "train loss:0.6156283673264198\n",
      "train loss:0.6179534027515039\n",
      "train loss:0.7155241342929713\n",
      "=== epoch:247, train acc:0.8366666666666667, test acc:0.6751 ===\n",
      "train loss:0.6705847475094882\n",
      "train loss:0.6744645543848665\n",
      "train loss:0.6048639584670154\n",
      "=== epoch:248, train acc:0.8366666666666667, test acc:0.6799 ===\n",
      "train loss:0.7358882075508916\n",
      "train loss:0.5409216346346549\n",
      "train loss:0.5497702365996482\n",
      "=== epoch:249, train acc:0.8366666666666667, test acc:0.6697 ===\n",
      "train loss:0.5248666045091392\n",
      "train loss:0.545053594541034\n",
      "train loss:0.6210391935319606\n",
      "=== epoch:250, train acc:0.8366666666666667, test acc:0.6802 ===\n",
      "train loss:0.7211221929742693\n",
      "train loss:0.5697785553931007\n",
      "train loss:0.6555635703289922\n",
      "=== epoch:251, train acc:0.8333333333333334, test acc:0.6808 ===\n",
      "train loss:0.636551077875551\n",
      "train loss:0.4984943213348563\n",
      "train loss:0.5690769626418931\n",
      "=== epoch:252, train acc:0.8366666666666667, test acc:0.6821 ===\n",
      "train loss:0.5499725776070776\n",
      "train loss:0.6058460986675108\n",
      "train loss:0.592661440955802\n",
      "=== epoch:253, train acc:0.8433333333333334, test acc:0.6862 ===\n",
      "train loss:0.5516041454617744\n",
      "train loss:0.6299754636315498\n",
      "train loss:0.6352843274578885\n",
      "=== epoch:254, train acc:0.84, test acc:0.6887 ===\n",
      "train loss:0.6166658195026848\n",
      "train loss:0.5807407762930666\n",
      "train loss:0.5783256530043964\n",
      "=== epoch:255, train acc:0.8433333333333334, test acc:0.687 ===\n",
      "train loss:0.5475925028656586\n",
      "train loss:0.693370070655681\n",
      "train loss:0.5379005615029137\n",
      "=== epoch:256, train acc:0.85, test acc:0.6888 ===\n",
      "train loss:0.4973571726564897\n",
      "train loss:0.6888975198604294\n",
      "train loss:0.678531704606875\n",
      "=== epoch:257, train acc:0.8466666666666667, test acc:0.6859 ===\n",
      "train loss:0.5328748280158127\n",
      "train loss:0.5884534767186013\n",
      "train loss:0.5846958626760985\n",
      "=== epoch:258, train acc:0.85, test acc:0.6907 ===\n",
      "train loss:0.5527933923512897\n",
      "train loss:0.48117350109235313\n",
      "train loss:0.6519837923512852\n",
      "=== epoch:259, train acc:0.8533333333333334, test acc:0.6938 ===\n",
      "train loss:0.4744381085126646\n",
      "train loss:0.46542024416678\n",
      "train loss:0.6791389562444639\n",
      "=== epoch:260, train acc:0.85, test acc:0.6909 ===\n",
      "train loss:0.544585573918055\n",
      "train loss:0.5780839834051101\n",
      "train loss:0.7256825857458299\n",
      "=== epoch:261, train acc:0.85, test acc:0.6885 ===\n",
      "train loss:0.5423050050863233\n",
      "train loss:0.5459289207406788\n",
      "train loss:0.5845596546996312\n",
      "=== epoch:262, train acc:0.8466666666666667, test acc:0.6909 ===\n",
      "train loss:0.5032142732004075\n",
      "train loss:0.4803425516336814\n",
      "train loss:0.6078288690323407\n",
      "=== epoch:263, train acc:0.84, test acc:0.6936 ===\n",
      "train loss:0.48101884213827373\n",
      "train loss:0.5130314274252729\n",
      "train loss:0.4935588547000351\n",
      "=== epoch:264, train acc:0.8466666666666667, test acc:0.6916 ===\n",
      "train loss:0.510725824249224\n",
      "train loss:0.5530335088056857\n",
      "train loss:0.5679346216932345\n",
      "=== epoch:265, train acc:0.84, test acc:0.6906 ===\n",
      "train loss:0.5548662914344014\n",
      "train loss:0.5226770817339292\n",
      "train loss:0.5339950358162562\n",
      "=== epoch:266, train acc:0.8533333333333334, test acc:0.6909 ===\n",
      "train loss:0.5463172307444021\n",
      "train loss:0.6062694448920886\n",
      "train loss:0.5480137402216285\n",
      "=== epoch:267, train acc:0.8533333333333334, test acc:0.6986 ===\n",
      "train loss:0.5205274416967621\n",
      "train loss:0.5186129862119255\n",
      "train loss:0.6221407282988247\n",
      "=== epoch:268, train acc:0.85, test acc:0.6986 ===\n",
      "train loss:0.4878133185256641\n",
      "train loss:0.5643194633420827\n",
      "train loss:0.5221417604203091\n",
      "=== epoch:269, train acc:0.8366666666666667, test acc:0.6951 ===\n",
      "train loss:0.5133968225835538\n",
      "train loss:0.4938198838011862\n",
      "train loss:0.5225989735540427\n",
      "=== epoch:270, train acc:0.86, test acc:0.7005 ===\n",
      "train loss:0.4542163588311915\n",
      "train loss:0.4144726684141445\n",
      "train loss:0.5478615768615277\n",
      "=== epoch:271, train acc:0.86, test acc:0.701 ===\n",
      "train loss:0.439927717989529\n",
      "train loss:0.6020732032005832\n",
      "train loss:0.3947515121870795\n",
      "=== epoch:272, train acc:0.8566666666666667, test acc:0.7011 ===\n",
      "train loss:0.5077126919275519\n",
      "train loss:0.4785457240428251\n",
      "train loss:0.49152029448156964\n",
      "=== epoch:273, train acc:0.86, test acc:0.6994 ===\n",
      "train loss:0.5631519404288595\n",
      "train loss:0.5180970720471908\n",
      "train loss:0.5328964478373445\n",
      "=== epoch:274, train acc:0.8566666666666667, test acc:0.705 ===\n",
      "train loss:0.5121144612795397\n",
      "train loss:0.4767632986358264\n",
      "train loss:0.4781006950969189\n",
      "=== epoch:275, train acc:0.8666666666666667, test acc:0.7027 ===\n",
      "train loss:0.48789540197412423\n",
      "train loss:0.50843235199219\n",
      "train loss:0.5089720449281601\n",
      "=== epoch:276, train acc:0.8666666666666667, test acc:0.7034 ===\n",
      "train loss:0.5212174405171787\n",
      "train loss:0.3637889965403023\n",
      "train loss:0.5327693258160157\n",
      "=== epoch:277, train acc:0.8666666666666667, test acc:0.704 ===\n",
      "train loss:0.6778023619939261\n",
      "train loss:0.4937569015677222\n",
      "train loss:0.5454552741325929\n",
      "=== epoch:278, train acc:0.8633333333333333, test acc:0.7068 ===\n",
      "train loss:0.5189291441883086\n",
      "train loss:0.4950633351090486\n",
      "train loss:0.5088524137552125\n",
      "=== epoch:279, train acc:0.8733333333333333, test acc:0.7112 ===\n",
      "train loss:0.5971227240160643\n",
      "train loss:0.534317403893092\n",
      "train loss:0.5088039706346571\n",
      "=== epoch:280, train acc:0.8766666666666667, test acc:0.7069 ===\n",
      "train loss:0.45570406629990534\n",
      "train loss:0.480500038829719\n",
      "train loss:0.5167071312251073\n",
      "=== epoch:281, train acc:0.8766666666666667, test acc:0.7068 ===\n",
      "train loss:0.49818600713174876\n",
      "train loss:0.5130829231520828\n",
      "train loss:0.4074543369751764\n",
      "=== epoch:282, train acc:0.87, test acc:0.704 ===\n",
      "train loss:0.44329490065266586\n",
      "train loss:0.46269275122786074\n",
      "train loss:0.4233435775821277\n",
      "=== epoch:283, train acc:0.8733333333333333, test acc:0.7114 ===\n",
      "train loss:0.4195751178613861\n",
      "train loss:0.44160362590787544\n",
      "train loss:0.5715087720962005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:284, train acc:0.8766666666666667, test acc:0.7159 ===\n",
      "train loss:0.5258199994016468\n",
      "train loss:0.5596369962577019\n",
      "train loss:0.4809637604741694\n",
      "=== epoch:285, train acc:0.8733333333333333, test acc:0.7129 ===\n",
      "train loss:0.41874656325625514\n",
      "train loss:0.4709634471288097\n",
      "train loss:0.5144629973076129\n",
      "=== epoch:286, train acc:0.8766666666666667, test acc:0.716 ===\n",
      "train loss:0.463296062144096\n",
      "train loss:0.521849119953162\n",
      "train loss:0.5094019856268943\n",
      "=== epoch:287, train acc:0.8733333333333333, test acc:0.7101 ===\n",
      "train loss:0.47207315790931514\n",
      "train loss:0.401859249743229\n",
      "train loss:0.3625122561879223\n",
      "=== epoch:288, train acc:0.8766666666666667, test acc:0.7136 ===\n",
      "train loss:0.5362527225103721\n",
      "train loss:0.4838838278835429\n",
      "train loss:0.4491228806772592\n",
      "=== epoch:289, train acc:0.8666666666666667, test acc:0.7032 ===\n",
      "train loss:0.36540829537966885\n",
      "train loss:0.3770435806596958\n",
      "train loss:0.5582881383054846\n",
      "=== epoch:290, train acc:0.8766666666666667, test acc:0.7102 ===\n",
      "train loss:0.41425300478469057\n",
      "train loss:0.46844495873367625\n",
      "train loss:0.416730342855514\n",
      "=== epoch:291, train acc:0.8833333333333333, test acc:0.7135 ===\n",
      "train loss:0.4413457174540553\n",
      "train loss:0.4327296873567878\n",
      "train loss:0.4526232329443365\n",
      "=== epoch:292, train acc:0.8766666666666667, test acc:0.7145 ===\n",
      "train loss:0.47162331052978573\n",
      "train loss:0.48577311032367243\n",
      "train loss:0.48933099239499805\n",
      "=== epoch:293, train acc:0.8766666666666667, test acc:0.7108 ===\n",
      "train loss:0.4506535466004161\n",
      "train loss:0.4978720678574667\n",
      "train loss:0.34819531717648816\n",
      "=== epoch:294, train acc:0.8766666666666667, test acc:0.71 ===\n",
      "train loss:0.4364014625617464\n",
      "train loss:0.47949182813778407\n",
      "train loss:0.3862652569000729\n",
      "=== epoch:295, train acc:0.8833333333333333, test acc:0.7158 ===\n",
      "train loss:0.35151574622453574\n",
      "train loss:0.46846117690500655\n",
      "train loss:0.5114567901314286\n",
      "=== epoch:296, train acc:0.88, test acc:0.71 ===\n",
      "train loss:0.4718398287116644\n",
      "train loss:0.433453716698651\n",
      "train loss:0.45106249698021345\n",
      "=== epoch:297, train acc:0.8833333333333333, test acc:0.711 ===\n",
      "train loss:0.4587657916019366\n",
      "train loss:0.4851988937330362\n",
      "train loss:0.29228036714497013\n",
      "=== epoch:298, train acc:0.89, test acc:0.7143 ===\n",
      "train loss:0.475081218954344\n",
      "train loss:0.36973674017531927\n",
      "train loss:0.42541300154815187\n",
      "=== epoch:299, train acc:0.8866666666666667, test acc:0.7121 ===\n",
      "train loss:0.42642521689208757\n",
      "train loss:0.4559988178266627\n",
      "train loss:0.32764733343171265\n",
      "=== epoch:300, train acc:0.87, test acc:0.7207 ===\n",
      "train loss:0.35831425994463906\n",
      "train loss:0.5034274147728585\n",
      "train loss:0.4347143141509377\n",
      "=== epoch:301, train acc:0.8766666666666667, test acc:0.7172 ===\n",
      "train loss:0.44995091940559834\n",
      "train loss:0.4242991889797384\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7212\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trial(use_dropout=True, dropout_ratio=0.15) # テキストと同じ0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.403446555321777\n",
      "=== epoch:1, train acc:0.07333333333333333, test acc:0.1038 ===\n",
      "train loss:2.37736670595611\n",
      "train loss:2.330877856111378\n",
      "train loss:2.3296280964704965\n",
      "=== epoch:2, train acc:0.09333333333333334, test acc:0.1195 ===\n",
      "train loss:2.3070252733201277\n",
      "train loss:2.3249275769709987\n",
      "train loss:2.2691753567664703\n",
      "=== epoch:3, train acc:0.13333333333333333, test acc:0.1235 ===\n",
      "train loss:2.2413634057740515\n",
      "train loss:2.2048856687416354\n",
      "train loss:2.258269781831598\n",
      "=== epoch:4, train acc:0.2, test acc:0.1533 ===\n",
      "train loss:2.156122490158655\n",
      "train loss:2.1717481457302097\n",
      "train loss:2.114141163636077\n",
      "=== epoch:5, train acc:0.25666666666666665, test acc:0.1872 ===\n",
      "train loss:2.1428397068300438\n",
      "train loss:2.111112768585493\n",
      "train loss:2.1103785018647887\n",
      "=== epoch:6, train acc:0.2733333333333333, test acc:0.21 ===\n",
      "train loss:2.0838575749183335\n",
      "train loss:2.1144424089691185\n",
      "train loss:2.060265679380756\n",
      "=== epoch:7, train acc:0.2866666666666667, test acc:0.2124 ===\n",
      "train loss:2.0591478904678\n",
      "train loss:2.0300469618456454\n",
      "train loss:2.026572017517679\n",
      "=== epoch:8, train acc:0.35, test acc:0.2516 ===\n",
      "train loss:1.993331093684982\n",
      "train loss:2.050969809099415\n",
      "train loss:1.990502302428216\n",
      "=== epoch:9, train acc:0.38333333333333336, test acc:0.2881 ===\n",
      "train loss:1.9809110999898343\n",
      "train loss:1.9238298807013419\n",
      "train loss:1.9750708766030178\n",
      "=== epoch:10, train acc:0.4166666666666667, test acc:0.3067 ===\n",
      "train loss:1.9151476976471407\n",
      "train loss:1.8960875763946858\n",
      "train loss:1.8898537471275143\n",
      "=== epoch:11, train acc:0.44333333333333336, test acc:0.3457 ===\n",
      "train loss:1.9501293564098545\n",
      "train loss:1.9045957634308448\n",
      "train loss:1.7925258137963969\n",
      "=== epoch:12, train acc:0.52, test acc:0.3799 ===\n",
      "train loss:1.8250306924104143\n",
      "train loss:1.772050620555608\n",
      "train loss:1.7805899727669572\n",
      "=== epoch:13, train acc:0.55, test acc:0.4041 ===\n",
      "train loss:1.7156455032370477\n",
      "train loss:1.6978106175896672\n",
      "train loss:1.7799763802626691\n",
      "=== epoch:14, train acc:0.54, test acc:0.4223 ===\n",
      "train loss:1.7831678971427325\n",
      "train loss:1.666485760181675\n",
      "train loss:1.6322725420013526\n",
      "=== epoch:15, train acc:0.5533333333333333, test acc:0.4314 ===\n",
      "train loss:1.6461021030856176\n",
      "train loss:1.6970480445305673\n",
      "train loss:1.583279794287108\n",
      "=== epoch:16, train acc:0.6166666666666667, test acc:0.4529 ===\n",
      "train loss:1.6512996213261824\n",
      "train loss:1.5972631794774657\n",
      "train loss:1.583312935976891\n",
      "=== epoch:17, train acc:0.63, test acc:0.4872 ===\n",
      "train loss:1.608002194242427\n",
      "train loss:1.5589127548323827\n",
      "train loss:1.6388250926154138\n",
      "=== epoch:18, train acc:0.6233333333333333, test acc:0.4882 ===\n",
      "train loss:1.543415555388713\n",
      "train loss:1.5734406055237111\n",
      "train loss:1.4547148939421926\n",
      "=== epoch:19, train acc:0.6366666666666667, test acc:0.5005 ===\n",
      "train loss:1.4373051425151013\n",
      "train loss:1.5082886349920526\n",
      "train loss:1.4179698252692168\n",
      "=== epoch:20, train acc:0.6333333333333333, test acc:0.513 ===\n",
      "train loss:1.4229252271336024\n",
      "train loss:1.3749305278151513\n",
      "train loss:1.3198340396227979\n",
      "=== epoch:21, train acc:0.6466666666666666, test acc:0.527 ===\n",
      "train loss:1.252672995684554\n",
      "train loss:1.3084136068499166\n",
      "train loss:1.32874230343056\n",
      "=== epoch:22, train acc:0.6666666666666666, test acc:0.537 ===\n",
      "train loss:1.3091934463217914\n",
      "train loss:1.2478285444504607\n",
      "train loss:1.2361834127445062\n",
      "=== epoch:23, train acc:0.72, test acc:0.5621 ===\n",
      "train loss:1.1561856062701958\n",
      "train loss:1.1841330075247838\n",
      "train loss:1.164347527703026\n",
      "=== epoch:24, train acc:0.7166666666666667, test acc:0.5649 ===\n",
      "train loss:1.129368830102282\n",
      "train loss:1.0178489439580494\n",
      "train loss:1.2125671195613454\n",
      "=== epoch:25, train acc:0.7066666666666667, test acc:0.5831 ===\n",
      "train loss:1.1302752163594922\n",
      "train loss:1.1486547810368575\n",
      "train loss:1.113808647541243\n",
      "=== epoch:26, train acc:0.73, test acc:0.5976 ===\n",
      "train loss:1.0652679881678173\n",
      "train loss:1.0309765203129333\n",
      "train loss:0.9910617279787691\n",
      "=== epoch:27, train acc:0.7433333333333333, test acc:0.5821 ===\n",
      "train loss:0.9405455878402837\n",
      "train loss:0.9866815298207969\n",
      "train loss:1.2348492224791585\n",
      "=== epoch:28, train acc:0.74, test acc:0.6091 ===\n",
      "train loss:0.8635358777545673\n",
      "train loss:0.9627250553621658\n",
      "train loss:0.9214125412570275\n",
      "=== epoch:29, train acc:0.7733333333333333, test acc:0.6263 ===\n",
      "train loss:1.0612351915848064\n",
      "train loss:0.8735021121104072\n",
      "train loss:0.8245236762906677\n",
      "=== epoch:30, train acc:0.78, test acc:0.6283 ===\n",
      "train loss:0.951321057451601\n",
      "train loss:0.7367809265303563\n",
      "train loss:0.90650370186793\n",
      "=== epoch:31, train acc:0.7933333333333333, test acc:0.6412 ===\n",
      "train loss:0.8355981971243378\n",
      "train loss:0.9306947004524355\n",
      "train loss:0.833184451635639\n",
      "=== epoch:32, train acc:0.8033333333333333, test acc:0.6399 ===\n",
      "train loss:0.8454721327236154\n",
      "train loss:0.7019302678317652\n",
      "train loss:0.7354685906420404\n",
      "=== epoch:33, train acc:0.79, test acc:0.6309 ===\n",
      "train loss:0.9671879796708336\n",
      "train loss:0.7716353414688135\n",
      "train loss:0.6962482790418332\n",
      "=== epoch:34, train acc:0.8, test acc:0.643 ===\n",
      "train loss:0.729674862787883\n",
      "train loss:0.6765329501919547\n",
      "train loss:0.7285478571656315\n",
      "=== epoch:35, train acc:0.83, test acc:0.6577 ===\n",
      "train loss:0.554317937597466\n",
      "train loss:0.8121209501053525\n",
      "train loss:0.6789488556861046\n",
      "=== epoch:36, train acc:0.8166666666666667, test acc:0.6589 ===\n",
      "train loss:0.7497826833270341\n",
      "train loss:0.7531829391253416\n",
      "train loss:0.6310905829713404\n",
      "=== epoch:37, train acc:0.8466666666666667, test acc:0.6534 ===\n",
      "train loss:0.6446371902743953\n",
      "train loss:0.6303690618005102\n",
      "train loss:0.6056123299986819\n",
      "=== epoch:38, train acc:0.8333333333333334, test acc:0.661 ===\n",
      "train loss:0.7104570804564696\n",
      "train loss:0.565172008941746\n",
      "train loss:0.6853878284584785\n",
      "=== epoch:39, train acc:0.8366666666666667, test acc:0.6655 ===\n",
      "train loss:0.6131011624405647\n",
      "train loss:0.5743678590990475\n",
      "train loss:0.5415360361449867\n",
      "=== epoch:40, train acc:0.86, test acc:0.6775 ===\n",
      "train loss:0.5101472210636687\n",
      "train loss:0.5708960125913408\n",
      "train loss:0.5261175723183524\n",
      "=== epoch:41, train acc:0.86, test acc:0.6798 ===\n",
      "train loss:0.49615856144669046\n",
      "train loss:0.4816052311733887\n",
      "train loss:0.4546236673423252\n",
      "=== epoch:42, train acc:0.8533333333333334, test acc:0.6692 ===\n",
      "train loss:0.5733734041668895\n",
      "train loss:0.5279109470635571\n",
      "train loss:0.5599716122992767\n",
      "=== epoch:43, train acc:0.86, test acc:0.6773 ===\n",
      "train loss:0.5879559631178015\n",
      "train loss:0.521133505866748\n",
      "train loss:0.5878911010046822\n",
      "=== epoch:44, train acc:0.88, test acc:0.682 ===\n",
      "train loss:0.48671219217084\n",
      "train loss:0.4906130672316164\n",
      "train loss:0.5402840802795589\n",
      "=== epoch:45, train acc:0.8866666666666667, test acc:0.6789 ===\n",
      "train loss:0.4921260153614895\n",
      "train loss:0.4367532868368359\n",
      "train loss:0.4316385577391614\n",
      "=== epoch:46, train acc:0.8833333333333333, test acc:0.6948 ===\n",
      "train loss:0.5023148325370485\n",
      "train loss:0.48572404924702917\n",
      "train loss:0.5261949373135265\n",
      "=== epoch:47, train acc:0.9, test acc:0.6998 ===\n",
      "train loss:0.49140059963984045\n",
      "train loss:0.4006095987864947\n",
      "train loss:0.49318867097395874\n",
      "=== epoch:48, train acc:0.8866666666666667, test acc:0.6808 ===\n",
      "train loss:0.5353481291281316\n",
      "train loss:0.4131085631833183\n",
      "train loss:0.4114795067020406\n",
      "=== epoch:49, train acc:0.9, test acc:0.7007 ===\n",
      "train loss:0.4092237433831539\n",
      "train loss:0.3902143958421871\n",
      "train loss:0.3645447818317681\n",
      "=== epoch:50, train acc:0.9166666666666666, test acc:0.6988 ===\n",
      "train loss:0.33826624511141046\n",
      "train loss:0.39220885836478603\n",
      "train loss:0.492613961607182\n",
      "=== epoch:51, train acc:0.9133333333333333, test acc:0.7071 ===\n",
      "train loss:0.330211598670608\n",
      "train loss:0.3598681551442528\n",
      "train loss:0.28625542564770695\n",
      "=== epoch:52, train acc:0.9133333333333333, test acc:0.7086 ===\n",
      "train loss:0.41202072694238423\n",
      "train loss:0.43648314955507056\n",
      "train loss:0.39108380725035624\n",
      "=== epoch:53, train acc:0.93, test acc:0.717 ===\n",
      "train loss:0.2717608331709722\n",
      "train loss:0.313036448672711\n",
      "train loss:0.3975183121263215\n",
      "=== epoch:54, train acc:0.9266666666666666, test acc:0.7039 ===\n",
      "train loss:0.3724334995884402\n",
      "train loss:0.40761398686587397\n",
      "train loss:0.3264626187997235\n",
      "=== epoch:55, train acc:0.9266666666666666, test acc:0.7078 ===\n",
      "train loss:0.41717038205946794\n",
      "train loss:0.35415434303621546\n",
      "train loss:0.33759140273111643\n",
      "=== epoch:56, train acc:0.94, test acc:0.7204 ===\n",
      "train loss:0.3113936156503511\n",
      "train loss:0.33312658478913065\n",
      "train loss:0.3710284502200254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.94, test acc:0.7136 ===\n",
      "train loss:0.31385218473606025\n",
      "train loss:0.2944814447266545\n",
      "train loss:0.2523091488908046\n",
      "=== epoch:58, train acc:0.94, test acc:0.7263 ===\n",
      "train loss:0.3063167520726839\n",
      "train loss:0.3221073650141096\n",
      "train loss:0.3098913950101416\n",
      "=== epoch:59, train acc:0.94, test acc:0.7285 ===\n",
      "train loss:0.231452620770517\n",
      "train loss:0.2804189430166614\n",
      "train loss:0.2048587601540945\n",
      "=== epoch:60, train acc:0.95, test acc:0.7306 ===\n",
      "train loss:0.25776923111650346\n",
      "train loss:0.2780323090139145\n",
      "train loss:0.33546693060686245\n",
      "=== epoch:61, train acc:0.95, test acc:0.7321 ===\n",
      "train loss:0.24767950919837453\n",
      "train loss:0.25237883630766667\n",
      "train loss:0.2158326500935375\n",
      "=== epoch:62, train acc:0.95, test acc:0.7268 ===\n",
      "train loss:0.19215294460158955\n",
      "train loss:0.2185020773142319\n",
      "train loss:0.20518512912219852\n",
      "=== epoch:63, train acc:0.95, test acc:0.7352 ===\n",
      "train loss:0.28816096434065275\n",
      "train loss:0.2296871653337674\n",
      "train loss:0.23905809224376123\n",
      "=== epoch:64, train acc:0.96, test acc:0.7292 ===\n",
      "train loss:0.23069360843503842\n",
      "train loss:0.23605429866423336\n",
      "train loss:0.238181666022748\n",
      "=== epoch:65, train acc:0.9633333333333334, test acc:0.7297 ===\n",
      "train loss:0.22277382585814148\n",
      "train loss:0.2663260196441881\n",
      "train loss:0.21605840037125654\n",
      "=== epoch:66, train acc:0.96, test acc:0.7308 ===\n",
      "train loss:0.247048163911799\n",
      "train loss:0.1495200114729305\n",
      "train loss:0.23127264632323036\n",
      "=== epoch:67, train acc:0.9566666666666667, test acc:0.7335 ===\n",
      "train loss:0.2023974102279859\n",
      "train loss:0.21192069545959152\n",
      "train loss:0.1888352278811512\n",
      "=== epoch:68, train acc:0.9633333333333334, test acc:0.7318 ===\n",
      "train loss:0.21772349155431164\n",
      "train loss:0.23341148420402213\n",
      "train loss:0.20581914833018392\n",
      "=== epoch:69, train acc:0.9666666666666667, test acc:0.7341 ===\n",
      "train loss:0.2724288764740816\n",
      "train loss:0.19189260323975943\n",
      "train loss:0.2005392404458426\n",
      "=== epoch:70, train acc:0.97, test acc:0.7366 ===\n",
      "train loss:0.20120802426698695\n",
      "train loss:0.192542881048099\n",
      "train loss:0.2029384703483589\n",
      "=== epoch:71, train acc:0.97, test acc:0.7409 ===\n",
      "train loss:0.15857977803703738\n",
      "train loss:0.22826216472097363\n",
      "train loss:0.1528527173925979\n",
      "=== epoch:72, train acc:0.97, test acc:0.7309 ===\n",
      "train loss:0.19790553053658175\n",
      "train loss:0.17723460426395174\n",
      "train loss:0.1743111602613729\n",
      "=== epoch:73, train acc:0.9766666666666667, test acc:0.7385 ===\n",
      "train loss:0.16862461914573607\n",
      "train loss:0.23128887556713854\n",
      "train loss:0.18083662683552748\n",
      "=== epoch:74, train acc:0.98, test acc:0.7364 ===\n",
      "train loss:0.1510624507991543\n",
      "train loss:0.18406095337448072\n",
      "train loss:0.1995936173939053\n",
      "=== epoch:75, train acc:0.9866666666666667, test acc:0.7374 ===\n",
      "train loss:0.19469128273283506\n",
      "train loss:0.1756321269403731\n",
      "train loss:0.16475643994207823\n",
      "=== epoch:76, train acc:0.9766666666666667, test acc:0.7392 ===\n",
      "train loss:0.1403250164574846\n",
      "train loss:0.14239112692773437\n",
      "train loss:0.19865045949628843\n",
      "=== epoch:77, train acc:0.9833333333333333, test acc:0.7365 ===\n",
      "train loss:0.1318473581244196\n",
      "train loss:0.17058224224250046\n",
      "train loss:0.1433165449439757\n",
      "=== epoch:78, train acc:0.9766666666666667, test acc:0.7402 ===\n",
      "train loss:0.14691768115554027\n",
      "train loss:0.16344984566183998\n",
      "train loss:0.16216590885163257\n",
      "=== epoch:79, train acc:0.9833333333333333, test acc:0.7365 ===\n",
      "train loss:0.14690932869303158\n",
      "train loss:0.10840928149727516\n",
      "train loss:0.15782175310111288\n",
      "=== epoch:80, train acc:0.98, test acc:0.7435 ===\n",
      "train loss:0.13845793228712627\n",
      "train loss:0.17366687605195893\n",
      "train loss:0.11061935862908076\n",
      "=== epoch:81, train acc:0.98, test acc:0.7481 ===\n",
      "train loss:0.11486454259904236\n",
      "train loss:0.1139510473992573\n",
      "train loss:0.16686454309271284\n",
      "=== epoch:82, train acc:0.9833333333333333, test acc:0.7476 ===\n",
      "train loss:0.16837826107746884\n",
      "train loss:0.15203744649223355\n",
      "train loss:0.13938917181436497\n",
      "=== epoch:83, train acc:0.99, test acc:0.7456 ===\n",
      "train loss:0.14521148305588416\n",
      "train loss:0.1441871948987502\n",
      "train loss:0.10174987548157183\n",
      "=== epoch:84, train acc:0.9866666666666667, test acc:0.7446 ===\n",
      "train loss:0.127550008132553\n",
      "train loss:0.18643872454864024\n",
      "train loss:0.13851495149360724\n",
      "=== epoch:85, train acc:0.99, test acc:0.7401 ===\n",
      "train loss:0.1706250102881638\n",
      "train loss:0.09528747937744883\n",
      "train loss:0.135946056915922\n",
      "=== epoch:86, train acc:0.9933333333333333, test acc:0.7428 ===\n",
      "train loss:0.1441587180560132\n",
      "train loss:0.11604153328197826\n",
      "train loss:0.16288881770140762\n",
      "=== epoch:87, train acc:0.9933333333333333, test acc:0.7483 ===\n",
      "train loss:0.1295237836574135\n",
      "train loss:0.1084524691331868\n",
      "train loss:0.16850898743618456\n",
      "=== epoch:88, train acc:0.9966666666666667, test acc:0.7452 ===\n",
      "train loss:0.11527825194455907\n",
      "train loss:0.10270777623857967\n",
      "train loss:0.10246745553062343\n",
      "=== epoch:89, train acc:0.99, test acc:0.748 ===\n",
      "train loss:0.09881401600329626\n",
      "train loss:0.10471457734541872\n",
      "train loss:0.12004070057718969\n",
      "=== epoch:90, train acc:0.9866666666666667, test acc:0.7447 ===\n",
      "train loss:0.09846968828456036\n",
      "train loss:0.1334347032643583\n",
      "train loss:0.11588274874684001\n",
      "=== epoch:91, train acc:0.9933333333333333, test acc:0.7454 ===\n",
      "train loss:0.15187581195597344\n",
      "train loss:0.11926741565439103\n",
      "train loss:0.10848317374536127\n",
      "=== epoch:92, train acc:0.9933333333333333, test acc:0.7435 ===\n",
      "train loss:0.07813789929187243\n",
      "train loss:0.11613564085895682\n",
      "train loss:0.12298279791047001\n",
      "=== epoch:93, train acc:0.9933333333333333, test acc:0.7429 ===\n",
      "train loss:0.1240567161157749\n",
      "train loss:0.12604635792061453\n",
      "train loss:0.10146725264897366\n",
      "=== epoch:94, train acc:0.9933333333333333, test acc:0.7493 ===\n",
      "train loss:0.10835032013244458\n",
      "train loss:0.11493573672727393\n",
      "train loss:0.10584726569739417\n",
      "=== epoch:95, train acc:0.9966666666666667, test acc:0.7461 ===\n",
      "train loss:0.08714078663991978\n",
      "train loss:0.10431015273976339\n",
      "train loss:0.12387631356440576\n",
      "=== epoch:96, train acc:0.9966666666666667, test acc:0.7515 ===\n",
      "train loss:0.1286425917228613\n",
      "train loss:0.10150120855887726\n",
      "train loss:0.07737149664191895\n",
      "=== epoch:97, train acc:0.9933333333333333, test acc:0.7478 ===\n",
      "train loss:0.09668554936370681\n",
      "train loss:0.11026198881554286\n",
      "train loss:0.09732599729286562\n",
      "=== epoch:98, train acc:0.9966666666666667, test acc:0.751 ===\n",
      "train loss:0.10798068535314942\n",
      "train loss:0.11008757886979259\n",
      "train loss:0.08316470037358822\n",
      "=== epoch:99, train acc:0.9933333333333333, test acc:0.7491 ===\n",
      "train loss:0.08506248825526751\n",
      "train loss:0.08816483057495185\n",
      "train loss:0.10196661716979927\n",
      "=== epoch:100, train acc:0.9966666666666667, test acc:0.7522 ===\n",
      "train loss:0.07803353331478714\n",
      "train loss:0.08772074426487597\n",
      "train loss:0.09182892645348889\n",
      "=== epoch:101, train acc:0.9966666666666667, test acc:0.7523 ===\n",
      "train loss:0.09958145669564807\n",
      "train loss:0.0860888199976219\n",
      "train loss:0.09699217289205088\n",
      "=== epoch:102, train acc:1.0, test acc:0.7522 ===\n",
      "train loss:0.07324849277366002\n",
      "train loss:0.07897902254470385\n",
      "train loss:0.10188181268915601\n",
      "=== epoch:103, train acc:1.0, test acc:0.7541 ===\n",
      "train loss:0.05603251715188084\n",
      "train loss:0.08311451607900726\n",
      "train loss:0.06159136017294582\n",
      "=== epoch:104, train acc:0.9966666666666667, test acc:0.7555 ===\n",
      "train loss:0.08236419298159324\n",
      "train loss:0.07764115703849142\n",
      "train loss:0.09165016118236771\n",
      "=== epoch:105, train acc:1.0, test acc:0.7557 ===\n",
      "train loss:0.06227016069631066\n",
      "train loss:0.09079246779055912\n",
      "train loss:0.08329130660307982\n",
      "=== epoch:106, train acc:0.9966666666666667, test acc:0.7508 ===\n",
      "train loss:0.06989683269501165\n",
      "train loss:0.0982382970440959\n",
      "train loss:0.06447393916590591\n",
      "=== epoch:107, train acc:0.9966666666666667, test acc:0.753 ===\n",
      "train loss:0.07772407913690636\n",
      "train loss:0.05289781801506254\n",
      "train loss:0.07544150227137608\n",
      "=== epoch:108, train acc:0.9966666666666667, test acc:0.7505 ===\n",
      "train loss:0.0804164792333778\n",
      "train loss:0.07008728815365475\n",
      "train loss:0.06328500132054998\n",
      "=== epoch:109, train acc:1.0, test acc:0.7484 ===\n",
      "train loss:0.04997965516488087\n",
      "train loss:0.06865643546440943\n",
      "train loss:0.08363828153881764\n",
      "=== epoch:110, train acc:1.0, test acc:0.7524 ===\n",
      "train loss:0.07369046658849393\n",
      "train loss:0.06566999556659218\n",
      "train loss:0.06403431132698519\n",
      "=== epoch:111, train acc:0.9966666666666667, test acc:0.7549 ===\n",
      "train loss:0.07799533959377614\n",
      "train loss:0.06326455126655381\n",
      "train loss:0.07322625391556951\n",
      "=== epoch:112, train acc:1.0, test acc:0.7533 ===\n",
      "train loss:0.08414041004705267\n",
      "train loss:0.05289089000081619\n",
      "train loss:0.06641198380821633\n",
      "=== epoch:113, train acc:0.9966666666666667, test acc:0.754 ===\n",
      "train loss:0.07062824492992306\n",
      "train loss:0.07834900405108812\n",
      "train loss:0.047188455547487095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:114, train acc:1.0, test acc:0.7543 ===\n",
      "train loss:0.0713933419710402\n",
      "train loss:0.052697621613532364\n",
      "train loss:0.049437810643248356\n",
      "=== epoch:115, train acc:1.0, test acc:0.7565 ===\n",
      "train loss:0.05313865034629605\n",
      "train loss:0.054547953762678125\n",
      "train loss:0.05800536894276069\n",
      "=== epoch:116, train acc:1.0, test acc:0.754 ===\n",
      "train loss:0.044720285955637484\n",
      "train loss:0.0538556330769772\n",
      "train loss:0.04827945824453634\n",
      "=== epoch:117, train acc:0.9966666666666667, test acc:0.755 ===\n",
      "train loss:0.06948153781107509\n",
      "train loss:0.06622502609316384\n",
      "train loss:0.06319323376576472\n",
      "=== epoch:118, train acc:0.9966666666666667, test acc:0.754 ===\n",
      "train loss:0.05901554815114606\n",
      "train loss:0.06321737667051493\n",
      "train loss:0.04605865711443536\n",
      "=== epoch:119, train acc:0.9966666666666667, test acc:0.7559 ===\n",
      "train loss:0.06127100577011874\n",
      "train loss:0.08778498476994351\n",
      "train loss:0.04477317122365543\n",
      "=== epoch:120, train acc:1.0, test acc:0.7568 ===\n",
      "train loss:0.061098452571099575\n",
      "train loss:0.06374078116017781\n",
      "train loss:0.057076833451545045\n",
      "=== epoch:121, train acc:1.0, test acc:0.7562 ===\n",
      "train loss:0.055857797416467125\n",
      "train loss:0.03708480469487858\n",
      "train loss:0.06472141695828022\n",
      "=== epoch:122, train acc:1.0, test acc:0.7557 ===\n",
      "train loss:0.05616457828416647\n",
      "train loss:0.04598450273981703\n",
      "train loss:0.04077950152271973\n",
      "=== epoch:123, train acc:1.0, test acc:0.7565 ===\n",
      "train loss:0.05949424815712517\n",
      "train loss:0.06283297161247849\n",
      "train loss:0.04696360147616829\n",
      "=== epoch:124, train acc:1.0, test acc:0.7549 ===\n",
      "train loss:0.056139675328424614\n",
      "train loss:0.04832016397438157\n",
      "train loss:0.04106965112644177\n",
      "=== epoch:125, train acc:0.9966666666666667, test acc:0.7569 ===\n",
      "train loss:0.0482500488104196\n",
      "train loss:0.040026657751380305\n",
      "train loss:0.054678093037914176\n",
      "=== epoch:126, train acc:0.9966666666666667, test acc:0.7567 ===\n",
      "train loss:0.053181020900559206\n",
      "train loss:0.04705400433980114\n",
      "train loss:0.05232232376938573\n",
      "=== epoch:127, train acc:1.0, test acc:0.7558 ===\n",
      "train loss:0.04724373352232151\n",
      "train loss:0.0684581690455309\n",
      "train loss:0.048896181952132196\n",
      "=== epoch:128, train acc:1.0, test acc:0.7556 ===\n",
      "train loss:0.04924187207567513\n",
      "train loss:0.04218022469858744\n",
      "train loss:0.06088982982900009\n",
      "=== epoch:129, train acc:1.0, test acc:0.7552 ===\n",
      "train loss:0.0562383170173504\n",
      "train loss:0.043185602690587827\n",
      "train loss:0.05024006564403345\n",
      "=== epoch:130, train acc:1.0, test acc:0.7575 ===\n",
      "train loss:0.045959643104269636\n",
      "train loss:0.043265651704011934\n",
      "train loss:0.03638378825895625\n",
      "=== epoch:131, train acc:0.9966666666666667, test acc:0.7563 ===\n",
      "train loss:0.044716913976843436\n",
      "train loss:0.052934421606512655\n",
      "train loss:0.04584009008401972\n",
      "=== epoch:132, train acc:1.0, test acc:0.7592 ===\n",
      "train loss:0.03740291463187781\n",
      "train loss:0.04679850587032343\n",
      "train loss:0.052041174804687035\n",
      "=== epoch:133, train acc:1.0, test acc:0.7583 ===\n",
      "train loss:0.03544431048195179\n",
      "train loss:0.057670134816859\n",
      "train loss:0.052254449288536266\n",
      "=== epoch:134, train acc:1.0, test acc:0.7584 ===\n",
      "train loss:0.03161848890719691\n",
      "train loss:0.05025491478076754\n",
      "train loss:0.03787335371810704\n",
      "=== epoch:135, train acc:1.0, test acc:0.761 ===\n",
      "train loss:0.039379759205764864\n",
      "train loss:0.04615209466534376\n",
      "train loss:0.05163849189214437\n",
      "=== epoch:136, train acc:1.0, test acc:0.7602 ===\n",
      "train loss:0.04026847702694662\n",
      "train loss:0.04185431246197279\n",
      "train loss:0.05084975371873299\n",
      "=== epoch:137, train acc:1.0, test acc:0.7593 ===\n",
      "train loss:0.03754347856664371\n",
      "train loss:0.039230430576231386\n",
      "train loss:0.045723632581426796\n",
      "=== epoch:138, train acc:1.0, test acc:0.756 ===\n",
      "train loss:0.03221762084763823\n",
      "train loss:0.0478262608140644\n",
      "train loss:0.04435356478294662\n",
      "=== epoch:139, train acc:1.0, test acc:0.7587 ===\n",
      "train loss:0.03799108078331484\n",
      "train loss:0.034652527995912535\n",
      "train loss:0.030435157036332025\n",
      "=== epoch:140, train acc:1.0, test acc:0.757 ===\n",
      "train loss:0.0336310956213499\n",
      "train loss:0.03731072174976241\n",
      "train loss:0.05652260691511666\n",
      "=== epoch:141, train acc:1.0, test acc:0.7585 ===\n",
      "train loss:0.0382513051529392\n",
      "train loss:0.03323579798052903\n",
      "train loss:0.04628955572896795\n",
      "=== epoch:142, train acc:1.0, test acc:0.7614 ===\n",
      "train loss:0.03350098062666395\n",
      "train loss:0.03233453806024986\n",
      "train loss:0.025696776505896444\n",
      "=== epoch:143, train acc:1.0, test acc:0.7621 ===\n",
      "train loss:0.033636148474368015\n",
      "train loss:0.03784002507314903\n",
      "train loss:0.039944830889161995\n",
      "=== epoch:144, train acc:1.0, test acc:0.7586 ===\n",
      "train loss:0.03709471926083328\n",
      "train loss:0.030662584991261538\n",
      "train loss:0.033730920215808376\n",
      "=== epoch:145, train acc:1.0, test acc:0.7575 ===\n",
      "train loss:0.043415933950176205\n",
      "train loss:0.02937733006777811\n",
      "train loss:0.033373270017867294\n",
      "=== epoch:146, train acc:1.0, test acc:0.7583 ===\n",
      "train loss:0.032625347876559754\n",
      "train loss:0.05803354422723304\n",
      "train loss:0.03524931236372035\n",
      "=== epoch:147, train acc:1.0, test acc:0.76 ===\n",
      "train loss:0.035583715086179385\n",
      "train loss:0.05150366078929224\n",
      "train loss:0.0332512080981576\n",
      "=== epoch:148, train acc:1.0, test acc:0.7592 ===\n",
      "train loss:0.03096821590142863\n",
      "train loss:0.03527824824621586\n",
      "train loss:0.03286465496554286\n",
      "=== epoch:149, train acc:1.0, test acc:0.7598 ===\n",
      "train loss:0.05076380000174248\n",
      "train loss:0.037251040577545\n",
      "train loss:0.02430122071773706\n",
      "=== epoch:150, train acc:1.0, test acc:0.7607 ===\n",
      "train loss:0.026355224493073936\n",
      "train loss:0.03826838959481141\n",
      "train loss:0.04422381466035492\n",
      "=== epoch:151, train acc:1.0, test acc:0.7614 ===\n",
      "train loss:0.03550506145671461\n",
      "train loss:0.04146353391540467\n",
      "train loss:0.0382188615514166\n",
      "=== epoch:152, train acc:1.0, test acc:0.7615 ===\n",
      "train loss:0.04291206643098119\n",
      "train loss:0.04070603995115002\n",
      "train loss:0.03136615188547251\n",
      "=== epoch:153, train acc:1.0, test acc:0.7591 ===\n",
      "train loss:0.027513747848272863\n",
      "train loss:0.03411169622395594\n",
      "train loss:0.0304933212571393\n",
      "=== epoch:154, train acc:1.0, test acc:0.7604 ===\n",
      "train loss:0.03033976075738259\n",
      "train loss:0.0336635758173607\n",
      "train loss:0.038665954344361396\n",
      "=== epoch:155, train acc:1.0, test acc:0.7617 ===\n",
      "train loss:0.024977996946808228\n",
      "train loss:0.03387276341392079\n",
      "train loss:0.03430024688326888\n",
      "=== epoch:156, train acc:1.0, test acc:0.7619 ===\n",
      "train loss:0.05454378120865571\n",
      "train loss:0.03442666991234721\n",
      "train loss:0.039824941506644965\n",
      "=== epoch:157, train acc:1.0, test acc:0.7616 ===\n",
      "train loss:0.02949567593404629\n",
      "train loss:0.046201062458904145\n",
      "train loss:0.028759373174024973\n",
      "=== epoch:158, train acc:1.0, test acc:0.7609 ===\n",
      "train loss:0.03340987765451285\n",
      "train loss:0.02391874966818449\n",
      "train loss:0.035261961965859984\n",
      "=== epoch:159, train acc:1.0, test acc:0.7628 ===\n",
      "train loss:0.036383267293916215\n",
      "train loss:0.02354557207827577\n",
      "train loss:0.03272989506059358\n",
      "=== epoch:160, train acc:1.0, test acc:0.761 ===\n",
      "train loss:0.029807566845792093\n",
      "train loss:0.025896466628214813\n",
      "train loss:0.029365323868557214\n",
      "=== epoch:161, train acc:1.0, test acc:0.7625 ===\n",
      "train loss:0.029500102223334677\n",
      "train loss:0.04160938634210502\n",
      "train loss:0.03474413095850655\n",
      "=== epoch:162, train acc:1.0, test acc:0.7607 ===\n",
      "train loss:0.0365126739250087\n",
      "train loss:0.0379475803438283\n",
      "train loss:0.02170191337376746\n",
      "=== epoch:163, train acc:1.0, test acc:0.7626 ===\n",
      "train loss:0.03250737721932592\n",
      "train loss:0.039465994746959415\n",
      "train loss:0.027176851939450478\n",
      "=== epoch:164, train acc:1.0, test acc:0.7628 ===\n",
      "train loss:0.027898463075192934\n",
      "train loss:0.030723757199732783\n",
      "train loss:0.030583220069190693\n",
      "=== epoch:165, train acc:1.0, test acc:0.7629 ===\n",
      "train loss:0.028495669322793734\n",
      "train loss:0.025935955831819665\n",
      "train loss:0.02138652044032969\n",
      "=== epoch:166, train acc:1.0, test acc:0.7635 ===\n",
      "train loss:0.031297015080082036\n",
      "train loss:0.028466955636105577\n",
      "train loss:0.03663442495092735\n",
      "=== epoch:167, train acc:1.0, test acc:0.7633 ===\n",
      "train loss:0.031054073908284217\n",
      "train loss:0.020437362315308655\n",
      "train loss:0.028410710898753053\n",
      "=== epoch:168, train acc:1.0, test acc:0.7613 ===\n",
      "train loss:0.025519671595772942\n",
      "train loss:0.028200121369854364\n",
      "train loss:0.024586989845482216\n",
      "=== epoch:169, train acc:1.0, test acc:0.7642 ===\n",
      "train loss:0.02472520526097525\n",
      "train loss:0.022327030932896494\n",
      "train loss:0.029874326630352117\n",
      "=== epoch:170, train acc:1.0, test acc:0.7644 ===\n",
      "train loss:0.024430216413995308\n",
      "train loss:0.024075342305758043\n",
      "train loss:0.019734021136640533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:171, train acc:1.0, test acc:0.7654 ===\n",
      "train loss:0.02479886991073087\n",
      "train loss:0.02701067983425585\n",
      "train loss:0.026348212436995363\n",
      "=== epoch:172, train acc:1.0, test acc:0.7637 ===\n",
      "train loss:0.02204786654776524\n",
      "train loss:0.028978647567994963\n",
      "train loss:0.02327702578165296\n",
      "=== epoch:173, train acc:1.0, test acc:0.7655 ===\n",
      "train loss:0.023497650922161765\n",
      "train loss:0.0306964752374646\n",
      "train loss:0.02364831898205216\n",
      "=== epoch:174, train acc:1.0, test acc:0.7642 ===\n",
      "train loss:0.02790798536928217\n",
      "train loss:0.026647622846801233\n",
      "train loss:0.016274716363087704\n",
      "=== epoch:175, train acc:1.0, test acc:0.7638 ===\n",
      "train loss:0.03070156761589607\n",
      "train loss:0.028795392032264475\n",
      "train loss:0.020466099834990398\n",
      "=== epoch:176, train acc:1.0, test acc:0.7614 ===\n",
      "train loss:0.02030841885841329\n",
      "train loss:0.023212076465234145\n",
      "train loss:0.02231281110900489\n",
      "=== epoch:177, train acc:1.0, test acc:0.7626 ===\n",
      "train loss:0.029182282966251697\n",
      "train loss:0.0184538371360748\n",
      "train loss:0.02676414443709247\n",
      "=== epoch:178, train acc:1.0, test acc:0.7633 ===\n",
      "train loss:0.01592468483845793\n",
      "train loss:0.022239100186187962\n",
      "train loss:0.022471413292253176\n",
      "=== epoch:179, train acc:1.0, test acc:0.7629 ===\n",
      "train loss:0.02243017404475238\n",
      "train loss:0.023214507396084008\n",
      "train loss:0.02691571470986457\n",
      "=== epoch:180, train acc:1.0, test acc:0.7638 ===\n",
      "train loss:0.027252683403641044\n",
      "train loss:0.018325256425265782\n",
      "train loss:0.02388078806258617\n",
      "=== epoch:181, train acc:1.0, test acc:0.7624 ===\n",
      "train loss:0.018820866297145522\n",
      "train loss:0.019978693567630115\n",
      "train loss:0.020602661804299864\n",
      "=== epoch:182, train acc:1.0, test acc:0.7619 ===\n",
      "train loss:0.02740819897112097\n",
      "train loss:0.024829033265823277\n",
      "train loss:0.01880222814668209\n",
      "=== epoch:183, train acc:1.0, test acc:0.7632 ===\n",
      "train loss:0.023234393796260647\n",
      "train loss:0.016545827435381112\n",
      "train loss:0.02425669218790195\n",
      "=== epoch:184, train acc:1.0, test acc:0.763 ===\n",
      "train loss:0.019866346457301405\n",
      "train loss:0.03290731443922497\n",
      "train loss:0.026715449207693812\n",
      "=== epoch:185, train acc:1.0, test acc:0.7638 ===\n",
      "train loss:0.024355634719776104\n",
      "train loss:0.02240293321203371\n",
      "train loss:0.024948333323124107\n",
      "=== epoch:186, train acc:1.0, test acc:0.7648 ===\n",
      "train loss:0.019693595385298846\n",
      "train loss:0.017871246123518265\n",
      "train loss:0.025433955906558153\n",
      "=== epoch:187, train acc:1.0, test acc:0.7637 ===\n",
      "train loss:0.025018489113952422\n",
      "train loss:0.0180406484234212\n",
      "train loss:0.020761151235541325\n",
      "=== epoch:188, train acc:1.0, test acc:0.7644 ===\n",
      "train loss:0.019092571886847983\n",
      "train loss:0.023854023683656987\n",
      "train loss:0.01734155874907942\n",
      "=== epoch:189, train acc:1.0, test acc:0.7634 ===\n",
      "train loss:0.015156315426674373\n",
      "train loss:0.020345350808871624\n",
      "train loss:0.01957142452428268\n",
      "=== epoch:190, train acc:1.0, test acc:0.765 ===\n",
      "train loss:0.022849813820455908\n",
      "train loss:0.03069109795781572\n",
      "train loss:0.023810295517270902\n",
      "=== epoch:191, train acc:1.0, test acc:0.7649 ===\n",
      "train loss:0.014837550869824663\n",
      "train loss:0.021470911947678514\n",
      "train loss:0.01974230119873143\n",
      "=== epoch:192, train acc:1.0, test acc:0.764 ===\n",
      "train loss:0.021793396065709295\n",
      "train loss:0.02328695652192869\n",
      "train loss:0.022851496827570154\n",
      "=== epoch:193, train acc:1.0, test acc:0.7644 ===\n",
      "train loss:0.021708577295589254\n",
      "train loss:0.028608255220236037\n",
      "train loss:0.018528008108331773\n",
      "=== epoch:194, train acc:1.0, test acc:0.7644 ===\n",
      "train loss:0.01781009075834137\n",
      "train loss:0.023717178970633267\n",
      "train loss:0.021743961157022363\n",
      "=== epoch:195, train acc:1.0, test acc:0.7639 ===\n",
      "train loss:0.017014654901035396\n",
      "train loss:0.01541459311115675\n",
      "train loss:0.01941514276039982\n",
      "=== epoch:196, train acc:1.0, test acc:0.7631 ===\n",
      "train loss:0.019797941208765414\n",
      "train loss:0.0186073716966398\n",
      "train loss:0.02263576141034558\n",
      "=== epoch:197, train acc:1.0, test acc:0.7637 ===\n",
      "train loss:0.026037793063246606\n",
      "train loss:0.023810892567663937\n",
      "train loss:0.020661218517294327\n",
      "=== epoch:198, train acc:1.0, test acc:0.7639 ===\n",
      "train loss:0.023213761943409583\n",
      "train loss:0.02948653748412644\n",
      "train loss:0.0196456991050965\n",
      "=== epoch:199, train acc:1.0, test acc:0.7638 ===\n",
      "train loss:0.023077367953029945\n",
      "train loss:0.01906083459650563\n",
      "train loss:0.01723435643732203\n",
      "=== epoch:200, train acc:1.0, test acc:0.7645 ===\n",
      "train loss:0.01723366636935761\n",
      "train loss:0.016249777965599455\n",
      "train loss:0.020775290163024672\n",
      "=== epoch:201, train acc:1.0, test acc:0.7634 ===\n",
      "train loss:0.02121941301076459\n",
      "train loss:0.019581218565421474\n",
      "train loss:0.02078604313448884\n",
      "=== epoch:202, train acc:1.0, test acc:0.763 ===\n",
      "train loss:0.023054043487939175\n",
      "train loss:0.015427058342808967\n",
      "train loss:0.018557439749152155\n",
      "=== epoch:203, train acc:1.0, test acc:0.7639 ===\n",
      "train loss:0.01972141284032028\n",
      "train loss:0.017264545731404884\n",
      "train loss:0.020829127150291712\n",
      "=== epoch:204, train acc:1.0, test acc:0.7644 ===\n",
      "train loss:0.01723859691971836\n",
      "train loss:0.018579432531715782\n",
      "train loss:0.026671222515573984\n",
      "=== epoch:205, train acc:1.0, test acc:0.7631 ===\n",
      "train loss:0.01854842851332684\n",
      "train loss:0.017400936148421888\n",
      "train loss:0.014535107472055473\n",
      "=== epoch:206, train acc:1.0, test acc:0.7647 ===\n",
      "train loss:0.014215333894513621\n",
      "train loss:0.019511459989178213\n",
      "train loss:0.017769273185534225\n",
      "=== epoch:207, train acc:1.0, test acc:0.7656 ===\n",
      "train loss:0.016135895653968432\n",
      "train loss:0.01545717067326392\n",
      "train loss:0.017407572715242906\n",
      "=== epoch:208, train acc:1.0, test acc:0.7646 ===\n",
      "train loss:0.012285586874875307\n",
      "train loss:0.013661228684409204\n",
      "train loss:0.019224827710873443\n",
      "=== epoch:209, train acc:1.0, test acc:0.7657 ===\n",
      "train loss:0.016922577349551347\n",
      "train loss:0.01516271272749218\n",
      "train loss:0.015661278620480973\n",
      "=== epoch:210, train acc:1.0, test acc:0.7645 ===\n",
      "train loss:0.0204816320919794\n",
      "train loss:0.016221172762992044\n",
      "train loss:0.016268861973118512\n",
      "=== epoch:211, train acc:1.0, test acc:0.763 ===\n",
      "train loss:0.012813708557797714\n",
      "train loss:0.01250698291336367\n",
      "train loss:0.015203938737783576\n",
      "=== epoch:212, train acc:1.0, test acc:0.7646 ===\n",
      "train loss:0.015350524911675966\n",
      "train loss:0.014575422791296288\n",
      "train loss:0.020964523072659263\n",
      "=== epoch:213, train acc:1.0, test acc:0.7646 ===\n",
      "train loss:0.01591159168187625\n",
      "train loss:0.023013405164147502\n",
      "train loss:0.023106483853681005\n",
      "=== epoch:214, train acc:1.0, test acc:0.7653 ===\n",
      "train loss:0.01755920434384298\n",
      "train loss:0.016774356737898753\n",
      "train loss:0.018643024416406404\n",
      "=== epoch:215, train acc:1.0, test acc:0.7634 ===\n",
      "train loss:0.014240352388345487\n",
      "train loss:0.012616088787884294\n",
      "train loss:0.017371845757371204\n",
      "=== epoch:216, train acc:1.0, test acc:0.7639 ===\n",
      "train loss:0.01710715532472066\n",
      "train loss:0.017233365172853832\n",
      "train loss:0.01178715811501764\n",
      "=== epoch:217, train acc:1.0, test acc:0.7642 ===\n",
      "train loss:0.011456062727677387\n",
      "train loss:0.01506150679162336\n",
      "train loss:0.014433950852249888\n",
      "=== epoch:218, train acc:1.0, test acc:0.7645 ===\n",
      "train loss:0.01955982784892554\n",
      "train loss:0.011617242654593301\n",
      "train loss:0.01615762169003705\n",
      "=== epoch:219, train acc:1.0, test acc:0.7639 ===\n",
      "train loss:0.020149282713212462\n",
      "train loss:0.014670874106784533\n",
      "train loss:0.01721568029753638\n",
      "=== epoch:220, train acc:1.0, test acc:0.7639 ===\n",
      "train loss:0.015920544554839166\n",
      "train loss:0.014652432450463847\n",
      "train loss:0.011970580249270817\n",
      "=== epoch:221, train acc:1.0, test acc:0.7645 ===\n",
      "train loss:0.021928070803320977\n",
      "train loss:0.01908614724432544\n",
      "train loss:0.01753035335645171\n",
      "=== epoch:222, train acc:1.0, test acc:0.7646 ===\n",
      "train loss:0.012983684297894305\n",
      "train loss:0.012331613910407985\n",
      "train loss:0.018028543909393437\n",
      "=== epoch:223, train acc:1.0, test acc:0.765 ===\n",
      "train loss:0.01154859279080707\n",
      "train loss:0.01674223466197663\n",
      "train loss:0.019201386614696565\n",
      "=== epoch:224, train acc:1.0, test acc:0.7656 ===\n",
      "train loss:0.011096696958636769\n",
      "train loss:0.01703960403119349\n",
      "train loss:0.014276647770129394\n",
      "=== epoch:225, train acc:1.0, test acc:0.7647 ===\n",
      "train loss:0.01053416851516459\n",
      "train loss:0.01294228628927089\n",
      "train loss:0.013991730369430168\n",
      "=== epoch:226, train acc:1.0, test acc:0.7652 ===\n",
      "train loss:0.01688129627054428\n",
      "train loss:0.01436644049013833\n",
      "train loss:0.01587420025949271\n",
      "=== epoch:227, train acc:1.0, test acc:0.7662 ===\n",
      "train loss:0.010609947580031176\n",
      "train loss:0.017194727616755046\n",
      "train loss:0.010750616884142245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:228, train acc:1.0, test acc:0.7652 ===\n",
      "train loss:0.01885419605070927\n",
      "train loss:0.017535121194841612\n",
      "train loss:0.01839994511763643\n",
      "=== epoch:229, train acc:1.0, test acc:0.7663 ===\n",
      "train loss:0.017719351500486688\n",
      "train loss:0.01591105146686008\n",
      "train loss:0.015295499840720596\n",
      "=== epoch:230, train acc:1.0, test acc:0.7636 ===\n",
      "train loss:0.016915716596275845\n",
      "train loss:0.015262859303272966\n",
      "train loss:0.01899773260131375\n",
      "=== epoch:231, train acc:1.0, test acc:0.7661 ===\n",
      "train loss:0.014627453026152917\n",
      "train loss:0.00974294510784097\n",
      "train loss:0.013975397279163644\n",
      "=== epoch:232, train acc:1.0, test acc:0.7659 ===\n",
      "train loss:0.011885410253454845\n",
      "train loss:0.012332048489474673\n",
      "train loss:0.010850255663416925\n",
      "=== epoch:233, train acc:1.0, test acc:0.7653 ===\n",
      "train loss:0.014667337038931699\n",
      "train loss:0.010192993962051746\n",
      "train loss:0.01779850209794944\n",
      "=== epoch:234, train acc:1.0, test acc:0.7658 ===\n",
      "train loss:0.014078029495418304\n",
      "train loss:0.015340208520079168\n",
      "train loss:0.014463746016621757\n",
      "=== epoch:235, train acc:1.0, test acc:0.7657 ===\n",
      "train loss:0.017780182669274756\n",
      "train loss:0.012116977722042533\n",
      "train loss:0.011946571712737789\n",
      "=== epoch:236, train acc:1.0, test acc:0.7659 ===\n",
      "train loss:0.014292298842664544\n",
      "train loss:0.014830887940016952\n",
      "train loss:0.015015464635447623\n",
      "=== epoch:237, train acc:1.0, test acc:0.7655 ===\n",
      "train loss:0.011992747159694202\n",
      "train loss:0.010179134759314557\n",
      "train loss:0.019893904318052064\n",
      "=== epoch:238, train acc:1.0, test acc:0.7648 ===\n",
      "train loss:0.011720023551784708\n",
      "train loss:0.012309193953372934\n",
      "train loss:0.015299696405895725\n",
      "=== epoch:239, train acc:1.0, test acc:0.7665 ===\n",
      "train loss:0.018122273548037783\n",
      "train loss:0.013711896903121523\n",
      "train loss:0.01325027083158181\n",
      "=== epoch:240, train acc:1.0, test acc:0.7655 ===\n",
      "train loss:0.016706532549875672\n",
      "train loss:0.012768829322749288\n",
      "train loss:0.010080029298266978\n",
      "=== epoch:241, train acc:1.0, test acc:0.7667 ===\n",
      "train loss:0.011435514587743422\n",
      "train loss:0.010140173334216209\n",
      "train loss:0.013231023465729044\n",
      "=== epoch:242, train acc:1.0, test acc:0.7672 ===\n",
      "train loss:0.013484494729561959\n",
      "train loss:0.012608341905260041\n",
      "train loss:0.015256286042313452\n",
      "=== epoch:243, train acc:1.0, test acc:0.767 ===\n",
      "train loss:0.014992476186184262\n",
      "train loss:0.014601854132930773\n",
      "train loss:0.008866917817053756\n",
      "=== epoch:244, train acc:1.0, test acc:0.7661 ===\n",
      "train loss:0.014494428186994555\n",
      "train loss:0.012738985793986815\n",
      "train loss:0.011233503419251445\n",
      "=== epoch:245, train acc:1.0, test acc:0.7655 ===\n",
      "train loss:0.0145372426064813\n",
      "train loss:0.01015773003919346\n",
      "train loss:0.016297546610317456\n",
      "=== epoch:246, train acc:1.0, test acc:0.7656 ===\n",
      "train loss:0.015920702615577702\n",
      "train loss:0.011550881295382285\n",
      "train loss:0.015381846463856501\n",
      "=== epoch:247, train acc:1.0, test acc:0.7659 ===\n",
      "train loss:0.010289497753935672\n",
      "train loss:0.013785824773696038\n",
      "train loss:0.014373076559510171\n",
      "=== epoch:248, train acc:1.0, test acc:0.7657 ===\n",
      "train loss:0.012489509312483866\n",
      "train loss:0.011810609544240064\n",
      "train loss:0.013370401731449086\n",
      "=== epoch:249, train acc:1.0, test acc:0.7665 ===\n",
      "train loss:0.0147876958016381\n",
      "train loss:0.007825425750929905\n",
      "train loss:0.015409800033377061\n",
      "=== epoch:250, train acc:1.0, test acc:0.7666 ===\n",
      "train loss:0.015391980210592333\n",
      "train loss:0.012540797429093616\n",
      "train loss:0.013009264015669623\n",
      "=== epoch:251, train acc:1.0, test acc:0.7646 ===\n",
      "train loss:0.00964282714178654\n",
      "train loss:0.014759035800440086\n",
      "train loss:0.011957162723274028\n",
      "=== epoch:252, train acc:1.0, test acc:0.7656 ===\n",
      "train loss:0.014441760969103445\n",
      "train loss:0.010157330307856245\n",
      "train loss:0.010981553724756262\n",
      "=== epoch:253, train acc:1.0, test acc:0.766 ===\n",
      "train loss:0.009179286432503283\n",
      "train loss:0.013043575664691491\n",
      "train loss:0.01131516095097997\n",
      "=== epoch:254, train acc:1.0, test acc:0.7672 ===\n",
      "train loss:0.011766557460545784\n",
      "train loss:0.013365006310029697\n",
      "train loss:0.010453701721475424\n",
      "=== epoch:255, train acc:1.0, test acc:0.7668 ===\n",
      "train loss:0.011134687602860276\n",
      "train loss:0.013793541799445858\n",
      "train loss:0.010777333832792392\n",
      "=== epoch:256, train acc:1.0, test acc:0.7662 ===\n",
      "train loss:0.013817630046087347\n",
      "train loss:0.01148463149500919\n",
      "train loss:0.010995957179083339\n",
      "=== epoch:257, train acc:1.0, test acc:0.7669 ===\n",
      "train loss:0.013723099588244601\n",
      "train loss:0.012879387896948783\n",
      "train loss:0.011864596329942145\n",
      "=== epoch:258, train acc:1.0, test acc:0.7671 ===\n",
      "train loss:0.010260912305374015\n",
      "train loss:0.01400941610242032\n",
      "train loss:0.008489618164674542\n",
      "=== epoch:259, train acc:1.0, test acc:0.767 ===\n",
      "train loss:0.009362487036234803\n",
      "train loss:0.013147961544787357\n",
      "train loss:0.012137896104045056\n",
      "=== epoch:260, train acc:1.0, test acc:0.7674 ===\n",
      "train loss:0.010979445244112818\n",
      "train loss:0.010801044056972686\n",
      "train loss:0.014098843707890198\n",
      "=== epoch:261, train acc:1.0, test acc:0.7671 ===\n",
      "train loss:0.009490162287373498\n",
      "train loss:0.01318603050265369\n",
      "train loss:0.011090837316358215\n",
      "=== epoch:262, train acc:1.0, test acc:0.7677 ===\n",
      "train loss:0.00987896562681176\n",
      "train loss:0.009818232693464718\n",
      "train loss:0.012715840118849343\n",
      "=== epoch:263, train acc:1.0, test acc:0.7673 ===\n",
      "train loss:0.009863598762838908\n",
      "train loss:0.009577425086643507\n",
      "train loss:0.00995683132901563\n",
      "=== epoch:264, train acc:1.0, test acc:0.7665 ===\n",
      "train loss:0.009281370866145942\n",
      "train loss:0.010831340188928977\n",
      "train loss:0.009079830866206342\n",
      "=== epoch:265, train acc:1.0, test acc:0.7669 ===\n",
      "train loss:0.007446367702534689\n",
      "train loss:0.011939606631445266\n",
      "train loss:0.011772777928779614\n",
      "=== epoch:266, train acc:1.0, test acc:0.7673 ===\n",
      "train loss:0.011134602228325385\n",
      "train loss:0.0119077139205944\n",
      "train loss:0.013069334380282077\n",
      "=== epoch:267, train acc:1.0, test acc:0.7666 ===\n",
      "train loss:0.011412212151778473\n",
      "train loss:0.010598640764063826\n",
      "train loss:0.009432375939875932\n",
      "=== epoch:268, train acc:1.0, test acc:0.7677 ===\n",
      "train loss:0.01111858281464135\n",
      "train loss:0.007797980464129343\n",
      "train loss:0.011774439304763802\n",
      "=== epoch:269, train acc:1.0, test acc:0.7671 ===\n",
      "train loss:0.012614757160252598\n",
      "train loss:0.00898107917796209\n",
      "train loss:0.012327118351535757\n",
      "=== epoch:270, train acc:1.0, test acc:0.7669 ===\n",
      "train loss:0.011253315304867829\n",
      "train loss:0.012689923279294728\n",
      "train loss:0.009266502586725952\n",
      "=== epoch:271, train acc:1.0, test acc:0.7671 ===\n",
      "train loss:0.009840235667287301\n",
      "train loss:0.008003399061115204\n",
      "train loss:0.010953340645201861\n",
      "=== epoch:272, train acc:1.0, test acc:0.7668 ===\n",
      "train loss:0.008868554583602686\n",
      "train loss:0.009649360387212914\n",
      "train loss:0.013637018032034398\n",
      "=== epoch:273, train acc:1.0, test acc:0.7676 ===\n",
      "train loss:0.012062350819614119\n",
      "train loss:0.01121599852539582\n",
      "train loss:0.010466080492188223\n",
      "=== epoch:274, train acc:1.0, test acc:0.7674 ===\n",
      "train loss:0.014457107773863427\n",
      "train loss:0.008826775310735\n",
      "train loss:0.010847622111435038\n",
      "=== epoch:275, train acc:1.0, test acc:0.7672 ===\n",
      "train loss:0.01013767535621134\n",
      "train loss:0.011029390931450882\n",
      "train loss:0.009634495929718793\n",
      "=== epoch:276, train acc:1.0, test acc:0.7681 ===\n",
      "train loss:0.009932153701340532\n",
      "train loss:0.010755167836272816\n",
      "train loss:0.009539583633843137\n",
      "=== epoch:277, train acc:1.0, test acc:0.7671 ===\n",
      "train loss:0.00983474782245958\n",
      "train loss:0.00894715408893389\n",
      "train loss:0.01048898834028021\n",
      "=== epoch:278, train acc:1.0, test acc:0.7672 ===\n",
      "train loss:0.011992204004838126\n",
      "train loss:0.008793728017403375\n",
      "train loss:0.010250111874564254\n",
      "=== epoch:279, train acc:1.0, test acc:0.7675 ===\n",
      "train loss:0.012308213320682255\n",
      "train loss:0.008820104191959272\n",
      "train loss:0.009541865691387146\n",
      "=== epoch:280, train acc:1.0, test acc:0.7675 ===\n",
      "train loss:0.008425221886983835\n",
      "train loss:0.010855675670685563\n",
      "train loss:0.012291348172066917\n",
      "=== epoch:281, train acc:1.0, test acc:0.768 ===\n",
      "train loss:0.009089598964127622\n",
      "train loss:0.01051433733790713\n",
      "train loss:0.008579984658704014\n",
      "=== epoch:282, train acc:1.0, test acc:0.7676 ===\n",
      "train loss:0.009170011064825767\n",
      "train loss:0.010284278608296142\n",
      "train loss:0.011332805337615834\n",
      "=== epoch:283, train acc:1.0, test acc:0.7675 ===\n",
      "train loss:0.009166232481166747\n",
      "train loss:0.010706719753487779\n",
      "train loss:0.009424286793546706\n",
      "=== epoch:284, train acc:1.0, test acc:0.7673 ===\n",
      "train loss:0.008257450289360858\n",
      "train loss:0.00824783252610438\n",
      "train loss:0.010531125973730739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:285, train acc:1.0, test acc:0.768 ===\n",
      "train loss:0.012317373726847873\n",
      "train loss:0.008702905998713165\n",
      "train loss:0.008915357174307062\n",
      "=== epoch:286, train acc:1.0, test acc:0.7687 ===\n",
      "train loss:0.009665773271616763\n",
      "train loss:0.010570849876114286\n",
      "train loss:0.008503962423280888\n",
      "=== epoch:287, train acc:1.0, test acc:0.768 ===\n",
      "train loss:0.007863948574105271\n",
      "train loss:0.009297619957353728\n",
      "train loss:0.009326681233302311\n",
      "=== epoch:288, train acc:1.0, test acc:0.7679 ===\n",
      "train loss:0.009009133589802593\n",
      "train loss:0.00872766540029152\n",
      "train loss:0.007654104498017298\n",
      "=== epoch:289, train acc:1.0, test acc:0.7674 ===\n",
      "train loss:0.011522949962167843\n",
      "train loss:0.0095284976696316\n",
      "train loss:0.011120777883280928\n",
      "=== epoch:290, train acc:1.0, test acc:0.7679 ===\n",
      "train loss:0.009586568315100652\n",
      "train loss:0.008835943954983352\n",
      "train loss:0.009902880871643156\n",
      "=== epoch:291, train acc:1.0, test acc:0.7681 ===\n",
      "train loss:0.010541730296408529\n",
      "train loss:0.009775003704458418\n",
      "train loss:0.012371751387496266\n",
      "=== epoch:292, train acc:1.0, test acc:0.768 ===\n",
      "train loss:0.008754219889963789\n",
      "train loss:0.010825696628303688\n",
      "train loss:0.007274221109811519\n",
      "=== epoch:293, train acc:1.0, test acc:0.7677 ===\n",
      "train loss:0.010605694230552883\n",
      "train loss:0.00873593056542271\n",
      "train loss:0.010763562323803504\n",
      "=== epoch:294, train acc:1.0, test acc:0.768 ===\n",
      "train loss:0.008262466309553535\n",
      "train loss:0.009561990881383986\n",
      "train loss:0.0070158625122833385\n",
      "=== epoch:295, train acc:1.0, test acc:0.7683 ===\n",
      "train loss:0.009581882277563236\n",
      "train loss:0.007508534466726422\n",
      "train loss:0.006882728554628816\n",
      "=== epoch:296, train acc:1.0, test acc:0.7676 ===\n",
      "train loss:0.0075113410560456164\n",
      "train loss:0.008112196187179637\n",
      "train loss:0.007254263297791437\n",
      "=== epoch:297, train acc:1.0, test acc:0.7671 ===\n",
      "train loss:0.010207263942196945\n",
      "train loss:0.0081699758282705\n",
      "train loss:0.010617326949166428\n",
      "=== epoch:298, train acc:1.0, test acc:0.7685 ===\n",
      "train loss:0.008771375360775375\n",
      "train loss:0.006942141943683853\n",
      "train loss:0.008027090743600328\n",
      "=== epoch:299, train acc:1.0, test acc:0.7679 ===\n",
      "train loss:0.00795000212959222\n",
      "train loss:0.009723909151665665\n",
      "train loss:0.008518454522488207\n",
      "=== epoch:300, train acc:1.0, test acc:0.7689 ===\n",
      "train loss:0.007800350890893073\n",
      "train loss:0.008826580752034033\n",
      "train loss:0.006374995158843448\n",
      "=== epoch:301, train acc:1.0, test acc:0.7691 ===\n",
      "train loss:0.007393477413378685\n",
      "train loss:0.009581643236812187\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.769\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trial(use_dropout=False, dropout_ratio=0.15) #dropoutを使わない場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果まとめ\n",
    "\n",
    "\n",
    "1. trial(use_dropout=True, dropout_ratio=0.2)の場合、訓練データとテストデータの差は小さくなっているものの、テストデータの場合の正解率が0.4程度(0.4682)と低めになっている。これは、dropout_ratioが高すぎなのだろう\n",
    "2. trial(use_dropout=True, dropout_ratio=0.15)の場合、訓練データとテストデータの差は開き始めたが、今度はテストデータの場合の正解率が上がり、0.7212となっている。\n",
    "3. trial(use_dropout=False, dropout_ratio=0.2)の場合、明らかに過学習が発生しているが、テストデータの正解率が0.769であり、これらの試行で一番高い値を示している。\n",
    "\n",
    "ここまでの結果を見ると、Dropoutを使わないほうが、テストデータに対する正解率が高いような気がする。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.303468036580703\n",
      "=== epoch:1, train acc:0.11, test acc:0.0947 ===\n",
      "train loss:2.3018915441842527\n",
      "train loss:2.3029688016001804\n",
      "train loss:2.301923138179282\n",
      "=== epoch:2, train acc:0.10333333333333333, test acc:0.0886 ===\n",
      "train loss:2.3004292505025505\n",
      "train loss:2.3003166265069694\n",
      "train loss:2.3018130936616474\n",
      "=== epoch:3, train acc:0.09666666666666666, test acc:0.0844 ===\n",
      "train loss:2.3019925188620443\n",
      "train loss:2.3020742163798924\n",
      "train loss:2.3023687813709937\n",
      "=== epoch:4, train acc:0.10666666666666667, test acc:0.0851 ===\n",
      "train loss:2.302572936494491\n",
      "train loss:2.3019458626906193\n",
      "train loss:2.3008692383281426\n",
      "=== epoch:5, train acc:0.11, test acc:0.0868 ===\n",
      "train loss:2.302416474010892\n",
      "train loss:2.302155005525942\n",
      "train loss:2.3006666002488707\n",
      "=== epoch:6, train acc:0.11, test acc:0.0887 ===\n",
      "train loss:2.3015194551864364\n",
      "train loss:2.300608274392026\n",
      "train loss:2.30137794766032\n",
      "=== epoch:7, train acc:0.13333333333333333, test acc:0.1206 ===\n",
      "train loss:2.3016346531728096\n",
      "train loss:2.302035918409296\n",
      "train loss:2.3008835813427297\n",
      "=== epoch:8, train acc:0.16333333333333333, test acc:0.1341 ===\n",
      "train loss:2.301859519980849\n",
      "train loss:2.301521778744985\n",
      "train loss:2.3007175165963702\n",
      "=== epoch:9, train acc:0.15, test acc:0.1351 ===\n",
      "train loss:2.3024979702741377\n",
      "train loss:2.3006782788689253\n",
      "train loss:2.3014282987785237\n",
      "=== epoch:10, train acc:0.16333333333333333, test acc:0.1376 ===\n",
      "train loss:2.300871701838345\n",
      "train loss:2.301150328722646\n",
      "train loss:2.301671244965208\n",
      "=== epoch:11, train acc:0.16333333333333333, test acc:0.1365 ===\n",
      "train loss:2.3008658708886895\n",
      "train loss:2.3000886334995623\n",
      "train loss:2.301234992585194\n",
      "=== epoch:12, train acc:0.16666666666666666, test acc:0.134 ===\n",
      "train loss:2.3025628232123605\n",
      "train loss:2.302395889327048\n",
      "train loss:2.299258211490326\n",
      "=== epoch:13, train acc:0.14666666666666667, test acc:0.1297 ===\n",
      "train loss:2.3013323483446144\n",
      "train loss:2.2998685297000225\n",
      "train loss:2.300181900827332\n",
      "=== epoch:14, train acc:0.14, test acc:0.1244 ===\n",
      "train loss:2.299895019695052\n",
      "train loss:2.2991945528439244\n",
      "train loss:2.2999398673877836\n",
      "=== epoch:15, train acc:0.13333333333333333, test acc:0.1239 ===\n",
      "train loss:2.299560828958015\n",
      "train loss:2.30144603328427\n",
      "train loss:2.3002205795370547\n",
      "=== epoch:16, train acc:0.13333333333333333, test acc:0.123 ===\n",
      "train loss:2.2991478576781144\n",
      "train loss:2.298943858513287\n",
      "train loss:2.300055822089667\n",
      "=== epoch:17, train acc:0.13, test acc:0.117 ===\n",
      "train loss:2.301232698308549\n",
      "train loss:2.301683891347734\n",
      "train loss:2.3015831566172418\n",
      "=== epoch:18, train acc:0.13, test acc:0.1178 ===\n",
      "train loss:2.30265773728116\n",
      "train loss:2.3007519063013304\n",
      "train loss:2.301590630433507\n",
      "=== epoch:19, train acc:0.13, test acc:0.1181 ===\n",
      "train loss:2.30078823564929\n",
      "train loss:2.3000646421165136\n",
      "train loss:2.3017343058221553\n",
      "=== epoch:20, train acc:0.13, test acc:0.1185 ===\n",
      "train loss:2.298981236137859\n",
      "train loss:2.3008500044334146\n",
      "train loss:2.300016340023797\n",
      "=== epoch:21, train acc:0.13333333333333333, test acc:0.1241 ===\n",
      "train loss:2.2984711250468375\n",
      "train loss:2.298478412259208\n",
      "train loss:2.2994062636804338\n",
      "=== epoch:22, train acc:0.13333333333333333, test acc:0.1226 ===\n",
      "train loss:2.303201672877585\n",
      "train loss:2.299577733135828\n",
      "train loss:2.299036824140719\n",
      "=== epoch:23, train acc:0.13333333333333333, test acc:0.1217 ===\n",
      "train loss:2.2994896547335664\n",
      "train loss:2.2988849540542673\n",
      "train loss:2.3004951360665262\n",
      "=== epoch:24, train acc:0.13333333333333333, test acc:0.1249 ===\n",
      "train loss:2.3026219331126683\n",
      "train loss:2.298442406089243\n",
      "train loss:2.299936840726701\n",
      "=== epoch:25, train acc:0.13, test acc:0.1194 ===\n",
      "train loss:2.299028297129336\n",
      "train loss:2.2974222892257297\n",
      "train loss:2.29831482084907\n",
      "=== epoch:26, train acc:0.13333333333333333, test acc:0.1197 ===\n",
      "train loss:2.2991398095375253\n",
      "train loss:2.299358115492217\n",
      "train loss:2.300272256816442\n",
      "=== epoch:27, train acc:0.13333333333333333, test acc:0.1216 ===\n",
      "train loss:2.3023146082935155\n",
      "train loss:2.2985111161559546\n",
      "train loss:2.298796746311834\n",
      "=== epoch:28, train acc:0.13333333333333333, test acc:0.1216 ===\n",
      "train loss:2.297481065811473\n",
      "train loss:2.299172448158802\n",
      "train loss:2.299029770922057\n",
      "=== epoch:29, train acc:0.13333333333333333, test acc:0.1206 ===\n",
      "train loss:2.2970922314673725\n",
      "train loss:2.2958656636814108\n",
      "train loss:2.3004226781700536\n",
      "=== epoch:30, train acc:0.13, test acc:0.117 ===\n",
      "train loss:2.2971731973735072\n",
      "train loss:2.30015273940227\n",
      "train loss:2.3015446935128674\n",
      "=== epoch:31, train acc:0.13, test acc:0.1148 ===\n",
      "train loss:2.30147149912829\n",
      "train loss:2.3003221581855358\n",
      "train loss:2.2990115911004043\n",
      "=== epoch:32, train acc:0.13, test acc:0.1141 ===\n",
      "train loss:2.3036405384568748\n",
      "train loss:2.2988993776907036\n",
      "train loss:2.2979688893920334\n",
      "=== epoch:33, train acc:0.13, test acc:0.114 ===\n",
      "train loss:2.2993515117352845\n",
      "train loss:2.301823807970605\n",
      "train loss:2.2991983701842718\n",
      "=== epoch:34, train acc:0.13, test acc:0.1137 ===\n",
      "train loss:2.29721613209554\n",
      "train loss:2.298290323155247\n",
      "train loss:2.2972014082732937\n",
      "=== epoch:35, train acc:0.13, test acc:0.1138 ===\n",
      "train loss:2.2994504248623873\n",
      "train loss:2.3001970467835986\n",
      "train loss:2.2996452213203797\n",
      "=== epoch:36, train acc:0.13, test acc:0.1137 ===\n",
      "train loss:2.2966322418758542\n",
      "train loss:2.2985282637980613\n",
      "train loss:2.301736363176664\n",
      "=== epoch:37, train acc:0.13, test acc:0.1139 ===\n",
      "train loss:2.2987627635975976\n",
      "train loss:2.3013925643426867\n",
      "train loss:2.2985746279884456\n",
      "=== epoch:38, train acc:0.13, test acc:0.1137 ===\n",
      "train loss:2.2982340465805153\n",
      "train loss:2.296386882417559\n",
      "train loss:2.300539362752489\n",
      "=== epoch:39, train acc:0.13, test acc:0.1136 ===\n",
      "train loss:2.298476687710963\n",
      "train loss:2.300038318351764\n",
      "train loss:2.2939899522393508\n",
      "=== epoch:40, train acc:0.13, test acc:0.1139 ===\n",
      "train loss:2.30025512136217\n",
      "train loss:2.3022537238382204\n",
      "train loss:2.2996165328967084\n",
      "=== epoch:41, train acc:0.13, test acc:0.114 ===\n",
      "train loss:2.29949046505576\n",
      "train loss:2.2987132419842102\n",
      "train loss:2.3020861995357587\n",
      "=== epoch:42, train acc:0.13, test acc:0.114 ===\n",
      "train loss:2.296193282819056\n",
      "train loss:2.2983464379606833\n",
      "train loss:2.2947826220006764\n",
      "=== epoch:43, train acc:0.13, test acc:0.1136 ===\n",
      "train loss:2.296069629143015\n",
      "train loss:2.2982247741057673\n",
      "train loss:2.2982365467075985\n",
      "=== epoch:44, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2983556981945865\n",
      "train loss:2.2971664377902368\n",
      "train loss:2.3004367995190464\n",
      "=== epoch:45, train acc:0.13, test acc:0.1136 ===\n",
      "train loss:2.299545814778403\n",
      "train loss:2.2995037301702785\n",
      "train loss:2.2990335190379\n",
      "=== epoch:46, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2960320363180435\n",
      "train loss:2.298187573778117\n",
      "train loss:2.293542154989492\n",
      "=== epoch:47, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2965848106011304\n",
      "train loss:2.3002940005562\n",
      "train loss:2.3006384327559446\n",
      "=== epoch:48, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3003913647463525\n",
      "train loss:2.2956897409844297\n",
      "train loss:2.2948379697893553\n",
      "=== epoch:49, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2975214984223573\n",
      "train loss:2.3002326319747737\n",
      "train loss:2.298045849492999\n",
      "=== epoch:50, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299715044798216\n",
      "train loss:2.297267329934778\n",
      "train loss:2.296178089763776\n",
      "=== epoch:51, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291885106571475\n",
      "train loss:2.30086598173979\n",
      "train loss:2.292757275087525\n",
      "=== epoch:52, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3019130612068652\n",
      "train loss:2.3029933469652515\n",
      "train loss:2.2978233284377794\n",
      "=== epoch:53, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299144647635315\n",
      "train loss:2.3008774228028646\n",
      "train loss:2.3002076437885446\n",
      "=== epoch:54, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2978492312993146\n",
      "train loss:2.300650464293648\n",
      "train loss:2.299639552399907\n",
      "=== epoch:55, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300353299814782\n",
      "train loss:2.2979001590470474\n",
      "train loss:2.3011429486903183\n",
      "=== epoch:56, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298834330777435\n",
      "train loss:2.29884957566724\n",
      "train loss:2.2996720988982298\n",
      "=== epoch:57, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2963525119769725\n",
      "train loss:2.298367620563215\n",
      "train loss:2.2951825063008684\n",
      "=== epoch:58, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3019276514474654\n",
      "train loss:2.2975224134028362\n",
      "train loss:2.296953443558635\n",
      "=== epoch:59, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2957449948009074\n",
      "train loss:2.2990748385680115\n",
      "train loss:2.296372478847492\n",
      "=== epoch:60, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3011137434121323\n",
      "train loss:2.298071602156703\n",
      "train loss:2.296732423225338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:61, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293190795924474\n",
      "train loss:2.294773784975934\n",
      "train loss:2.2933650697327725\n",
      "=== epoch:62, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951304632350182\n",
      "train loss:2.299967938639077\n",
      "train loss:2.2975595140080505\n",
      "=== epoch:63, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.29814740770441\n",
      "train loss:2.300554199581444\n",
      "train loss:2.2986016436393157\n",
      "=== epoch:64, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2993830679994023\n",
      "train loss:2.29406804211787\n",
      "train loss:2.299546635203419\n",
      "=== epoch:65, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2999702107698665\n",
      "train loss:2.293755313584101\n",
      "train loss:2.299940433057974\n",
      "=== epoch:66, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292636173360142\n",
      "train loss:2.2969441215094606\n",
      "train loss:2.294522297945267\n",
      "=== epoch:67, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292187751158157\n",
      "train loss:2.3037423770550696\n",
      "train loss:2.29495547050982\n",
      "=== epoch:68, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2940791265322167\n",
      "train loss:2.2971515949849373\n",
      "train loss:2.2971983874320223\n",
      "=== epoch:69, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2962784920448054\n",
      "train loss:2.301244645402869\n",
      "train loss:2.3009582847392527\n",
      "=== epoch:70, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2935450262567114\n",
      "train loss:2.295930858381821\n",
      "train loss:2.3021510372645095\n",
      "=== epoch:71, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2962627110777594\n",
      "train loss:2.2955645663294066\n",
      "train loss:2.2959476885750836\n",
      "=== epoch:72, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2999550704203493\n",
      "train loss:2.2959882717402804\n",
      "train loss:2.2960673166161074\n",
      "=== epoch:73, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2972035590839837\n",
      "train loss:2.2935829126721448\n",
      "train loss:2.3032401654461836\n",
      "=== epoch:74, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2919537298817185\n",
      "train loss:2.295623217017476\n",
      "train loss:2.2914542632772923\n",
      "=== epoch:75, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2977777882852464\n",
      "train loss:2.2962241368927825\n",
      "train loss:2.297071489549299\n",
      "=== epoch:76, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295982687875154\n",
      "train loss:2.293836573978365\n",
      "train loss:2.2958090501518216\n",
      "=== epoch:77, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.29951813184842\n",
      "train loss:2.299297242212586\n",
      "train loss:2.2947079196339253\n",
      "=== epoch:78, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2927189066717535\n",
      "train loss:2.302092836644519\n",
      "train loss:2.2932285162338197\n",
      "=== epoch:79, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2944665733237453\n",
      "train loss:2.2945214085140333\n",
      "train loss:2.2885018645938855\n",
      "=== epoch:80, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.29261414209511\n",
      "train loss:2.2999388681304094\n",
      "train loss:2.295990346854317\n",
      "=== epoch:81, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2939368166396807\n",
      "train loss:2.295103891724564\n",
      "train loss:2.299068239017407\n",
      "=== epoch:82, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2976261328040564\n",
      "train loss:2.293674261387541\n",
      "train loss:2.301335677575212\n",
      "=== epoch:83, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2974591702584553\n",
      "train loss:2.2978114341182443\n",
      "train loss:2.297762684693572\n",
      "=== epoch:84, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2962002671353696\n",
      "train loss:2.297634726287997\n",
      "train loss:2.2919706273727263\n",
      "=== epoch:85, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291871383432417\n",
      "train loss:2.2892683434696126\n",
      "train loss:2.292009482769542\n",
      "=== epoch:86, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2922252037277318\n",
      "train loss:2.299347186194569\n",
      "train loss:2.295891975862519\n",
      "=== epoch:87, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297026310486605\n",
      "train loss:2.293022039952198\n",
      "train loss:2.293912972101557\n",
      "=== epoch:88, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2950656235752054\n",
      "train loss:2.291351090677706\n",
      "train loss:2.3032113548345383\n",
      "=== epoch:89, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2961181864438767\n",
      "train loss:2.292990959653523\n",
      "train loss:2.2898900987330157\n",
      "=== epoch:90, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.30206205726535\n",
      "train loss:2.294669827436083\n",
      "train loss:2.2972816599378545\n",
      "=== epoch:91, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298527314597271\n",
      "train loss:2.2907430055395626\n",
      "train loss:2.297922479130388\n",
      "=== epoch:92, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2948815691044557\n",
      "train loss:2.2931115783095053\n",
      "train loss:2.3040329959119408\n",
      "=== epoch:93, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2848432662766287\n",
      "train loss:2.296464968568467\n",
      "train loss:2.296431693502516\n",
      "=== epoch:94, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297651404665447\n",
      "train loss:2.295754810796703\n",
      "train loss:2.288978960457561\n",
      "=== epoch:95, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300364092213951\n",
      "train loss:2.2971967626174385\n",
      "train loss:2.293323269047437\n",
      "=== epoch:96, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297805997271662\n",
      "train loss:2.296568441071995\n",
      "train loss:2.296754212716763\n",
      "=== epoch:97, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.29243172907565\n",
      "train loss:2.2976872782070785\n",
      "train loss:2.307502375323978\n",
      "=== epoch:98, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.29211856115092\n",
      "train loss:2.2976210032926816\n",
      "train loss:2.2964212199464766\n",
      "=== epoch:99, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2986359270406904\n",
      "train loss:2.2921504097098753\n",
      "train loss:2.2900621497791565\n",
      "=== epoch:100, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300453100543021\n",
      "train loss:2.286128403440994\n",
      "train loss:2.2978800886029975\n",
      "=== epoch:101, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2914282339024563\n",
      "train loss:2.291669811334896\n",
      "train loss:2.291833936417067\n",
      "=== epoch:102, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297816462897407\n",
      "train loss:2.2916699865588055\n",
      "train loss:2.2972205161872803\n",
      "=== epoch:103, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3012818180878054\n",
      "train loss:2.2938848267006446\n",
      "train loss:2.2937108242990742\n",
      "=== epoch:104, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291185425854585\n",
      "train loss:2.3010636474696637\n",
      "train loss:2.30147006942886\n",
      "=== epoch:105, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2959841196079736\n",
      "train loss:2.2894526958292705\n",
      "train loss:2.2984365775239537\n",
      "=== epoch:106, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295374493822378\n",
      "train loss:2.2899532445147956\n",
      "train loss:2.2979329601921\n",
      "=== epoch:107, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2955980614851987\n",
      "train loss:2.3026298003186714\n",
      "train loss:2.2914969818258135\n",
      "=== epoch:108, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.289993684296108\n",
      "train loss:2.296627415984517\n",
      "train loss:2.294588473064854\n",
      "=== epoch:109, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298966303488744\n",
      "train loss:2.2978002535043034\n",
      "train loss:2.293935077229525\n",
      "=== epoch:110, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.303019691992236\n",
      "train loss:2.290769297749344\n",
      "train loss:2.2893690438090415\n",
      "=== epoch:111, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294750542966184\n",
      "train loss:2.2903952635575977\n",
      "train loss:2.2920199580681917\n",
      "=== epoch:112, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3028149514661553\n",
      "train loss:2.2920279937411845\n",
      "train loss:2.291730622231964\n",
      "=== epoch:113, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2953051575531758\n",
      "train loss:2.2997611020686124\n",
      "train loss:2.3034387517213957\n",
      "=== epoch:114, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2957594287133922\n",
      "train loss:2.294518573884977\n",
      "train loss:2.294291622750633\n",
      "=== epoch:115, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3043875523470443\n",
      "train loss:2.2896634820386925\n",
      "train loss:2.287690157333843\n",
      "=== epoch:116, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951682086545175\n",
      "train loss:2.2970809908465313\n",
      "train loss:2.3011314246273944\n",
      "=== epoch:117, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2929058069958694\n",
      "train loss:2.2979942838883547\n",
      "train loss:2.291275900617849\n",
      "=== epoch:118, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.304898829534184\n",
      "train loss:2.2935885507599845\n",
      "train loss:2.2913200907973668\n",
      "=== epoch:119, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2927264814494563\n",
      "train loss:2.2938188971132325\n",
      "train loss:2.298170868917134\n",
      "=== epoch:120, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2943821119072023\n",
      "train loss:2.2988305027784097\n",
      "train loss:2.293337673901227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:121, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294370299757226\n",
      "train loss:2.2923795761896866\n",
      "train loss:2.29663631556619\n",
      "=== epoch:122, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2936339232569414\n",
      "train loss:2.2990791514570734\n",
      "train loss:2.297511554409169\n",
      "=== epoch:123, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297824911067822\n",
      "train loss:2.2919447292127284\n",
      "train loss:2.299130314040899\n",
      "=== epoch:124, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2932905638326484\n",
      "train loss:2.289264549666586\n",
      "train loss:2.288975708654996\n",
      "=== epoch:125, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3039608774610123\n",
      "train loss:2.2913270773377126\n",
      "train loss:2.2858073224614435\n",
      "=== epoch:126, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296915195635377\n",
      "train loss:2.2907681295426556\n",
      "train loss:2.2892798323434405\n",
      "=== epoch:127, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297489388847126\n",
      "train loss:2.2869676813157933\n",
      "train loss:2.292348771802639\n",
      "=== epoch:128, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2953043099485577\n",
      "train loss:2.281730242450184\n",
      "train loss:2.2995961210922014\n",
      "=== epoch:129, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291629210020159\n",
      "train loss:2.2857659918532973\n",
      "train loss:2.2917321294958692\n",
      "=== epoch:130, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293886540828905\n",
      "train loss:2.289373762190106\n",
      "train loss:2.287087888259449\n",
      "=== epoch:131, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290963665593561\n",
      "train loss:2.285221470325809\n",
      "train loss:2.2968695206923946\n",
      "=== epoch:132, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2872941411510808\n",
      "train loss:2.2921605199970303\n",
      "train loss:2.2940699913788407\n",
      "=== epoch:133, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3015390904756696\n",
      "train loss:2.2924429212604407\n",
      "train loss:2.3087653117374365\n",
      "=== epoch:134, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2877194795821616\n",
      "train loss:2.2917976651844554\n",
      "train loss:2.294700940897092\n",
      "=== epoch:135, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288991230771151\n",
      "train loss:2.3059837888846286\n",
      "train loss:2.29060300547869\n",
      "=== epoch:136, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2987091940933215\n",
      "train loss:2.291723595259884\n",
      "train loss:2.2932898967348168\n",
      "=== epoch:137, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2918721439850995\n",
      "train loss:2.296822917762396\n",
      "train loss:2.2985208818463034\n",
      "=== epoch:138, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291584249496065\n",
      "train loss:2.289970336525932\n",
      "train loss:2.289395523975919\n",
      "=== epoch:139, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293605827614513\n",
      "train loss:2.281811848654873\n",
      "train loss:2.2983040900870497\n",
      "=== epoch:140, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288037188000983\n",
      "train loss:2.2964372826963126\n",
      "train loss:2.279798278328438\n",
      "=== epoch:141, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298136788626324\n",
      "train loss:2.292716517397622\n",
      "train loss:2.283820962300125\n",
      "=== epoch:142, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3025200328747384\n",
      "train loss:2.2983882635232473\n",
      "train loss:2.2778941283967513\n",
      "=== epoch:143, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.304418573406419\n",
      "train loss:2.288461920608563\n",
      "train loss:2.2960172145513322\n",
      "=== epoch:144, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2965888252991618\n",
      "train loss:2.2908518218994884\n",
      "train loss:2.300055596229432\n",
      "=== epoch:145, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.281370791370394\n",
      "train loss:2.297686413722129\n",
      "train loss:2.2906443202184907\n",
      "=== epoch:146, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.27988774551265\n",
      "train loss:2.2907545640950033\n",
      "train loss:2.3013137163886097\n",
      "=== epoch:147, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2959403665159774\n",
      "train loss:2.300913112331133\n",
      "train loss:2.3019205775797604\n",
      "=== epoch:148, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2990537780303346\n",
      "train loss:2.294466352353606\n",
      "train loss:2.299133751356389\n",
      "=== epoch:149, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2909811319260682\n",
      "train loss:2.293128395296587\n",
      "train loss:2.281943329442542\n",
      "=== epoch:150, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292760975876123\n",
      "train loss:2.2914498471784457\n",
      "train loss:2.2918129098623994\n",
      "=== epoch:151, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291503730039237\n",
      "train loss:2.304217828827966\n",
      "train loss:2.295181216852582\n",
      "=== epoch:152, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294768950248485\n",
      "train loss:2.2928833961570585\n",
      "train loss:2.2949828707644793\n",
      "=== epoch:153, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300656079220907\n",
      "train loss:2.283988241051999\n",
      "train loss:2.292122199669959\n",
      "=== epoch:154, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3016666642923522\n",
      "train loss:2.2936491276620243\n",
      "train loss:2.2919096571557604\n",
      "=== epoch:155, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2912322793139346\n",
      "train loss:2.279558683258378\n",
      "train loss:2.2769358271594053\n",
      "=== epoch:156, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2911989308743497\n",
      "train loss:2.2930924036837363\n",
      "train loss:2.2946834017177484\n",
      "=== epoch:157, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293723443446215\n",
      "train loss:2.2998969833191416\n",
      "train loss:2.2993881640295384\n",
      "=== epoch:158, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.283256102004101\n",
      "train loss:2.2869984580618987\n",
      "train loss:2.2886717984608294\n",
      "=== epoch:159, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2912631156918364\n",
      "train loss:2.3020116801904806\n",
      "train loss:2.291842713287877\n",
      "=== epoch:160, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2834194819999505\n",
      "train loss:2.298260377733482\n",
      "train loss:2.288803778675407\n",
      "=== epoch:161, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2913424738624295\n",
      "train loss:2.2832924907114953\n",
      "train loss:2.3144702981863032\n",
      "=== epoch:162, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2941801913615536\n",
      "train loss:2.297815458251967\n",
      "train loss:2.290847615433799\n",
      "=== epoch:163, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2952879055539377\n",
      "train loss:2.2924958186623736\n",
      "train loss:2.284315958837232\n",
      "=== epoch:164, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3012243591960866\n",
      "train loss:2.2985613319423437\n",
      "train loss:2.281747948403328\n",
      "=== epoch:165, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297589696437108\n",
      "train loss:2.2906782920673043\n",
      "train loss:2.290009980033983\n",
      "=== epoch:166, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2920008847265576\n",
      "train loss:2.299758194412727\n",
      "train loss:2.2819352357578926\n",
      "=== epoch:167, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300522949267012\n",
      "train loss:2.30269124354534\n",
      "train loss:2.311357015343357\n",
      "=== epoch:168, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3028727646715748\n",
      "train loss:2.296206269583378\n",
      "train loss:2.3015637467250807\n",
      "=== epoch:169, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296822096800765\n",
      "train loss:2.2804458496103357\n",
      "train loss:2.2838262871505814\n",
      "=== epoch:170, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290889723306854\n",
      "train loss:2.3035539408497607\n",
      "train loss:2.29233713730529\n",
      "=== epoch:171, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2901301671791727\n",
      "train loss:2.2921692755169443\n",
      "train loss:2.307262224498251\n",
      "=== epoch:172, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294788233842417\n",
      "train loss:2.2923815097239237\n",
      "train loss:2.289661906394364\n",
      "=== epoch:173, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.305147969863863\n",
      "train loss:2.2910116542034884\n",
      "train loss:2.29927324394707\n",
      "=== epoch:174, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.287925709799394\n",
      "train loss:2.2956471322239738\n",
      "train loss:2.289937745800955\n",
      "=== epoch:175, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2971902125047334\n",
      "train loss:2.2867434638380053\n",
      "train loss:2.292996343967151\n",
      "=== epoch:176, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2785253267733063\n",
      "train loss:2.2928606668084712\n",
      "train loss:2.2950036732657604\n",
      "=== epoch:177, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951458596606864\n",
      "train loss:2.289891703237513\n",
      "train loss:2.2782660453995214\n",
      "=== epoch:178, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2901148008161583\n",
      "train loss:2.2915269969321552\n",
      "train loss:2.2938719871598363\n",
      "=== epoch:179, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2982442558043736\n",
      "train loss:2.2903781549072866\n",
      "train loss:2.2865727143142287\n",
      "=== epoch:180, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.306476501567838\n",
      "train loss:2.2866225195461176\n",
      "train loss:2.2943013354695676\n",
      "=== epoch:181, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2986707899035874\n",
      "train loss:2.2957690077018613\n",
      "train loss:2.2968465384273595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:182, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.273483959254552\n",
      "train loss:2.296221768234012\n",
      "train loss:2.2795121601989052\n",
      "=== epoch:183, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3056724921507232\n",
      "train loss:2.290625996341475\n",
      "train loss:2.305395759780495\n",
      "=== epoch:184, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2960788771822553\n",
      "train loss:2.2906929094747586\n",
      "train loss:2.296116016542239\n",
      "=== epoch:185, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3028007847443543\n",
      "train loss:2.284896596899362\n",
      "train loss:2.280139148052283\n",
      "=== epoch:186, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.282982251333235\n",
      "train loss:2.2947083092327105\n",
      "train loss:2.295833104856554\n",
      "=== epoch:187, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.280501750602932\n",
      "train loss:2.2857659963995154\n",
      "train loss:2.307224039204905\n",
      "=== epoch:188, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2954915197952612\n",
      "train loss:2.2925940933011235\n",
      "train loss:2.296147343772317\n",
      "=== epoch:189, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2907949660810916\n",
      "train loss:2.29027463837931\n",
      "train loss:2.2897452643098273\n",
      "=== epoch:190, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2879403599343204\n",
      "train loss:2.287464239325548\n",
      "train loss:2.296271665070335\n",
      "=== epoch:191, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2944962309487225\n",
      "train loss:2.291505641874649\n",
      "train loss:2.290180691377847\n",
      "=== epoch:192, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2838455738357335\n",
      "train loss:2.2873880147884162\n",
      "train loss:2.282879313095244\n",
      "=== epoch:193, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291028752327616\n",
      "train loss:2.2987388240373825\n",
      "train loss:2.282362177748287\n",
      "=== epoch:194, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2860933370356293\n",
      "train loss:2.285879647938108\n",
      "train loss:2.305165287780344\n",
      "=== epoch:195, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2970585706442783\n",
      "train loss:2.2803604079245723\n",
      "train loss:2.2907580627165403\n",
      "=== epoch:196, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2797742694156655\n",
      "train loss:2.293871937141152\n",
      "train loss:2.3068562291065446\n",
      "=== epoch:197, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2911356088792756\n",
      "train loss:2.294282253078653\n",
      "train loss:2.2911514516989313\n",
      "=== epoch:198, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2979797134019533\n",
      "train loss:2.304647794554528\n",
      "train loss:2.2910583884168747\n",
      "=== epoch:199, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292551408937012\n",
      "train loss:2.2888197512915727\n",
      "train loss:2.284918854647879\n",
      "=== epoch:200, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2925038268940914\n",
      "train loss:2.275225499796223\n",
      "train loss:2.2859623684136428\n",
      "=== epoch:201, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2908756427144077\n",
      "train loss:2.288107320940469\n",
      "train loss:2.296900864780616\n",
      "=== epoch:202, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2880227696272426\n",
      "train loss:2.2913771785983124\n",
      "train loss:2.273581886612426\n",
      "=== epoch:203, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.28412738886223\n",
      "train loss:2.2836658103526037\n",
      "train loss:2.2958709561744204\n",
      "=== epoch:204, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2826963714729582\n",
      "train loss:2.283173198893111\n",
      "train loss:2.2958949483747535\n",
      "=== epoch:205, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288750070099716\n",
      "train loss:2.2983216543366045\n",
      "train loss:2.2925982733872186\n",
      "=== epoch:206, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301899501139197\n",
      "train loss:2.2896227219930974\n",
      "train loss:2.297134738448548\n",
      "=== epoch:207, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292004208030729\n",
      "train loss:2.2705525980913497\n",
      "train loss:2.290961140919581\n",
      "=== epoch:208, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2972724759226777\n",
      "train loss:2.2958184899934007\n",
      "train loss:2.3011579312789903\n",
      "=== epoch:209, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2835131621341924\n",
      "train loss:2.3040029948190983\n",
      "train loss:2.302643011193909\n",
      "=== epoch:210, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2720550449924546\n",
      "train loss:2.2804724973497184\n",
      "train loss:2.2981854761161573\n",
      "=== epoch:211, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299175648983899\n",
      "train loss:2.2933262292739736\n",
      "train loss:2.2883598867355284\n",
      "=== epoch:212, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2814547046959275\n",
      "train loss:2.296051423540989\n",
      "train loss:2.295649174890629\n",
      "=== epoch:213, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2978309487751813\n",
      "train loss:2.2976342908834617\n",
      "train loss:2.2710792129944055\n",
      "=== epoch:214, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298006323026181\n",
      "train loss:2.2834280979343786\n",
      "train loss:2.2987674321675478\n",
      "=== epoch:215, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.283971936427408\n",
      "train loss:2.29540684523949\n",
      "train loss:2.291547391158926\n",
      "=== epoch:216, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302779678621638\n",
      "train loss:2.296021113038199\n",
      "train loss:2.292998199264497\n",
      "=== epoch:217, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2841177320052313\n",
      "train loss:2.2923941666899577\n",
      "train loss:2.31060846523733\n",
      "=== epoch:218, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291642524646635\n",
      "train loss:2.287537221827145\n",
      "train loss:2.299778240541222\n",
      "=== epoch:219, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.289621216384121\n",
      "train loss:2.28130494283848\n",
      "train loss:2.2917524733929504\n",
      "=== epoch:220, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2791321304630343\n",
      "train loss:2.2938314986852\n",
      "train loss:2.2999634304101786\n",
      "=== epoch:221, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2830846682623256\n",
      "train loss:2.295650907547866\n",
      "train loss:2.2866466127747462\n",
      "=== epoch:222, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292317726693591\n",
      "train loss:2.3004331566021032\n",
      "train loss:2.2892021539869414\n",
      "=== epoch:223, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2857890017724434\n",
      "train loss:2.2830522995391807\n",
      "train loss:2.2910156696512893\n",
      "=== epoch:224, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.28997347035276\n",
      "train loss:2.308534859258138\n",
      "train loss:2.2890295374018605\n",
      "=== epoch:225, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.270287380969858\n",
      "train loss:2.2910780092481122\n",
      "train loss:2.2985588047140877\n",
      "=== epoch:226, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2950960719379316\n",
      "train loss:2.283293379916115\n",
      "train loss:2.2759846743823244\n",
      "=== epoch:227, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294736657958314\n",
      "train loss:2.289101998945978\n",
      "train loss:2.2845483345709723\n",
      "=== epoch:228, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292548754746723\n",
      "train loss:2.2862408010775805\n",
      "train loss:2.2785466269225765\n",
      "=== epoch:229, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299127803890615\n",
      "train loss:2.2993975827966318\n",
      "train loss:2.292470836098338\n",
      "=== epoch:230, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300346667601829\n",
      "train loss:2.2867023654850804\n",
      "train loss:2.2863569403652217\n",
      "=== epoch:231, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2776721323666713\n",
      "train loss:2.2990763070877516\n",
      "train loss:2.2938026120088084\n",
      "=== epoch:232, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2952235346133767\n",
      "train loss:2.2915484595086504\n",
      "train loss:2.291118031729311\n",
      "=== epoch:233, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3017713822041044\n",
      "train loss:2.2974030495708657\n",
      "train loss:2.2912363917751417\n",
      "=== epoch:234, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2950138457602374\n",
      "train loss:2.28742455249784\n",
      "train loss:2.2814430636450704\n",
      "=== epoch:235, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294431842043822\n",
      "train loss:2.2921042763423873\n",
      "train loss:2.291699392016395\n",
      "=== epoch:236, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2732693754515427\n",
      "train loss:2.293492397405872\n",
      "train loss:2.2949470709383197\n",
      "=== epoch:237, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2899621853423224\n",
      "train loss:2.2870215467496515\n",
      "train loss:2.2970637686164626\n",
      "=== epoch:238, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2966652383091906\n",
      "train loss:2.2961367190304482\n",
      "train loss:2.3029459248387583\n",
      "=== epoch:239, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2940947670352454\n",
      "train loss:2.2742758417732363\n",
      "train loss:2.298011847445908\n",
      "=== epoch:240, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293619939622051\n",
      "train loss:2.317575633719192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.313497592368273\n",
      "=== epoch:241, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3063057695555136\n",
      "train loss:2.294993845700592\n",
      "train loss:2.2878614813301428\n",
      "=== epoch:242, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2751770187520055\n",
      "train loss:2.292799628808438\n",
      "train loss:2.2894396836775446\n",
      "=== epoch:243, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2950153901692016\n",
      "train loss:2.297286478431977\n",
      "train loss:2.2827522836167953\n",
      "=== epoch:244, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.305415690148585\n",
      "train loss:2.288070740433034\n",
      "train loss:2.310676257879845\n",
      "=== epoch:245, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3007620260163497\n",
      "train loss:2.2942571998439067\n",
      "train loss:2.3067655160978457\n",
      "=== epoch:246, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.271697156526628\n",
      "train loss:2.2830754054067457\n",
      "train loss:2.288659592467233\n",
      "=== epoch:247, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2849704524788255\n",
      "train loss:2.2937643653916777\n",
      "train loss:2.2955380531908953\n",
      "=== epoch:248, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.284977637564857\n",
      "train loss:2.2902940665735887\n",
      "train loss:2.297214816094776\n",
      "=== epoch:249, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2883175212938074\n",
      "train loss:2.2822184534612844\n",
      "train loss:2.2964158256988245\n",
      "=== epoch:250, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.280237027019181\n",
      "train loss:2.2847040310963784\n",
      "train loss:2.292526901007081\n",
      "=== epoch:251, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292727260807917\n",
      "train loss:2.284363326914608\n",
      "train loss:2.2845123101614786\n",
      "=== epoch:252, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2807268004848096\n",
      "train loss:2.2981862937797004\n",
      "train loss:2.295461927795102\n",
      "=== epoch:253, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.281855287628694\n",
      "train loss:2.2813547399736076\n",
      "train loss:2.3082560088781436\n",
      "=== epoch:254, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.30805067941815\n",
      "train loss:2.294698875264382\n",
      "train loss:2.2946173175254203\n",
      "=== epoch:255, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2875810015614353\n",
      "train loss:2.2986601505422932\n",
      "train loss:2.2735252313406336\n",
      "=== epoch:256, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2702046520913823\n",
      "train loss:2.2899284163910774\n",
      "train loss:2.275716108453332\n",
      "=== epoch:257, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2865672826269803\n",
      "train loss:2.302253678758427\n",
      "train loss:2.2900014659776717\n",
      "=== epoch:258, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2817736741574812\n",
      "train loss:2.2715276188547486\n",
      "train loss:2.270735754123986\n",
      "=== epoch:259, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294027876901345\n",
      "train loss:2.284790284259081\n",
      "train loss:2.310139677267683\n",
      "=== epoch:260, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2834638752910963\n",
      "train loss:2.2835155081160923\n",
      "train loss:2.295381099042444\n",
      "=== epoch:261, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2804715175830808\n",
      "train loss:2.2964820607205594\n",
      "train loss:2.2963014121490954\n",
      "=== epoch:262, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2968873088165456\n",
      "train loss:2.2960783223716215\n",
      "train loss:2.2853623872159896\n",
      "=== epoch:263, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.285599999291678\n",
      "train loss:2.29423990621343\n",
      "train loss:2.299072064765891\n",
      "=== epoch:264, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2864492701017887\n",
      "train loss:2.28379332041094\n",
      "train loss:2.2781249707406257\n",
      "=== epoch:265, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.27953649459039\n",
      "train loss:2.305453805876725\n",
      "train loss:2.2902829288969313\n",
      "=== epoch:266, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295283912558869\n",
      "train loss:2.2943488188134453\n",
      "train loss:2.293361629406155\n",
      "=== epoch:267, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2837970726080656\n",
      "train loss:2.284546766723311\n",
      "train loss:2.290322930383375\n",
      "=== epoch:268, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3052358839777147\n",
      "train loss:2.27348300715945\n",
      "train loss:2.29050360173085\n",
      "=== epoch:269, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2833571865072324\n",
      "train loss:2.301318744221307\n",
      "train loss:2.2879336177182754\n",
      "=== epoch:270, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2807139688533047\n",
      "train loss:2.284613358988232\n",
      "train loss:2.30696449644247\n",
      "=== epoch:271, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297163463718885\n",
      "train loss:2.2879409007301383\n",
      "train loss:2.2949001891535854\n",
      "=== epoch:272, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2945270935164275\n",
      "train loss:2.306067152118306\n",
      "train loss:2.2930079932562264\n",
      "=== epoch:273, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.283624035875156\n",
      "train loss:2.284759335319702\n",
      "train loss:2.2797771528780566\n",
      "=== epoch:274, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2858341843203065\n",
      "train loss:2.2722345830299973\n",
      "train loss:2.2865445538335396\n",
      "=== epoch:275, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2926794131451733\n",
      "train loss:2.288230063288377\n",
      "train loss:2.3028901695049373\n",
      "=== epoch:276, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2942147293450197\n",
      "train loss:2.2897350766434568\n",
      "train loss:2.296976963357759\n",
      "=== epoch:277, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.286234369004635\n",
      "train loss:2.28460171189385\n",
      "train loss:2.2811221315895445\n",
      "=== epoch:278, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300304371003138\n",
      "train loss:2.300235958296559\n",
      "train loss:2.2700357450982547\n",
      "=== epoch:279, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293175063848504\n",
      "train loss:2.3020565873576087\n",
      "train loss:2.3054237050053907\n",
      "=== epoch:280, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2976837649168504\n",
      "train loss:2.2845177391039213\n",
      "train loss:2.2968247593348963\n",
      "=== epoch:281, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.303611265038901\n",
      "train loss:2.2743980575966574\n",
      "train loss:2.28515731884358\n",
      "=== epoch:282, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294054409420169\n",
      "train loss:2.266252552869226\n",
      "train loss:2.2646170485401136\n",
      "=== epoch:283, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2826744468711833\n",
      "train loss:2.285467295159994\n",
      "train loss:2.290184482046203\n",
      "=== epoch:284, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2751459249111083\n",
      "train loss:2.2734973135507617\n",
      "train loss:2.306164123064262\n",
      "=== epoch:285, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.285401071557678\n",
      "train loss:2.2802334315155566\n",
      "train loss:2.2920125847756183\n",
      "=== epoch:286, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2918069070913645\n",
      "train loss:2.3063023558287483\n",
      "train loss:2.3100953243990436\n",
      "=== epoch:287, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2929107620412497\n",
      "train loss:2.3127171744385033\n",
      "train loss:2.289549075866561\n",
      "=== epoch:288, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2952014031284844\n",
      "train loss:2.2856672474304975\n",
      "train loss:2.2900744941761837\n",
      "=== epoch:289, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2878503907178525\n",
      "train loss:2.299047190192156\n",
      "train loss:2.289234432191819\n",
      "=== epoch:290, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2931226295820846\n",
      "train loss:2.2816928546042257\n",
      "train loss:2.2922384206340247\n",
      "=== epoch:291, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2758855150550987\n",
      "train loss:2.295230677621378\n",
      "train loss:2.306706589101402\n",
      "=== epoch:292, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2888464967596627\n",
      "train loss:2.2902242534345714\n",
      "train loss:2.3030077300876233\n",
      "=== epoch:293, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301288001748805\n",
      "train loss:2.2845594641233973\n",
      "train loss:2.27716998731094\n",
      "=== epoch:294, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301077577147817\n",
      "train loss:2.2826808593500165\n",
      "train loss:2.2858581875896427\n",
      "=== epoch:295, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2980582628689046\n",
      "train loss:2.304818092779817\n",
      "train loss:2.275046574424264\n",
      "=== epoch:296, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2918481700323046\n",
      "train loss:2.296571645991627\n",
      "train loss:2.279605886588952\n",
      "=== epoch:297, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.28438429043951\n",
      "train loss:2.2932844948519278\n",
      "train loss:2.2922112909733303\n",
      "=== epoch:298, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2819558788051126\n",
      "train loss:2.2825770154807916\n",
      "train loss:2.300055933806953\n",
      "=== epoch:299, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2773204872343276\n",
      "train loss:2.2832710117144797\n",
      "train loss:2.2857616568074897\n",
      "=== epoch:300, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2849134436822243\n",
      "train loss:2.2991687277622064\n",
      "train loss:2.289931148674396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:301, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2717890204327036\n",
      "train loss:2.284358831727542\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.1135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbdklEQVR4nO3de3RV9Z338fcnFwhIyiUEK6IFLUXRUtTUaqmOtlMVe1FmHEcttnXaUqe1tc88usSlbfWZWWvo9Knj01WFoZbWVq11FG+VCl5Qp6sXCYoiUiraqiEqEQUJcg3f54+zgZicE04wO+ck+/NaKytn7/07e39/2ZBP9u13FBGYmVl2VZS6ADMzKy0HgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZVxqQSBpnqS1kp4psFySfihptaSnJR2dVi1mZlZYmkcEPwNO62L5VGB88jUDmJ1iLWZmVkBqQRARjwFvdNHkDODnkfMHYJikA9Kqx8zM8qsq4bYPBF5uN92UzHulY0NJM8gdNbDffvsdc9hhh/VKgWZm/cXSpUtfj4j6fMtKGQTKMy/veBcRMReYC9DQ0BCNjY1p1mVm1u9IerHQslLeNdQEHNRuegzQXKJazMwyq5RBcA/w+eTuoeOADRHR6bSQmZmlK7VTQ5J+CZwEjJTUBHwXqAaIiDnAAuB0YDXwNnBBWrWYmVlhqQVBRJy7l+UBfD2t7ZuZWXH8ZLGZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnGpBoGk0yStkrRa0sw8y4dKulfSU5JWSLogzXrMzKyz1IJAUiVwHTAVmAicK2lih2ZfB56NiA8BJwE/kDQgrZrMzKyzNI8IjgVWR8QLEbENuBU4o0ObAGolCRgCvAHsSLEmMzPrIM0gOBB4ud10UzKvvR8BhwPNwHLg4ojY2XFFkmZIapTU2NLSkla9ZmaZlGYQKM+86DB9KrAMGA1MBn4k6T2d3hQxNyIaIqKhvr6+p+s0M8u0NIOgCTio3fQYcn/5t3cBMD9yVgN/AQ5LsSYzM+sgzSBYAoyXNC65AHwOcE+HNi8BnwCQtD8wAXghxZrMzKyDqrRWHBE7JF0ELAQqgXkRsULShcnyOcC/Aj+TtJzcqaTLIuL1tGoyM7POUgsCgIhYACzoMG9Ou9fNwClp1mBmZl3zk8VmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8u4VINA0mmSVklaLWlmgTYnSVomaYWkR9Osx8zMOqtKa8WSKoHrgE8CTcASSfdExLPt2gwDrgdOi4iXJI1Kqx4zM8svzSOCY4HVEfFCRGwDbgXO6NDmPGB+RLwEEBFrU6zHzMzySDMIDgRebjfdlMxr7wPAcEmPSFoq6fP5ViRphqRGSY0tLS0plWtmlk1pBoHyzIsO01XAMcCngFOBb0v6QKc3RcyNiIaIaKivr+/5Ss3MMqyoIJB0h6RPSepOcDQBB7WbHgM052lzf0RsiojXgceAD3VjG2Zm9i4V+4t9Nrnz+c9JmiXpsCLeswQYL2mcpAHAOcA9HdrcDZwgqUrSYOAjwMoiazIzsx5Q1F1DEfEg8KCkocC5wAOSXgZ+DNwUEdvzvGeHpIuAhUAlMC8iVki6MFk+JyJWSrofeBrYCdwQEc/0SM/MzKwoiuh42r5AQ6kOmA6cT+4Uz83Ax4APRsRJaRXYUUNDQzQ2NvbW5szM+gVJSyOiId+yoo4IJM0HDgN+AXwmIl5JFv1Kkn8rm5n1YcU+UPajiHg434JCCWNmZn1DsReLD0+eAgZA0nBJX0unJDMz603FBsFXImL9romIeBP4SioVmZlZryo2CCok7X5ALBlHaEA6JZmZWW8q9hrBQuA2SXPIPR18IXB/alWZmVmvKTYILgO+CvwzuaEjFgE3pFWUmZn1nmIfKNtJ7uni2emWY2Zmva3Y5wjGA/8OTARqds2PiENSqsvMzHpJsReLf0ruaGAHcDLwc3IPl5mZWR9XbBAMioiHyA1J8WJEXAV8PL2yzMystxR7sXhLMgT1c8lAcmsAf6ykmVk/UOwRwbeAwcA3yX2QzHTgCynVZGZmvWivRwTJw2NnR8SlQCtwQepVmZlZr9nrEUFEtAHHtH+y2MzM+o9irxE8Cdwt6b+BTbtmRsT8VKoyM7NeU2wQjADW8c47hQJwEJiZ9XHFPlns6wJmZv1UsU8W/5TcEcA7RMQ/9XhFZmbWq4o9NfTrdq9rgGnkPrfYzMz6uGJPDd3RflrSL4EHU6nIzMx6VbEPlHU0Hji4JwsxM7PSKPYawUbeeY3gVXKfUWBmZn1csaeGatMuxMzMSqOoU0OSpkka2m56mKQzU6vKzMx6TbHXCL4bERt2TUTEeuC7qVRkZma9qtggyNeu2FtPzcysjBUbBI2SrpF0qKRDJP0nsDTNwszMrHcUGwTfALYBvwJuAzYDX0+rKDMz6z3F3jW0CZiZci1mZlYCxd419ICkYe2mh0tamFpVZmbWa4o9NTQyuVMIgIh4E39msZlZv1BsEOyUtHtICUljyTMaqZmZ9T3F3gJ6BfBbSY8m0ycCM9IpyczMelOxF4vvl9RA7pf/MuBucncOmZlZH1fsxeIvAw8B/zv5+gVwVRHvO03SKkmrJRW860jShyW1STqruLLNzKynFHuN4GLgw8CLEXEycBTQ0tUbJFUC1wFTgYnAuZImFmj3PcB3IZmZlUCxQbAlIrYASBoYEX8CJuzlPccCqyPihYjYBtwKnJGn3TeAO4C1RdZiZmY9qNggaEqeI7gLeEDS3ez9oyoPBF5uv45k3m6SDiT3sZdzulqRpBmSGiU1trR0eSBiZmbdVOzF4mnJy6skLQaGAvfv5W3Kt6oO09cCl0VEm5Sv+e7tzwXmAjQ0NPi2VTOzHtTtEUQj4tG9twJyRwAHtZseQ+ejiAbg1iQERgKnS9oREXd1ty4zM9s3aQ4lvQQYL2kcsAY4BzivfYOIGLfrtaSfAb92CJiZ9a7UgiAidki6iNzdQJXAvIhYIenCZHmX1wXMzKx3pPrhMhGxAFjQYV7eAIiIL6ZZi5mZ5VfsXUNmZtZPOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyLtUgkHSapFWSVkuamWf55yQ9nXz9TtKH0qzHzMw6Sy0IJFUC1wFTgYnAuZImdmj2F+BvImIS8K/A3LTqMTOz/NI8IjgWWB0RL0TENuBW4Iz2DSLidxHxZjL5B2BMivWYmVkeaQbBgcDL7aabknmFfAn4Tb4FkmZIapTU2NLS0oMlmplZmkGgPPMib0PpZHJBcFm+5RExNyIaIqKhvr6+B0s0M7OqFNfdBBzUbnoM0NyxkaRJwA3A1IhYl2I9ZmaWR5pHBEuA8ZLGSRoAnAPc076BpIOB+cD5EfHnFGsxM7MCUjsiiIgdki4CFgKVwLyIWCHpwmT5HOA7QB1wvSSAHRHRkFZNZmbWmSLynrYvWw0NDdHY2Jja+u96cg3fX7iK5vWbGT1sEJeeOoEzj+rqGreZWfmTtLTQH9ppXiPoc+56cg2Xz1/O5u1tAKxZv5nL5y8HcBiY9XHbt2+nqamJLVu2lLqUVNXU1DBmzBiqq6uLfo+DAGjbGdz7VDP/cf+fdofALpu3t/H9hascBGZ9XFNTE7W1tYwdO5bkVHS/ExGsW7eOpqYmxo0bV/T7PNYQsGjFq3zrV8to3pD/L4Xm9Zt7uSIz62lbtmyhrq6u34YAgCTq6uq6fdTjIABeKRAAu4weNqiXKjGzNPXnENhlX/roIABeeL119+vKinf+EGuqK7j01Am9XZKZWa/xNQLg+bWbqK8dyMIdX2JErO/c4MFRcNRzvV6XmZVOT99BuH79em655Ra+9rWvdet9p59+OrfccgvDhg3b523vjY8IgOdbWvmbD9TnDwGATWs7zbrryTVMmfUw42bex5RZD3PXk2vSLdLMes2uOwjXrN9MsOcOwnfz/3z9+vVcf/31nea3tbXlab3HggULUg0B8BEBb23ZztqNW5lUu6no93TnNtNi/6oo93Z9oUb/bMqvXTnV+MOpe8Ypu/reFTzb/BYAO3YG23bsJCKQxICqCpY3bWBb2853rG/z9jYuvf0pbvifF3a3q2p3Knni6Pfw3c8cwZtvb+O1DVvY1raTAZUV7D+0huGDBzBz5kyef/55Jk+eTHV1NUOGDKGufn+WLVvG/If/wL98eTqvv9bM9m1bufjii5kxYwYAY8eO5cHHfseLr67jK587i2OOPY7lTy7h4DFjuPvuuxk06N1fw8z8A2XLXl7PFdffxL2DrqJi5/aC7Y6q+G/OP34sJ02o5+B5kxnJ+k5tWmIo5w+/iTVv5u4y2tbWxo42aGv3Mx5YVcE/n3QoJ35gzz/Kx/7cwuxHnmfrjp1l2a4v1OifTfm1K7cabzjjAD54xESGDKzi33+zklWvbqRtZ+R+4bf/NSh4Zs1bFHLk6PfsbjegsmL3dcUJ763lGx8fT8vGrexs93++QqK+diDrXmnirL87kyVPLOOxRx/l76d9lvkP/o7RB70PgA1vvsnwESMYUrWT0z9+Avc/8BB1dXUc9oFDueXXi2ltbeUzJxzNLfctZuKRk7jym1/irGlnMn369E41rly5ksMPP/wd87p6oCzzQXDzH19k+72X8PmBj1HRVvjuoS+Pe4gHV74GwF9rzivYbkrNnZxyxP4IceuSl3h7W9eHfWbWO3782QPY/+BDimr7pRsbadm4tdP8+tqB/OQL+zYKzpqXX+IbX/xH5j/0e5b8/rfM+c/v8ZPb7t29fPY1s3j4/l8D0Nz0ErNvuoNJR3+YqcdP4pb7FvP2pk1ceN407v2fpQDcOPv/MXxQJVdeeWWnbXU3CDJ/aujZpnX8S9Uf0YRT4dm7C7a7oXIWq869lFdqxsEvC69v4TePY8hLj8CmFr6pyxlW0/mUU0sMZcV5e8LsiFsaqNeGsm3XF2r0z6b82pVbjSt1G+9XFVvq9vyCrFm3kmp1/mNt5vH7ccXi7WzZvueIYlCVuOKjg5hU8Zfd87ZHZVHr2x6VVIwYxICqCsaN3I+moTWMGqzd63rkd40s/+1Cnrx3LtU1Q/j4ud9gRI0YN3I/qmnjiIoXaa3YTO3APe8ZVfEWm3fUdtrWvshEEHR1TnHri43UsQGOmAYv/j7vhWGqBkLzE0x4/StMmFz4aABgyA/eB23bABhW4Hbeem3gpAmj9szI8w+7rNr1hRr9sym/dmVYY7XaqK5pN/RCnl/aAH9/WA2Vwyft+b1RW8mlH63lzAmD92l91WrjgJEj2NTaSm1NNYMHVKF256M2bGxl+NBaBg8axJ9Wr2bJ439k8IAqamuqKfAxLlRqZ975+6LfB0FXF3anfvC9DHjjz7mfwuij4NIubhF9eQn84kx4ZFbXG/zIhTD2YzBqIlx7ZOF2N/8DVFRBRWXX67vzwq6Xl7pdKbdd7u1Kue1yb1fKbb/5YlHNzhy7gzO/emhuYvMb73p9dRWtTPnwZI48fAKDBtWw/4j37F522kkfZc4vbmfS357NhEPGclzDZNj4atHrfrf6/TWCKbMeZk2eISKGD65m+OABnL3+x3xlwEIqr3xt77+Ut72d++X9b118StpV7f4SuWpo4Xajj4KdO2BnG6x9tnC7YQfveb3+pdK0K+W2y71dKbdd7u1Kue087VaeehuHv28UVA7YMzM5es8rzXY9tc7RR+Wd7WsEHdy1+YvU1+Q5p9g2lAsG3syJdRtQ1bi9hwDAgOSwcL9R+U8h7Teq87xCZjyy53VXgfGt5aVvV8ptl3u7Um673NuVcttdtdv/iD2vm58sTbueWmcP6fdBkO/C0q75d35tCtVzXocR7+/eSrs6hWRm1sdk+sniagFvvAB1h6azgUJHCB3nl3u7Um673NuVctvl3q6U2y7UrqKq6+neapfWOvdRv79GsNdDyWs/CJ++FhoueNe1mVn5ynfevL/q7jWCTB8RcMMnc9/fO6m0dZiZlVC/v0bQpdGToeGfYMwxpa7EzKxk+n8QFLzDpx7O+1Xv12Nm5e/74wvfGbiPN4vs6zDUANdeey0zZsxg8ODBe2+8D/p/EPgOHzPrrnwh0NX8Iuwahnpfg2D69OkOAjOzHvObmfDq8r23y+enn8o//70fhKmFRx5oPwz1Jz/5SUaNGsVtt93G1q1bmTZtGldffTWbNm3i7LPPpqmpiba2Nr797W/z2muv0dzczMknn8zIkSNZvHjxvtXdBQeBmVkvmDVrFs888wzLli1j0aJF3H777Tz++ONEBJ/97Gd57LHHaGlpYfTo0dx3330AbNiwgaFDh3LNNdewePFiRo4cmUptDgIzy54u/nIHur7t/IL73vXmFy1axKJFizjqqNwQEa2trTz33HOccMIJXHLJJVx22WV8+tOf5oQTTnjX2yqGg8DMrJdFBJdffjlf/epXOy1bunQpCxYs4PLLL+eUU07hO9/5Tur1ZPs5AjOzfLrz9HSRamtr2bhxIwCnnnoq8+bNo7W1FYA1a9awdu1ampubGTx4MNOnT+eSSy7hiSee6PTeNPiIwMysoxTuNqyrq2PKlCkceeSRTJ06lfPOO4/jjz8egCFDhnDTTTexevVqLr30UioqKqiurmb27NkAzJgxg6lTp3LAAQekcrG4/w8xYWaGh5jwEBNmZlaQg8DMLOMcBGaWGX3tVPi+2Jc+OgjMLBNqampYt25dvw6DiGDdunXU1NR0632+a8jMMmHMmDE0NTXR0tJS6lJSVVNTw5gxY7r1HgeBmWVCdXU148aNK3UZZSnVU0OSTpO0StJqSTPzLJekHybLn5Z0dJr1mJlZZ6kFgaRK4DpgKjAROFfSxA7NpgLjk68ZwOy06jEzs/zSPCI4FlgdES9ExDbgVuCMDm3OAH4eOX8Ahkk6IMWazMysgzSvERwIvNxuugn4SBFtDgRead9I0gxyRwwArZJW7WNNI4HX9/G95cZ9KU/9pS/9pR/gvuzyvkIL0gwC5ZnX8b6tYtoQEXOBue+6IKmx0CPWfY37Up76S1/6Sz/AfSlGmqeGmoCD2k2PAZr3oY2ZmaUozSBYAoyXNE7SAOAc4J4Obe4BPp/cPXQcsCEiXum4IjMzS09qp4YiYoeki4CFQCUwLyJWSLowWT4HWACcDqwG3gYuSKuexLs+vVRG3Jfy1F/60l/6Ae7LXvW5YajNzKxneawhM7OMcxCYmWVcZoJgb8NdlDtJf5W0XNIySY3JvBGSHpD0XPJ9eKnr7EjSPElrJT3Tbl7BuiVdnuyjVZJOLU3V+RXoy1WS1iT7ZZmk09stK+e+HCRpsaSVklZIujiZ36f2TRf96HP7RVKNpMclPZX05epkfvr7JCL6/Re5i9XPA4cAA4CngImlrqubffgrMLLDvP8AZiavZwLfK3Wdeeo+ETgaeGZvdZMbiuQpYCAwLtlnlaXuw176chVwSZ625d6XA4Cjk9e1wJ+TmvvUvumiH31uv5B7rmpI8roa+CNwXG/sk6wcERQz3EVfdAZwY/L6RuDM0pWSX0Q8BrzRYXahus8Abo2IrRHxF3J3kx3bG3UWo0BfCin3vrwSEU8krzcCK8k91d+n9k0X/SikLPsBEDmtyWR18hX0wj7JShAUGsqiLwlgkaSlyZAbAPtH8txF8n1UyarrnkJ199X9dFEyeu68doftfaYvksYCR5H7C7TP7psO/YA+uF8kVUpaBqwFHoiIXtknWQmCooayKHNTIuJociO2fl3SiaUuKAV9cT/NBg4FJpMbI+sHyfw+0RdJQ4A7gG9FxFtdNc0zr2z6k6cffXK/RERbREwmN8rCsZKO7KJ5j/UlK0HQ54eyiIjm5Pta4E5yh4Cv7RqtNfm+tnQVdkuhuvvcfoqI15L/vDuBH7Pn0Lzs+yKpmtwvz5sjYn4yu8/tm3z96Mv7BSAi1gOPAKfRC/skK0FQzHAXZUvSfpJqd70GTgGeIdeHLyTNvgDcXZoKu61Q3fcA50gaKGkcuc+peLwE9RVN7xw2fRq5/QJl3hdJAn4CrIyIa9ot6lP7plA/+uJ+kVQvaVjyehDwt8Cf6I19Uuor5b14Rf50cncUPA9cUep6uln7IeTuDngKWLGrfqAOeAh4Lvk+otS15qn9l+QOzbeT+wvmS13VDVyR7KNVwNRS119EX34BLAeeTv5jHtBH+vIxcqcRngaWJV+n97V900U/+tx+ASYBTyY1PwN8J5mf+j7xEBNmZhmXlVNDZmZWgIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzFIm6SRJvy51HWaFOAjMzDLOQWCWkDQ9GQ9+maT/SgYAa5X0A0lPSHpIUn3SdrKkPySDmt25a1AzSe+X9GAypvwTkg5NVj9E0u2S/iTp5uSJWCTNkvRssp7/W6KuW8Y5CMwASYcD/0hucL/JQBvwOWA/4InIDfj3KPDd5C0/By6LiEnknmDdNf9m4LqI+BDwUXJPIkNuVMxvkRtD/hBgiqQR5IY/OCJZz7+l2UezQhwEZjmfAI4BliTDAH+C3C/sncCvkjY3AR+TNBQYFhGPJvNvBE5MxoM6MCLuBIiILRHxdtLm8YhoitwgaMuAscBbwBbgBkl/B+xqa9arHARmOQJujIjJydeEiLgqT7uuxmTJNyzwLlvbvW4DqiJiB7lRMe8g92Ej93evZLOe4SAwy3kIOEvSKNj9ObHvI/d/5KykzXnAbyNiA/CmpBOS+ecDj0ZuHPwmSWcm6xgoaXChDSZj6A+NiAXkThtN7vFemRWhqtQFmJWDiHhW0pXkPgWugtwIo18HNgFHSFoKbCB3HQFywwHPSX7RvwBckMw/H/gvSf8nWcc/dLHZWuBuSTXkjib+Vw93y6woHn3UrAuSWiNiSKnrMEuTTw2ZmWWcjwjMzDLORwRmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZx/x/UwYLjLgmcagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#遊びで、dropout_ratioが極端に高い場合を２例\n",
    "trial(use_dropout=True, dropout_ratio=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3024271042392277\n",
      "=== epoch:1, train acc:0.10666666666666667, test acc:0.0982 ===\n",
      "train loss:2.302404353926786\n",
      "train loss:2.302564911405997\n",
      "train loss:2.302333440890149\n",
      "=== epoch:2, train acc:0.10666666666666667, test acc:0.0982 ===\n",
      "train loss:2.3022444831633426\n",
      "train loss:2.302466376934317\n",
      "train loss:2.302320257728447\n",
      "=== epoch:3, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302230819723866\n",
      "train loss:2.302266720181889\n",
      "train loss:2.3020863567871914\n",
      "=== epoch:4, train acc:0.10333333333333333, test acc:0.1009 ===\n",
      "train loss:2.3022339313106075\n",
      "train loss:2.302115423997999\n",
      "train loss:2.302395420061067\n",
      "=== epoch:5, train acc:0.11333333333333333, test acc:0.101 ===\n",
      "train loss:2.3020631913506886\n",
      "train loss:2.3020625578933993\n",
      "train loss:2.301671073376626\n",
      "=== epoch:6, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301893538312859\n",
      "train loss:2.301811920518744\n",
      "train loss:2.3024787423738933\n",
      "=== epoch:7, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3022087268669096\n",
      "train loss:2.3021427221499895\n",
      "train loss:2.3019898316205403\n",
      "=== epoch:8, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302109121789985\n",
      "train loss:2.3019754484053094\n",
      "train loss:2.301806595126387\n",
      "=== epoch:9, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301750475670382\n",
      "train loss:2.3018226931117725\n",
      "train loss:2.300879870826475\n",
      "=== epoch:10, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3009458993122633\n",
      "train loss:2.3018424952408685\n",
      "train loss:2.300392398154632\n",
      "=== epoch:11, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302107753835935\n",
      "train loss:2.301097006311123\n",
      "train loss:2.3014885804990377\n",
      "=== epoch:12, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3010338425392365\n",
      "train loss:2.3011384263456667\n",
      "train loss:2.301562616515325\n",
      "=== epoch:13, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3022449888771215\n",
      "train loss:2.301607302910519\n",
      "train loss:2.3009728871083963\n",
      "=== epoch:14, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300281599138006\n",
      "train loss:2.3018782108344955\n",
      "train loss:2.301638414297253\n",
      "=== epoch:15, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3013882258324014\n",
      "train loss:2.301229435662675\n",
      "train loss:2.30029080944296\n",
      "=== epoch:16, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3015801293930482\n",
      "train loss:2.3008609946430174\n",
      "train loss:2.301074513553773\n",
      "=== epoch:17, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302716491007584\n",
      "train loss:2.3020633353204336\n",
      "train loss:2.3026327839775083\n",
      "=== epoch:18, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3004319148618158\n",
      "train loss:2.300887361863852\n",
      "train loss:2.301544541377656\n",
      "=== epoch:19, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299895592089148\n",
      "train loss:2.3014871196058335\n",
      "train loss:2.299645260398742\n",
      "=== epoch:20, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3009135310430673\n",
      "train loss:2.3006696813836975\n",
      "train loss:2.299791019678505\n",
      "=== epoch:21, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299819372070435\n",
      "train loss:2.301069486141902\n",
      "train loss:2.3024315030932185\n",
      "=== epoch:22, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2998311813956045\n",
      "train loss:2.299968020724711\n",
      "train loss:2.299544873876641\n",
      "=== epoch:23, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301346933226841\n",
      "train loss:2.3022944193732577\n",
      "train loss:2.301208804971398\n",
      "=== epoch:24, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301031825590374\n",
      "train loss:2.2995468202379374\n",
      "train loss:2.2987266323687816\n",
      "=== epoch:25, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3012849049924364\n",
      "train loss:2.3014239742107265\n",
      "train loss:2.2985028119671305\n",
      "=== epoch:26, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300595957946141\n",
      "train loss:2.2997558045384277\n",
      "train loss:2.297988344387874\n",
      "=== epoch:27, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.303922121488077\n",
      "train loss:2.3010073453019477\n",
      "train loss:2.302197208413642\n",
      "=== epoch:28, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2996765932884204\n",
      "train loss:2.301227542052857\n",
      "train loss:2.2991129537295865\n",
      "=== epoch:29, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3001735551008\n",
      "train loss:2.2999174414838537\n",
      "train loss:2.299872536728805\n",
      "=== epoch:30, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2998407383875814\n",
      "train loss:2.301174084080875\n",
      "train loss:2.2996112875526014\n",
      "=== epoch:31, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299338142720289\n",
      "train loss:2.303246980401187\n",
      "train loss:2.3031717699087655\n",
      "=== epoch:32, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2991856989021815\n",
      "train loss:2.2997727698621695\n",
      "train loss:2.29969731447129\n",
      "=== epoch:33, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298683417356979\n",
      "train loss:2.302501117286134\n",
      "train loss:2.299672575213884\n",
      "=== epoch:34, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299661542384698\n",
      "train loss:2.2988471536111263\n",
      "train loss:2.2996712598655384\n",
      "=== epoch:35, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300250944370682\n",
      "train loss:2.2995289022965704\n",
      "train loss:2.300503516476616\n",
      "=== epoch:36, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3004216600270606\n",
      "train loss:2.3010563469576186\n",
      "train loss:2.299159313326837\n",
      "=== epoch:37, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3022886136219887\n",
      "train loss:2.2990087916604223\n",
      "train loss:2.3003478996675177\n",
      "=== epoch:38, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2977346196253445\n",
      "train loss:2.2979333065509557\n",
      "train loss:2.300042467561486\n",
      "=== epoch:39, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2967726668883097\n",
      "train loss:2.301610243284448\n",
      "train loss:2.299829395182479\n",
      "=== epoch:40, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298045457955906\n",
      "train loss:2.3013727019031207\n",
      "train loss:2.2997378614162445\n",
      "=== epoch:41, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.303070942362752\n",
      "train loss:2.2988388063020775\n",
      "train loss:2.2976911509831184\n",
      "=== epoch:42, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3012506393576\n",
      "train loss:2.2997347599337554\n",
      "train loss:2.299286689884744\n",
      "=== epoch:43, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3006037086095317\n",
      "train loss:2.3023647866945285\n",
      "train loss:2.301157824202237\n",
      "=== epoch:44, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299532449530301\n",
      "train loss:2.301503742023831\n",
      "train loss:2.2996702344343087\n",
      "=== epoch:45, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.30332059884219\n",
      "train loss:2.298055162335\n",
      "train loss:2.3014948754566418\n",
      "=== epoch:46, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2975615692604143\n",
      "train loss:2.293746842899828\n",
      "train loss:2.302385373898672\n",
      "=== epoch:47, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2963310947738176\n",
      "train loss:2.30066273389288\n",
      "train loss:2.2977821299763446\n",
      "=== epoch:48, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2980983325618913\n",
      "train loss:2.2986029358068287\n",
      "train loss:2.296406150051632\n",
      "=== epoch:49, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2976915646509086\n",
      "train loss:2.3048198165386076\n",
      "train loss:2.2971915215212566\n",
      "=== epoch:50, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2978620114086454\n",
      "train loss:2.2985471232874257\n",
      "train loss:2.2988189259238667\n",
      "=== epoch:51, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2977922856942676\n",
      "train loss:2.3024576638234175\n",
      "train loss:2.298855973804116\n",
      "=== epoch:52, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2995742350427415\n",
      "train loss:2.2975381818881035\n",
      "train loss:2.2963407107057923\n",
      "=== epoch:53, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297831129192576\n",
      "train loss:2.296798112870542\n",
      "train loss:2.2987268861989794\n",
      "=== epoch:54, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298647211327408\n",
      "train loss:2.301742952314344\n",
      "train loss:2.300091964793602\n",
      "=== epoch:55, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2987005061790806\n",
      "train loss:2.3011736513918386\n",
      "train loss:2.3040458369935632\n",
      "=== epoch:56, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295267856436223\n",
      "train loss:2.295882979382143\n",
      "train loss:2.299711802766892\n",
      "=== epoch:57, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295147747134025\n",
      "train loss:2.297349581624874\n",
      "train loss:2.296491726399593\n",
      "=== epoch:58, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3027263264313356\n",
      "train loss:2.3030146737137853\n",
      "train loss:2.3007110350996327\n",
      "=== epoch:59, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2976239200723545\n",
      "train loss:2.2972959303978215\n",
      "train loss:2.300504956969156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:60, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2967544371650304\n",
      "train loss:2.2968488998799357\n",
      "train loss:2.3006711214929383\n",
      "=== epoch:61, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3000958476968436\n",
      "train loss:2.299800964161025\n",
      "train loss:2.2996582996148502\n",
      "=== epoch:62, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296167237927327\n",
      "train loss:2.3000588629253866\n",
      "train loss:2.3029745817516725\n",
      "=== epoch:63, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2995347639071775\n",
      "train loss:2.302642031488735\n",
      "train loss:2.298863232054889\n",
      "=== epoch:64, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298310344359558\n",
      "train loss:2.294158138874757\n",
      "train loss:2.3014760650747514\n",
      "=== epoch:65, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.30036910409893\n",
      "train loss:2.2979064017911326\n",
      "train loss:2.293094636041616\n",
      "=== epoch:66, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2972831994775063\n",
      "train loss:2.294411904502393\n",
      "train loss:2.293883935980971\n",
      "=== epoch:67, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2972489479728906\n",
      "train loss:2.2973675162714304\n",
      "train loss:2.2970011059849114\n",
      "=== epoch:68, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3029151560777956\n",
      "train loss:2.290352387599427\n",
      "train loss:2.2974968839941297\n",
      "=== epoch:69, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2952126456669077\n",
      "train loss:2.293853053771718\n",
      "train loss:2.287510933669974\n",
      "=== epoch:70, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296580928229005\n",
      "train loss:2.295293560917667\n",
      "train loss:2.296548564335984\n",
      "=== epoch:71, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2981083259014286\n",
      "train loss:2.293370141994942\n",
      "train loss:2.3037319552903326\n",
      "=== epoch:72, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.304365345584945\n",
      "train loss:2.2992627326662247\n",
      "train loss:2.297180756038794\n",
      "=== epoch:73, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2979574692062625\n",
      "train loss:2.297743916821656\n",
      "train loss:2.2949610125749573\n",
      "=== epoch:74, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2933981296783146\n",
      "train loss:2.300336919215995\n",
      "train loss:2.2964147312055836\n",
      "=== epoch:75, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293657416031052\n",
      "train loss:2.30044696800043\n",
      "train loss:2.3031116080752203\n",
      "=== epoch:76, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2986093360706414\n",
      "train loss:2.2974615113422074\n",
      "train loss:2.2981242409612137\n",
      "=== epoch:77, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297442082383632\n",
      "train loss:2.3026451276607736\n",
      "train loss:2.2982099606747486\n",
      "=== epoch:78, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2960583320775014\n",
      "train loss:2.297924656878269\n",
      "train loss:2.298059099835866\n",
      "=== epoch:79, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2991352694373957\n",
      "train loss:2.3028484282483506\n",
      "train loss:2.295244322290803\n",
      "=== epoch:80, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3024063351905846\n",
      "train loss:2.301501275124244\n",
      "train loss:2.3016205899605957\n",
      "=== epoch:81, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2957729992853\n",
      "train loss:2.301137971336566\n",
      "train loss:2.299881081043755\n",
      "=== epoch:82, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.305311780133158\n",
      "train loss:2.300769525644167\n",
      "train loss:2.29741823906\n",
      "=== epoch:83, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2968268005517154\n",
      "train loss:2.2931642289268894\n",
      "train loss:2.294326999061683\n",
      "=== epoch:84, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297419057211007\n",
      "train loss:2.295491172637439\n",
      "train loss:2.295845082485054\n",
      "=== epoch:85, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.30622201051828\n",
      "train loss:2.2985885985933145\n",
      "train loss:2.293152622841216\n",
      "=== epoch:86, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951833805369843\n",
      "train loss:2.297926118589046\n",
      "train loss:2.300791618803485\n",
      "=== epoch:87, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300292587078461\n",
      "train loss:2.298568698132604\n",
      "train loss:2.2978223698057714\n",
      "=== epoch:88, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2982521534499964\n",
      "train loss:2.294681080999146\n",
      "train loss:2.2947624483287505\n",
      "=== epoch:89, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292700876260034\n",
      "train loss:2.2996265643485625\n",
      "train loss:2.303028997153526\n",
      "=== epoch:90, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288655272436683\n",
      "train loss:2.2956002777472233\n",
      "train loss:2.301306355698795\n",
      "=== epoch:91, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291083362506848\n",
      "train loss:2.303090689848038\n",
      "train loss:2.2905119139758425\n",
      "=== epoch:92, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3013864327803377\n",
      "train loss:2.292237110738765\n",
      "train loss:2.299278342746203\n",
      "=== epoch:93, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294610689229944\n",
      "train loss:2.3008817134883315\n",
      "train loss:2.291680020900024\n",
      "=== epoch:94, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951777807241736\n",
      "train loss:2.2955401978388967\n",
      "train loss:2.293452870958331\n",
      "=== epoch:95, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3025380708793897\n",
      "train loss:2.292729726015381\n",
      "train loss:2.2991522475092427\n",
      "=== epoch:96, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295952138037912\n",
      "train loss:2.2950112664532667\n",
      "train loss:2.2976666776244326\n",
      "=== epoch:97, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290369107002086\n",
      "train loss:2.2932977976879223\n",
      "train loss:2.296868241175406\n",
      "=== epoch:98, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2950049903405083\n",
      "train loss:2.29267555921641\n",
      "train loss:2.2959880627472073\n",
      "=== epoch:99, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295462030465744\n",
      "train loss:2.2887891048613\n",
      "train loss:2.291495322651756\n",
      "=== epoch:100, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297780157404715\n",
      "train loss:2.2894660072441133\n",
      "train loss:2.300144888202906\n",
      "=== epoch:101, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2986370701800136\n",
      "train loss:2.2947300233314034\n",
      "train loss:2.2960438185654395\n",
      "=== epoch:102, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2940210976353996\n",
      "train loss:2.2969891131957816\n",
      "train loss:2.294285870030425\n",
      "=== epoch:103, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290335697027594\n",
      "train loss:2.294821174289713\n",
      "train loss:2.29918672025597\n",
      "=== epoch:104, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2938295930700723\n",
      "train loss:2.2977446004558577\n",
      "train loss:2.2938438094433393\n",
      "=== epoch:105, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290740735020829\n",
      "train loss:2.295288118102959\n",
      "train loss:2.299516259772304\n",
      "=== epoch:106, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294264068317121\n",
      "train loss:2.29047690791133\n",
      "train loss:2.2932260750147315\n",
      "=== epoch:107, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296488081579809\n",
      "train loss:2.300112284268338\n",
      "train loss:2.2955244200826734\n",
      "=== epoch:108, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3029626801981067\n",
      "train loss:2.2953761944060562\n",
      "train loss:2.2923528923101486\n",
      "=== epoch:109, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2924924026226345\n",
      "train loss:2.2977486293370877\n",
      "train loss:2.30813300655086\n",
      "=== epoch:110, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.289479361640499\n",
      "train loss:2.2970542325609147\n",
      "train loss:2.2885538309194104\n",
      "=== epoch:111, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2962081473692337\n",
      "train loss:2.3025356530862324\n",
      "train loss:2.298299037813994\n",
      "=== epoch:112, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2925055348745254\n",
      "train loss:2.2899067963249333\n",
      "train loss:2.293040677313265\n",
      "=== epoch:113, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3008521766217007\n",
      "train loss:2.2946478847856775\n",
      "train loss:2.2924893021144697\n",
      "=== epoch:114, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3026088626200063\n",
      "train loss:2.295845905673646\n",
      "train loss:2.2848192249296733\n",
      "=== epoch:115, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2954030954743048\n",
      "train loss:2.2999570825808515\n",
      "train loss:2.298807806277105\n",
      "=== epoch:116, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302095498898488\n",
      "train loss:2.2964520777780844\n",
      "train loss:2.298982193958021\n",
      "=== epoch:117, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294483793127948\n",
      "train loss:2.298383554826443\n",
      "train loss:2.298299004630563\n",
      "=== epoch:118, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2992577876563125\n",
      "train loss:2.294064981970982\n",
      "train loss:2.3002554915289055\n",
      "=== epoch:119, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2907398107853068\n",
      "train loss:2.29962749145233\n",
      "train loss:2.292195845832989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:120, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3064675722602543\n",
      "train loss:2.295121873642679\n",
      "train loss:2.28840678548444\n",
      "=== epoch:121, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298229496248158\n",
      "train loss:2.299337592530497\n",
      "train loss:2.2933774045184414\n",
      "=== epoch:122, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290316512976953\n",
      "train loss:2.2977459853909235\n",
      "train loss:2.3025936387451877\n",
      "=== epoch:123, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294159402074753\n",
      "train loss:2.299725086593022\n",
      "train loss:2.293184936038452\n",
      "=== epoch:124, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2949425368027274\n",
      "train loss:2.2961251258159097\n",
      "train loss:2.2974384254936693\n",
      "=== epoch:125, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292448402577312\n",
      "train loss:2.29127077172556\n",
      "train loss:2.2958087594691414\n",
      "=== epoch:126, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302083609343977\n",
      "train loss:2.3107979155934117\n",
      "train loss:2.295488572946459\n",
      "=== epoch:127, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2961362826101697\n",
      "train loss:2.2908970663380672\n",
      "train loss:2.294994358854581\n",
      "=== epoch:128, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293942556342973\n",
      "train loss:2.305553052746363\n",
      "train loss:2.297480800889738\n",
      "=== epoch:129, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2962470650111975\n",
      "train loss:2.2959356650722467\n",
      "train loss:2.2939537818465885\n",
      "=== epoch:130, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.30100584670208\n",
      "train loss:2.2924377324880623\n",
      "train loss:2.2929382807979284\n",
      "=== epoch:131, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2866084456633597\n",
      "train loss:2.2956271607515166\n",
      "train loss:2.3102877325093285\n",
      "=== epoch:132, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.289634313337794\n",
      "train loss:2.3033387708764015\n",
      "train loss:2.2913862916757948\n",
      "=== epoch:133, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2926953283266567\n",
      "train loss:2.296845503527599\n",
      "train loss:2.28191967653426\n",
      "=== epoch:134, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302688021431614\n",
      "train loss:2.2889118701590023\n",
      "train loss:2.3048769952582364\n",
      "=== epoch:135, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297901847424589\n",
      "train loss:2.2969894793517884\n",
      "train loss:2.2972509186679577\n",
      "=== epoch:136, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2931129699161894\n",
      "train loss:2.294900605030798\n",
      "train loss:2.2999654581233018\n",
      "=== epoch:137, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2973557708190446\n",
      "train loss:2.29593226807968\n",
      "train loss:2.2891947767855734\n",
      "=== epoch:138, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300233635136948\n",
      "train loss:2.29008176519601\n",
      "train loss:2.2877136021801725\n",
      "=== epoch:139, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.286101808945866\n",
      "train loss:2.294297071976468\n",
      "train loss:2.297262748601234\n",
      "=== epoch:140, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2940914723791455\n",
      "train loss:2.298580258177599\n",
      "train loss:2.293940219994328\n",
      "=== epoch:141, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3033586903716987\n",
      "train loss:2.2888896850914984\n",
      "train loss:2.29182235576332\n",
      "=== epoch:142, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301966882078625\n",
      "train loss:2.302909884905274\n",
      "train loss:2.3055337183705755\n",
      "=== epoch:143, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951973005496384\n",
      "train loss:2.2951232710978964\n",
      "train loss:2.2894576949873477\n",
      "=== epoch:144, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2995126342916263\n",
      "train loss:2.295780054656993\n",
      "train loss:2.2904404060985555\n",
      "=== epoch:145, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3063898224043067\n",
      "train loss:2.2971430604056007\n",
      "train loss:2.296317621017397\n",
      "=== epoch:146, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295673703587748\n",
      "train loss:2.301947865548629\n",
      "train loss:2.296075171826917\n",
      "=== epoch:147, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2966075578528504\n",
      "train loss:2.2910811313098307\n",
      "train loss:2.298277221844615\n",
      "=== epoch:148, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2923750153228255\n",
      "train loss:2.3078769009136453\n",
      "train loss:2.2871805563628755\n",
      "=== epoch:149, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297365075676217\n",
      "train loss:2.2936389085710323\n",
      "train loss:2.27974098691452\n",
      "=== epoch:150, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2969384275267415\n",
      "train loss:2.27814992158796\n",
      "train loss:2.292843470193258\n",
      "=== epoch:151, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951755367437205\n",
      "train loss:2.302427661629029\n",
      "train loss:2.2907492464522594\n",
      "=== epoch:152, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295876228268111\n",
      "train loss:2.295607942366391\n",
      "train loss:2.2972317532611384\n",
      "=== epoch:153, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2949727534279365\n",
      "train loss:2.2832945168540424\n",
      "train loss:2.3047996841643186\n",
      "=== epoch:154, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299428256863356\n",
      "train loss:2.2982692488266183\n",
      "train loss:2.2834547734778066\n",
      "=== epoch:155, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.304936396581466\n",
      "train loss:2.300701961116754\n",
      "train loss:2.2908523539553896\n",
      "=== epoch:156, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2939896348779096\n",
      "train loss:2.292218572411289\n",
      "train loss:2.2902788097032287\n",
      "=== epoch:157, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293621799753202\n",
      "train loss:2.2876581142257773\n",
      "train loss:2.2946226480821457\n",
      "=== epoch:158, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.29535818739261\n",
      "train loss:2.296530345002175\n",
      "train loss:2.2982175090309247\n",
      "=== epoch:159, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.289627136489661\n",
      "train loss:2.294564333993429\n",
      "train loss:2.283908994460924\n",
      "=== epoch:160, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2984672055630853\n",
      "train loss:2.2968083154702374\n",
      "train loss:2.2889988889632535\n",
      "=== epoch:161, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297173645213141\n",
      "train loss:2.3041053826552473\n",
      "train loss:2.302308866112412\n",
      "=== epoch:162, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2906249614711043\n",
      "train loss:2.2883059298917052\n",
      "train loss:2.2961888006590097\n",
      "=== epoch:163, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3031002029433214\n",
      "train loss:2.297410134328413\n",
      "train loss:2.292803966532226\n",
      "=== epoch:164, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.30178032633079\n",
      "train loss:2.2964379440244267\n",
      "train loss:2.2838560575796585\n",
      "=== epoch:165, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2993287931432733\n",
      "train loss:2.2965998236946854\n",
      "train loss:2.3048247805790023\n",
      "=== epoch:166, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2872227230085076\n",
      "train loss:2.2942732008777\n",
      "train loss:2.2907577520537985\n",
      "=== epoch:167, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2903876443861315\n",
      "train loss:2.278613066694713\n",
      "train loss:2.2935900410684056\n",
      "=== epoch:168, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3005604204237717\n",
      "train loss:2.2882622905327454\n",
      "train loss:2.294263851770163\n",
      "=== epoch:169, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2966316085659826\n",
      "train loss:2.2936093073648713\n",
      "train loss:2.301061459731234\n",
      "=== epoch:170, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297654287299045\n",
      "train loss:2.2926471391340164\n",
      "train loss:2.283674035362269\n",
      "=== epoch:171, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3027756826538686\n",
      "train loss:2.300719980542517\n",
      "train loss:2.2968095237250368\n",
      "=== epoch:172, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300606775120662\n",
      "train loss:2.3000959663992933\n",
      "train loss:2.3010956014119883\n",
      "=== epoch:173, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294561478581448\n",
      "train loss:2.2962604452982127\n",
      "train loss:2.2911195457617626\n",
      "=== epoch:174, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298447459299596\n",
      "train loss:2.298349740241446\n",
      "train loss:2.285863089285627\n",
      "=== epoch:175, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.314666649626949\n",
      "train loss:2.2962899518129656\n",
      "train loss:2.2886706540906205\n",
      "=== epoch:176, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296343976623801\n",
      "train loss:2.2962024173430162\n",
      "train loss:2.2966692943642286\n",
      "=== epoch:177, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2920878553975386\n",
      "train loss:2.297380093658102\n",
      "train loss:2.2892013487158427\n",
      "=== epoch:178, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2941399321959914\n",
      "train loss:2.2992543335102327\n",
      "train loss:2.2897926705430165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:179, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2908049091302445\n",
      "train loss:2.3000297389986977\n",
      "train loss:2.2828063421566793\n",
      "=== epoch:180, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293161502691601\n",
      "train loss:2.2964264474122116\n",
      "train loss:2.30751959727578\n",
      "=== epoch:181, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2902225816462876\n",
      "train loss:2.289915498534274\n",
      "train loss:2.295486794965727\n",
      "=== epoch:182, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.289885372585765\n",
      "train loss:2.28791605167248\n",
      "train loss:2.284256892924044\n",
      "=== epoch:183, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3001749343321025\n",
      "train loss:2.2863696761764927\n",
      "train loss:2.2863812068938514\n",
      "=== epoch:184, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291675995336345\n",
      "train loss:2.30154852110672\n",
      "train loss:2.27878083453762\n",
      "=== epoch:185, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2914697853339496\n",
      "train loss:2.291503778222453\n",
      "train loss:2.29593912111428\n",
      "=== epoch:186, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3052508245651695\n",
      "train loss:2.2904445317121263\n",
      "train loss:2.3001623428675786\n",
      "=== epoch:187, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2847693232848894\n",
      "train loss:2.2890873242224425\n",
      "train loss:2.296294043918849\n",
      "=== epoch:188, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3010590914524305\n",
      "train loss:2.294112943375692\n",
      "train loss:2.2971193633285627\n",
      "=== epoch:189, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295027426601896\n",
      "train loss:2.3035520006041144\n",
      "train loss:2.2832935235236347\n",
      "=== epoch:190, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288294995799926\n",
      "train loss:2.2972766783243164\n",
      "train loss:2.2869572242016805\n",
      "=== epoch:191, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290465186086694\n",
      "train loss:2.287162892497514\n",
      "train loss:2.291585240912031\n",
      "=== epoch:192, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.301059802067835\n",
      "train loss:2.2948540627743093\n",
      "train loss:2.2809184524731503\n",
      "=== epoch:193, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2988227696881305\n",
      "train loss:2.301723273696185\n",
      "train loss:2.3005696101511717\n",
      "=== epoch:194, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2912150939212688\n",
      "train loss:2.279044575858834\n",
      "train loss:2.2801173440249873\n",
      "=== epoch:195, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302370875083561\n",
      "train loss:2.2880270003038947\n",
      "train loss:2.2972350965226926\n",
      "=== epoch:196, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291880609868349\n",
      "train loss:2.291250643243505\n",
      "train loss:2.2868093802630565\n",
      "=== epoch:197, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2859143149397667\n",
      "train loss:2.291607508467663\n",
      "train loss:2.2986712402757736\n",
      "=== epoch:198, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293057313100092\n",
      "train loss:2.2973786397441143\n",
      "train loss:2.289071602224941\n",
      "=== epoch:199, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.28993685370441\n",
      "train loss:2.2856339214890413\n",
      "train loss:2.281868533433021\n",
      "=== epoch:200, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2983036007851756\n",
      "train loss:2.2944827778215395\n",
      "train loss:2.283645641622789\n",
      "=== epoch:201, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2929561946856984\n",
      "train loss:2.291210129607173\n",
      "train loss:2.287415598795147\n",
      "=== epoch:202, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2770621398036233\n",
      "train loss:2.291161299894309\n",
      "train loss:2.2989761873840284\n",
      "=== epoch:203, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2929152146646863\n",
      "train loss:2.3065107231777713\n",
      "train loss:2.288715094509728\n",
      "=== epoch:204, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2997647289767245\n",
      "train loss:2.2923898799939697\n",
      "train loss:2.2910397417666895\n",
      "=== epoch:205, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2935000213048795\n",
      "train loss:2.3043138631854108\n",
      "train loss:2.286220927911734\n",
      "=== epoch:206, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2828104723771427\n",
      "train loss:2.2933976631634163\n",
      "train loss:2.2954948487696285\n",
      "=== epoch:207, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.290076090721768\n",
      "train loss:2.2978301361291655\n",
      "train loss:2.290831740133834\n",
      "=== epoch:208, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2799534709458205\n",
      "train loss:2.295787484463481\n",
      "train loss:2.2967585056831745\n",
      "=== epoch:209, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.286028280690702\n",
      "train loss:2.2807697239125604\n",
      "train loss:2.2888389407489322\n",
      "=== epoch:210, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2949236033879017\n",
      "train loss:2.2906807579505917\n",
      "train loss:2.294850140333591\n",
      "=== epoch:211, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2854674677743008\n",
      "train loss:2.2914929309275403\n",
      "train loss:2.289000958092781\n",
      "=== epoch:212, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2920008728018293\n",
      "train loss:2.3014768075797543\n",
      "train loss:2.2969990982263995\n",
      "=== epoch:213, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.312391034156937\n",
      "train loss:2.291663378186803\n",
      "train loss:2.2907806432925297\n",
      "=== epoch:214, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2982399283626767\n",
      "train loss:2.2961024416120774\n",
      "train loss:2.2873204959039426\n",
      "=== epoch:215, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.282448706128044\n",
      "train loss:2.2931853936052313\n",
      "train loss:2.2899876670325203\n",
      "=== epoch:216, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298989838208791\n",
      "train loss:2.299397861213668\n",
      "train loss:2.287335126169768\n",
      "=== epoch:217, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3053115869688177\n",
      "train loss:2.2936749475722755\n",
      "train loss:2.290108925088419\n",
      "=== epoch:218, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3025184369910106\n",
      "train loss:2.3025181576333638\n",
      "train loss:2.288288367756732\n",
      "=== epoch:219, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.285741603305421\n",
      "train loss:2.279727134712657\n",
      "train loss:2.303933218032049\n",
      "=== epoch:220, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3016740685229213\n",
      "train loss:2.2910162011654123\n",
      "train loss:2.2865651161479263\n",
      "=== epoch:221, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2836874113884424\n",
      "train loss:2.27181994569447\n",
      "train loss:2.268859795657735\n",
      "=== epoch:222, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2951441496939595\n",
      "train loss:2.289449630741194\n",
      "train loss:2.2943076239761133\n",
      "=== epoch:223, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296387629745794\n",
      "train loss:2.2918680606384614\n",
      "train loss:2.2927149721118183\n",
      "=== epoch:224, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.289747459908455\n",
      "train loss:2.2944303000178063\n",
      "train loss:2.291731057537945\n",
      "=== epoch:225, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2818205993683045\n",
      "train loss:2.2923903244746917\n",
      "train loss:2.297081955556573\n",
      "=== epoch:226, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2875843665663407\n",
      "train loss:2.3000098780751927\n",
      "train loss:2.296839680613766\n",
      "=== epoch:227, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297350111874336\n",
      "train loss:2.2826862924099127\n",
      "train loss:2.2889409458404812\n",
      "=== epoch:228, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2787332739755186\n",
      "train loss:2.297236243918502\n",
      "train loss:2.293277065861922\n",
      "=== epoch:229, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.304823883766278\n",
      "train loss:2.29476791567951\n",
      "train loss:2.2820725881646706\n",
      "=== epoch:230, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294002232429664\n",
      "train loss:2.2839120478774046\n",
      "train loss:2.296530279717264\n",
      "=== epoch:231, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3022865267436248\n",
      "train loss:2.2933111938289747\n",
      "train loss:2.286781251970294\n",
      "=== epoch:232, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2914792216116875\n",
      "train loss:2.2841116720143644\n",
      "train loss:2.292965186825001\n",
      "=== epoch:233, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.275387507434464\n",
      "train loss:2.3052242183245126\n",
      "train loss:2.2881580480437567\n",
      "=== epoch:234, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2904622908195065\n",
      "train loss:2.287636504720314\n",
      "train loss:2.292126487502675\n",
      "=== epoch:235, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293787589652353\n",
      "train loss:2.2882089647315036\n",
      "train loss:2.291502170497624\n",
      "=== epoch:236, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298433203568694\n",
      "train loss:2.290100159171813\n",
      "train loss:2.29473488772172\n",
      "=== epoch:237, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2925995832113615\n",
      "train loss:2.3040118458454955\n",
      "train loss:2.293662558753736\n",
      "=== epoch:238, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.28248245036929\n",
      "train loss:2.3067610343726184\n",
      "train loss:2.2875360413244015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:239, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2912758103818898\n",
      "train loss:2.287698332498569\n",
      "train loss:2.2961273640735986\n",
      "=== epoch:240, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2853646578066353\n",
      "train loss:2.2826518943673806\n",
      "train loss:2.290922727458935\n",
      "=== epoch:241, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.293718411610709\n",
      "train loss:2.3022946040890035\n",
      "train loss:2.2988873423494063\n",
      "=== epoch:242, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.300677969674131\n",
      "train loss:2.2892380255970384\n",
      "train loss:2.2896873759419476\n",
      "=== epoch:243, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292840562049453\n",
      "train loss:2.3034359863214164\n",
      "train loss:2.3052370088258924\n",
      "=== epoch:244, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292103164665946\n",
      "train loss:2.2898014903150403\n",
      "train loss:2.3127982810195307\n",
      "=== epoch:245, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2915422572872286\n",
      "train loss:2.2918236666800986\n",
      "train loss:2.291738648000923\n",
      "=== epoch:246, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2881462250922326\n",
      "train loss:2.2958999166641356\n",
      "train loss:2.296516736047893\n",
      "=== epoch:247, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.294847200895693\n",
      "train loss:2.2940592321622\n",
      "train loss:2.2879984537642146\n",
      "=== epoch:248, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2910143798603873\n",
      "train loss:2.3006550393649845\n",
      "train loss:2.2840600772312114\n",
      "=== epoch:249, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288737013501705\n",
      "train loss:2.2858796372989456\n",
      "train loss:2.2883708351158423\n",
      "=== epoch:250, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.28213158539052\n",
      "train loss:2.2871134215479736\n",
      "train loss:2.2790722773349783\n",
      "=== epoch:251, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.27160421242756\n",
      "train loss:2.2880933689690193\n",
      "train loss:2.2966414054687774\n",
      "=== epoch:252, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2916336331356892\n",
      "train loss:2.2791555700013615\n",
      "train loss:2.283129626500221\n",
      "=== epoch:253, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.278076888176223\n",
      "train loss:2.3041906376411068\n",
      "train loss:2.292583573897879\n",
      "=== epoch:254, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2886691411128814\n",
      "train loss:2.2966653421800483\n",
      "train loss:2.3049957868823707\n",
      "=== epoch:255, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2875222713933345\n",
      "train loss:2.28714214150732\n",
      "train loss:2.294290570655528\n",
      "=== epoch:256, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2784574103380635\n",
      "train loss:2.2952754344234068\n",
      "train loss:2.2925861428809604\n",
      "=== epoch:257, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2958692689296556\n",
      "train loss:2.273706511696283\n",
      "train loss:2.2806751717022338\n",
      "=== epoch:258, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2900531820071945\n",
      "train loss:2.2853017771940873\n",
      "train loss:2.2939796653520346\n",
      "=== epoch:259, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.292016061619265\n",
      "train loss:2.3057508593787976\n",
      "train loss:2.291474222932597\n",
      "=== epoch:260, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3035562365338533\n",
      "train loss:2.2962330935261943\n",
      "train loss:2.286046313913936\n",
      "=== epoch:261, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2903190220275405\n",
      "train loss:2.2923227545668015\n",
      "train loss:2.2853723542203745\n",
      "=== epoch:262, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.286878098846227\n",
      "train loss:2.3041178413924697\n",
      "train loss:2.2791398363126754\n",
      "=== epoch:263, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2809427622250995\n",
      "train loss:2.2854870634992093\n",
      "train loss:2.2936377883029264\n",
      "=== epoch:264, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2808011285240988\n",
      "train loss:2.284945473737651\n",
      "train loss:2.282412282744984\n",
      "=== epoch:265, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291348387468535\n",
      "train loss:2.295280664073587\n",
      "train loss:2.290612088598057\n",
      "=== epoch:266, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3029284414495685\n",
      "train loss:2.294956055430691\n",
      "train loss:2.2949406975762088\n",
      "=== epoch:267, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2910336000301954\n",
      "train loss:2.297522655911307\n",
      "train loss:2.2794249072255024\n",
      "=== epoch:268, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288325607029021\n",
      "train loss:2.2968434656684233\n",
      "train loss:2.300540838717899\n",
      "=== epoch:269, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.282004439618497\n",
      "train loss:2.2986230903682743\n",
      "train loss:2.2827695589306285\n",
      "=== epoch:270, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291156278628604\n",
      "train loss:2.2892028322848588\n",
      "train loss:2.299479229353336\n",
      "=== epoch:271, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291979751619006\n",
      "train loss:2.3037425370247826\n",
      "train loss:2.2771074273618197\n",
      "=== epoch:272, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2717112617525475\n",
      "train loss:2.278341401188498\n",
      "train loss:2.285260123428688\n",
      "=== epoch:273, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.295388484308422\n",
      "train loss:2.299547671371611\n",
      "train loss:2.2841307526663615\n",
      "=== epoch:274, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3082045432533103\n",
      "train loss:2.295345773121955\n",
      "train loss:2.2793310087011434\n",
      "=== epoch:275, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.307268105409767\n",
      "train loss:2.2908225627461816\n",
      "train loss:2.282311644294175\n",
      "=== epoch:276, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3008276509561196\n",
      "train loss:2.292166676853453\n",
      "train loss:2.3065026503095343\n",
      "=== epoch:277, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2845965034936193\n",
      "train loss:2.3014802122256\n",
      "train loss:2.2908367853071328\n",
      "=== epoch:278, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.299456465114564\n",
      "train loss:2.300121768221681\n",
      "train loss:2.301307810185501\n",
      "=== epoch:279, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298867586744616\n",
      "train loss:2.2852804133813858\n",
      "train loss:2.2932161955342782\n",
      "=== epoch:280, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2940770660297995\n",
      "train loss:2.2969904285434675\n",
      "train loss:2.2789673270905695\n",
      "=== epoch:281, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2622399649813683\n",
      "train loss:2.290477525654849\n",
      "train loss:2.2939962989476905\n",
      "=== epoch:282, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2997519780936644\n",
      "train loss:2.2792521482883874\n",
      "train loss:2.3058852831560173\n",
      "=== epoch:283, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.320585125368416\n",
      "train loss:2.2842314868791838\n",
      "train loss:2.280481635096277\n",
      "=== epoch:284, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2885538560309295\n",
      "train loss:2.2915847726498773\n",
      "train loss:2.2909047287063213\n",
      "=== epoch:285, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2799202123106754\n",
      "train loss:2.286661872656489\n",
      "train loss:2.283490259351642\n",
      "=== epoch:286, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.296625747347372\n",
      "train loss:2.2930801945206247\n",
      "train loss:2.298043595356953\n",
      "=== epoch:287, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2683136785954003\n",
      "train loss:2.298736991850842\n",
      "train loss:2.283433797151516\n",
      "=== epoch:288, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298835066957914\n",
      "train loss:2.2845714824020487\n",
      "train loss:2.2720892449701138\n",
      "=== epoch:289, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.302144229176833\n",
      "train loss:2.2965548626477146\n",
      "train loss:2.2910224848137712\n",
      "=== epoch:290, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2992540775965007\n",
      "train loss:2.286609056434497\n",
      "train loss:2.295100815489341\n",
      "=== epoch:291, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.291321725557579\n",
      "train loss:2.29424694903054\n",
      "train loss:2.294738179969699\n",
      "=== epoch:292, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.297750226788804\n",
      "train loss:2.2867224746625574\n",
      "train loss:2.2912009853071704\n",
      "=== epoch:293, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.3133184180478223\n",
      "train loss:2.2842130950125754\n",
      "train loss:2.301794069267095\n",
      "=== epoch:294, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2781217325606744\n",
      "train loss:2.3010839311360476\n",
      "train loss:2.2821159496583614\n",
      "=== epoch:295, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.298817404054608\n",
      "train loss:2.283188232991158\n",
      "train loss:2.290939279424639\n",
      "=== epoch:296, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2840643686961424\n",
      "train loss:2.2987556597899057\n",
      "train loss:2.2741397046858847\n",
      "=== epoch:297, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.288168358840404\n",
      "train loss:2.288855445126515\n",
      "train loss:2.2925727962515516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:298, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2933995851870823\n",
      "train loss:2.292947337007985\n",
      "train loss:2.2801789688352945\n",
      "=== epoch:299, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.2808463309529627\n",
      "train loss:2.2972485206823574\n",
      "train loss:2.291199750925014\n",
      "=== epoch:300, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.29739190720718\n",
      "train loss:2.302129172919417\n",
      "train loss:2.2824337421076013\n",
      "=== epoch:301, train acc:0.13, test acc:0.1135 ===\n",
      "train loss:2.286336062982447\n",
      "train loss:2.2994488723258386\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.1135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ40lEQVR4nO3de5RV5Z3m8e9TFyyQEuRmUDSgTVQ0BLU0GoOtSUfFJCLdjlFDYjtJ0I7aOjO6xNF4mZ61YqYndjodhSaGxMQY4yjeieKFSGfZRgpFEfGCGrVApURBQK7Fb/44GyirThWngF2nqt7ns1Ytzt77PXv/Xjacp/btPYoIzMwsXRXlLsDMzMrLQWBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmlrjcgkDSdEnLJL3QxnJJ+qmkxZKel3R4XrWYmVnb8jwi+BVwcjvLxwEjs59JwJQcazEzszbkFgQRMQf4oJ0m44FfR8FTQH9JQ/Oqx8zMiqsq47b3Ad5uNt2QzXunZUNJkygcNbD77rsfcdBBB3VKgWZmPcW8efPej4jBxZaVMwhUZF7R8S4iYhowDaCuri7q6+vzrMvMrMeR9GZby8p511ADsG+z6WHA0jLVYmaWrHIGwX3At7O7h44GVkZEq9NCZmaWr9xODUn6HXA8MEhSA3ANUA0QEVOBmcApwGLgY+DcvGoxM7O25RYEEXHWdpYHcEFe2zczs9L4yWIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLXK5BIOlkSS9LWixpcpHl/STdL+k5SQslnZtnPWZm1lpuQSCpErgRGAeMAs6SNKpFswuAFyPic8DxwI8l9cqrJjMzay3PI4KjgMUR8XpEbABuB8a3aBNArSQBfYEPgE051mRmZi3kGQT7AG83m27I5jX3M+BgYCmwALg4Ija3XJGkSZLqJdU3NjbmVa+ZWZLyDAIVmRctpk8C5gN7A2OAn0nao9WbIqZFRF1E1A0ePHhX12lmlrQ8g6AB2LfZ9DAKv/k3dy4wIwoWA28AB+VYk5mZtZBnEMwFRkoakV0APhO4r0Wbt4AvA0jaCzgQeD3HmszMrIWqvFYcEZskXQg8DFQC0yNioaTzs+VTgX8CfiVpAYVTSZdHxPt51WRmZq3lFgQAETETmNli3tRmr5cCJ+ZZg5mZtc9PFpuZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWuFyDQNLJkl6WtFjS5DbaHC9pvqSFkp7Isx4zM2utKq8VS6oEbgS+AjQAcyXdFxEvNmvTH7gJODki3pI0JK96zMysuDyPCI4CFkfE6xGxAbgdGN+izdnAjIh4CyAiluVYj5mZFZFnEOwDvN1suiGb19xngD0l/VHSPEnfLrYiSZMk1Uuqb2xszKlcM7M05RkEKjIvWkxXAUcAXwVOAn4g6TOt3hQxLSLqIqJu8ODBu75SM7OElRQEku6S9FVJHQmOBmDfZtPDgKVF2jwUEWsi4n1gDvC5DmzDzMx2Uqkf7FMonM9/VdL1kg4q4T1zgZGSRkjqBZwJ3Neizb3AWElVkvoAnwcWlViTmZntAiXdNRQRjwKPSuoHnAU8Iult4OfArRGxsch7Nkm6EHgYqASmR8RCSedny6dGxCJJDwHPA5uBmyPihV3SMzMzK4kiWp62b6OhNBCYCHyLwime3wJfBD4bEcfnVWBLdXV1UV9f31mbMzPrESTNi4i6YstKOiKQNAM4CPgN8PWIeCdb9HtJ/lQ2M+vGSn2g7GcR8XixBW0ljJmZdQ+lXiw+OHsKGABJe0r6fj4lmZlZZyo1CL4XESu2TETEh8D3cqnIzMw6ValBUCFp6wNi2ThCvfIpyczMOlOp1wgeBu6QNJXC08HnAw/lVpWZmXWaUoPgcuA84B8oDB0xC7g5r6LMzKzzlPpA2WYKTxdPybccMzPrbKU+RzAS+CEwCqjZMj8i9s+pLjMz6ySlXiz+JYWjgU3ACcCvKTxcZmZm3VypQdA7Ih6jMCTFmxFxLfCl/MoyM7POUurF4nXZENSvZgPJLQH8tZJmZj1AqUcElwB9gH+k8EUyE4FzcqrJzMw60XaPCLKHx86IiMuA1cC5uVdlZmadZrtHBBHRBBzR/MliMzPrOUq9RvAscK+k/wes2TIzImbkUpWZmXWaUoNgALCcT94pFICDwMysmyv1yWJfFzAz66FKfbL4lxSOAD4hIv7rLq/IzMw6Vamnhh5o9roGmEDhe4vNzKybK/XU0F3NpyX9Dng0l4rMzKxTlfpAWUsjgf12ZSFmZlYepV4jWMUnrxG8S+E7CszMrJsr9dRQbd6FmJlZeZR0akjSBEn9mk33l3RablWZmVmnKfUawTURsXLLRESsAK7JpSIzM+tUpQZBsXal3npqZmZdWKlBUC/pBkkHSNpf0r8A8/IszMzMOkepQXARsAH4PXAHsBa4IK+izMys85R619AaYHLOtZiZWRmUetfQI5L6N5veU9LDuVVlZmadptRTQ4OyO4UAiIgP8XcWm5n1CKUGwWZJW4eUkDScIqORmplZ91PqLaBXAn+S9EQ2fRwwKZ+SzMysM5V6sfghSXUUPvznA/dSuHPIzMy6uVIvFn8XeAz4H9nPb4BrS3jfyZJelrRYUpt3HUk6UlKTpNNLK9vMzHaVUq8RXAwcCbwZEScAhwGN7b1BUiVwIzAOGAWcJWlUG+1+BPguJDOzMig1CNZFxDoASbtFxEvAgdt5z1HA4oh4PSI2ALcD44u0uwi4C1hWYi1mZrYLlRoEDdlzBPcAj0i6l+1/VeU+wNvN15HN20rSPhS+9nJqeyuSNElSvaT6xsZ2D0TMzKyDSr1YPCF7ea2k2UA/4KHtvE3FVtVi+ifA5RHRJBVrvnX704BpAHV1db5t1cxsF+rwCKIR8cT2WwGFI4B9m00Po/VRRB1wexYCg4BTJG2KiHs6WpeZme2YPIeSnguMlDQCWAKcCZzdvEFEjNjyWtKvgAccAmZmnSu3IIiITZIupHA3UCUwPSIWSjo/W97udQEzM+scuX65TETMBGa2mFc0ACLi7/OsxczMiiv1riEzM+uhHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZonLNQgknSzpZUmLJU0usvybkp7Pfp6U9Lk86zEzs9ZyCwJJlcCNwDhgFHCWpFEtmr0B/HVEjAb+CZiWVz1mZlZcnkcERwGLI+L1iNgA3A6Mb94gIp6MiA+zyaeAYTnWY2ZmReQZBPsAbzebbsjmteU7wB+KLZA0SVK9pPrGxsZdWKKZmeUZBCoyL4o2lE6gEASXF1seEdMioi4i6gYPHrwLSzQzs6oc190A7NtsehiwtGUjSaOBm4FxEbE8x3rMzKyIPI8I5gIjJY2Q1As4E7iveQNJ+wEzgG9FxCs51mJmZm3I7YggIjZJuhB4GKgEpkfEQknnZ8unAlcDA4GbJAFsioi6vGoyM7PWFFH0tH2XVVdXF/X19eUuw8ysW5E0r61ftPO8RmBm1mVs3LiRhoYG1q1bV+5SclVTU8OwYcOorq4u+T0OAjNLQkNDA7W1tQwfPpzsVHSPExEsX76choYGRowYUfL7PNaQmSVh3bp1DBw4sMeGAIAkBg4c2OGjHgeBmSWjJ4fAFjvSRweBmVniHARmZkXc8+wSjr3+cUZMfpBjr3+ce55dslPrW7FiBTfddFOH33fKKaewYsWKndr29jgIzMxauOfZJVwxYwFLVqwlgCUr1nLFjAU7FQZtBUFTU1O775s5cyb9+/ff4e2WwncNtXDPs0v454dfZumKtezdvzeXnXQgpx3Weqy81Np1hxr9d9P12nWlGn86bts4Zdfdv5AXl34EwKbNwYZNm4kIJNGrqoIFDSvZ0LT5E+tbu7GJy+58jpv/4/Wt7aoqtp2PH7X3Hlzz9UP48OMNvLdyHRuaNtOrsoK9+tWwZ59eTJ48mddee40xY8ZQXV1N3759GTh4L+bPn8+Mx5/iv393Iu+/t5SNG9Zz8cUXM2nSJACGDx/Oo3Oe5M13l/O9b57OEUcdzYJn57LfsGHce++99O7du+jfe0ck+UDZOyvXMrRfbyKCl95dxdqNhUSe80ojU/74Gus3bfsHsFtVBf9w/AEc95lt/4hSa9cdavTfTddr19VqvHn8UD57yCj67lbFD/+wiJffXUXT5ih84Df/GBS8sOQj2nLo3ntsbdersoLKLAwO/FQtF31pJI2r1rO52edqhcTg2t1Y/k4Dp//tacx9Zj5znniCv5twKjMefZK99/00ACs//JA9Bwygb9VmTvnSWB565DEGDhzIQZ85gNsemM3q1av5+tjDue3B2Yw6dDRX/eN3OH3CaUycOLFVjYsWLeLggw/+xLz2HihLLgjmvNLIOb98mocvOY73V63n7Jv/vAurM7Ou6uenDmWv/fYvqe13bqmncdX6VvMH1+7GL87ZsVFwlrz9Fhf9/TeY8dh/Mvc//8TUf/kRv7jj/q3Lp9xwPY8/9AAASxveYsqtdzH68CMZd8xobntwNh+vWcP5Z0/g/v+YB8AtU/6VPXtXctVVV7XaVkeDILlTQ0+/8QERMP/tFbz0zip2q6pg6sQjkOCQ2+oYrJWt3tMY/Vh49rbwSa1dd6jRfzddr11Xq3GR7uCvVMW6gds+IGuWL6Jarc/RTz5md66cvZF1G7cdUfSuEld+oTejK97YOm9jVJa0vo1RScWA3vSqqmDEoN1p6FfDkD7auq4/PlnPgj89zLP3T6O6pi9fOusiBtSIEYN2p5omDql4k9UVa6ndbdt7hlR8xNpNta22tSOSC4IFSwr/OF55dxVPvLKMz+8/kBMOGlJYWOQfGMBgreT4A4dsm5Fau+5Qo/9uul67LlhjtZqormk29EKRD22Avzuohso9R2+7xlBbyWVfqOW0A/vs0Pqq1cTQQQNYs3o1tTXV9OlVhZqdj1q5ajV79qulT+/evLR4MXOf/jN9elVRW1NNG1/jQqU2F52/I5IIguYXjfprNZdVPcADiybSuHwFP9vjEbh7+vZXcvf5pW0stXbl3HZXb1fObXf1duXc9odvltTstOGbOO28AwoTaz/Y6fUNrFjNsUeO4dCDD6R37xr2GrDH1mUnH/8Fpv7mTkb/zRkcuP9wjq4bA6veLXndO6vHXyNY98P9qVnf+vtuVkVv/q1pAv+z6jbot1/h+9RWvNX2ivrvt+11au3Kue2u3q6c2+7q7cq57SLtFp10Bwd/eghU9to2s2lD2+vLs92uWufehxWd7WsELRQLAYBarWVizZPwqSPhu48WZl7br+0VXbJg2+vU2pVz2129XTm33dXblXPb7bXb65Btr5c+W552u2qdu0jSD5Ttt+kvMObscpdhZlZWSQcBx1wIo7+xbXr3IcXbtZyfWrtybrurtyvntrt6u3Juu612FVXtT3dWu7zWuYN6/DWCdg8Rry1+Z4GZ9TzFzpv3VB29RpD2EYGZmSUQBB05jDUzS1CPv2uIy14tdwVm1t3880hYs6z1/N2H7PBnyooVK7jtttv4/ve/3+H3/uQnP2HSpEn06dNn+413QM8/IjAz66hiIdDe/BLs6PcRQCEIPv744x3e9vb0/CMCM7OW/jAZ3l2w/XbF/PKrxed/6rMw7vo239Z8GOqvfOUrDBkyhDvuuIP169czYcIErrvuOtasWcMZZ5xBQ0MDTU1N/OAHP+C9995j6dKlnHDCCQwaNIjZs2fvWN3tcBCYmXWC66+/nhdeeIH58+cza9Ys7rzzTp5++mkiglNPPZU5c+bQ2NjI3nvvzYMPPgjAypUr6devHzfccAOzZ89m0KBBudTmIDCz9LTzmzvQ/m3n5z6405ufNWsWs2bN4rDDCkNErF69mldffZWxY8dy6aWXcvnll/O1r32NsWPH7vS2SuEgMDPrZBHBFVdcwXnnnddq2bx585g5cyZXXHEFJ554IldffXXu9fhisZlZSzncdl5bW8uqVasAOOmkk5g+fTqrV68GYMmSJSxbtoylS5fSp08fJk6cyKWXXsozzzzT6r158BGBmVlLOdx2PnDgQI499lgOPfRQxo0bx9lnn80xxxwDQN++fbn11ltZvHgxl112GRUVFVRXVzNlyhQAJk2axLhx4xg6dGguF4t7/hATZmZ4iAkPMWFmZm1yEJiZJc5BYGbJ6G6nwnfEjvTRQWBmSaipqWH58uU9OgwiguXLl1NTU9Oh9/muITNLwrBhw2hoaKCxsbHcpeSqpqaGYcOGdeg9DgIzS0J1dTUjRowodxldUq6nhiSdLOllSYslTS6yXJJ+mi1/XtLhedZjZmat5RYEkiqBG4FxwCjgLEmjWjQbB4zMfiYBU/Kqx8zMisvziOAoYHFEvB4RG4DbgfEt2owHfh0FTwH9JQ3NsSYzM2shz2sE+wBvN5tuAD5fQpt9gHeaN5I0icIRA8BqSS/vYE2DgPd38L1djfvSNfWUvvSUfoD7ssWn21qQZxCoyLyW922V0oaImAZM2+mCpPq2HrHubtyXrqmn9KWn9APcl1LkeWqoAdi32fQwYOkOtDEzsxzlGQRzgZGSRkjqBZwJ3NeizX3At7O7h44GVkbEOy1XZGZm+cnt1FBEbJJ0IfAwUAlMj4iFks7Plk8FZgKnAIuBj4Fz86ons9Onl7oQ96Vr6il96Sn9APdlu7rdMNRmZrZreawhM7PEOQjMzBKXTBBsb7iLrk7SXyQtkDRfUn02b4CkRyS9mv25Z7nrbEnSdEnLJL3QbF6bdUu6IttHL0s6qTxVF9dGX66VtCTbL/MlndJsWVfuy76SZktaJGmhpIuz+d1q37TTj263XyTVSHpa0nNZX67L5ue/TyKix/9QuFj9GrA/0At4DhhV7ro62Ie/AINazPs/wOTs9WTgR+Wus0jdxwGHAy9sr24KQ5E8B+wGjMj2WWW5+7CdvlwLXFqkbVfvy1Dg8Ox1LfBKVnO32jft9KPb7RcKz1X1zV5XA38Gju6MfZLKEUEpw110R+OBW7LXtwCnla+U4iJiDvBBi9lt1T0euD0i1kfEGxTuJjuqM+osRRt9aUtX78s7EfFM9noVsIjCU/3dat+004+2dMl+AETB6myyOvsJOmGfpBIEbQ1l0Z0EMEvSvGzIDYC9InvuIvtzSNmq65i26u6u++nCbPTc6c0O27tNXyQNBw6j8Btot903LfoB3XC/SKqUNB9YBjwSEZ2yT1IJgpKGsujijo2IwymM2HqBpOPKXVAOuuN+mgIcAIyhMEbWj7P53aIvkvoCdwGXRMRH7TUtMq/L9KdIP7rlfomIpogYQ2GUhaMkHdpO813Wl1SCoNsPZRERS7M/lwF3UzgEfG/LaK3Zn8vKV2GHtFV3t9tPEfFe9p93M/Bzth2ad/m+SKqm8OH524iYkc3udvumWD+6834BiIgVwB+Bk+mEfZJKEJQy3EWXJWl3SbVbXgMnAi9Q6MM5WbNzgHvLU2GHtVX3fcCZknaTNILC91Q8XYb6SqZPDps+gcJ+gS7eF0kCfgEsiogbmi3qVvumrX50x/0iabCk/tnr3sDfAC/RGfuk3FfKO/GK/CkU7ih4Dbiy3PV0sPb9Kdwd8BywcEv9wEDgMeDV7M8B5a61SO2/o3BovpHCbzDfaa9u4MpsH70MjCt3/SX05TfAAuD57D/m0G7Sly9SOI3wPDA/+zmlu+2bdvrR7fYLMBp4Nqv5BeDqbH7u+8RDTJiZJS6VU0NmZtYGB4GZWeIcBGZmiXMQmJklzkFgZpY4B4FZziQdL+mBctdh1hYHgZlZ4hwEZhlJE7Px4OdL+vdsALDVkn4s6RlJj0kanLUdI+mpbFCzu7cMaibpryQ9mo0p/4ykA7LV95V0p6SXJP02eyIWSddLejFbz/8tU9ctcQ4CM0DSwcA3KAzuNwZoAr4J7A48E4UB/54Arsne8mvg8ogYTeEJ1i3zfwvcGBGfA75A4UlkKIyKeQmFMeT3B46VNIDC8AeHZOv533n20awtDgKzgi8DRwBzs2GAv0zhA3sz8Pusza3AFyX1A/pHxBPZ/FuA47LxoPaJiLsBImJdRHyctXk6IhqiMAjafGA48BGwDrhZ0t8CW9qadSoHgVmBgFsiYkz2c2BEXFukXXtjshQbFniL9c1eNwFVEbGJwqiYd1H4spGHOlay2a7hIDAreAw4XdIQ2Po9sZ+m8H/k9KzN2cCfImIl8KGksdn8bwFPRGEc/AZJp2Xr2E1Sn7Y2mI2h3y8iZlI4bTRml/fKrARV5S7ArCuIiBclXUXhW+AqKIwwegGwBjhE0jxgJYXrCFAYDnhq9kH/OnBuNv9bwL9L+l/ZOv5LO5utBe6VVEPhaOK/7eJumZXEo4+atUPS6ojoW+46zPLkU0NmZonzEYGZWeJ8RGBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmlrj/D4DmIpieeKNRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#遊びで、dropout_ratioが極端に高い場合を２例\n",
    "trial(use_dropout=True, dropout_ratio=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
